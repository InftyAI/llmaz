fullnameOverride: "llmaz"

backendRuntime:
  enabled: true
  llamacpp:
    image:
      repository: ghcr.io/ggerganov/llama.cpp
      tag: server
  ollama:
    image:
      repository: ollama/ollama
      tag: latest
  sglang:
    image:
      repository: lmsysorg/sglang
      tag: v0.4.5-cu121
  tensorrt_llm:
    image:
      repository: nvcr.io/nvidia/tritonserver
      tag: 25.03-trtllm-python-py3
  tgi:
    image:
      repository: ghcr.io/huggingface/text-generation-inference
      tag: 2.3.1
  vllm:
    image:
      repository: vllm/vllm-openai
      tag: v0.7.3

leaderWorkerSet:
  enabled: true

kube-scheduler:
  # If kube-scheduler is enabled, please set the globalConfig.configData.scheduler-name with the name of the kube-scheduler.
  enabled: false
  # scheduler:
  #   name: inftyai-scheduler

prometheus:
  # Prometheus is required to enable smart routing.
  enabled: false

open-webui:
  enabled: true
  persistence:
    enabled: false
  enableOpenaiApi: true
  # Assumes the OpenAI-compatible API is exposed via the Envoy AI Gateway. Please replace this value if you
  # deployed the envoy-ai-gateway example to a namespace other than "default".
  openaiBaseApiUrl: "http://envoy-default-default-envoy-ai-gateway-dbec795a.llmaz-system.svc.cluster.local/v1"
  extraEnvVars:
  - name: OPENAI_API_KEY
    value: "ChangeMe"
  ollama:
    enabled: false
  pipelines:
    enabled: false
  tika:
    enabled: false
  redis-cluster:
    enabled: false

envoy-gateway:
  enabled: true
envoy-ai-gateway:
  enabled: true
