{{- if .Values.backendRuntime.install -}}
apiVersion: inference.llmaz.io/v1alpha1
kind: BackendRuntime
metadata:
  labels:
    app.kubernetes.io/name: backendruntime
    app.kubernetes.io/part-of: llmaz
    app.kubernetes.io/created-by: llmaz
  name: ollama
spec:
  command:
    - sh
    - -c
  image: {{ .Values.backendRuntime.ollama.image.repository }}
  version: {{ .Values.backendRuntime.ollama.image.tag }}
  envs:
    - name: OLLAMA_HOST
      value: 0.0.0.0:8080
  # Do not edit the preset argument name unless you know what you're doing.
  # Free to add more arguments with your requirements.
  recommendedConfigs:
    - name: default
      args:
        - "ollama serve &
          while true; do output=$(ollama list 2>&1);
          if ! echo $output | grep -q 'could not connect to ollama app' && echo $output | grep -q 'NAME';then echo 'ollama is running';break; else echo 'Waiting for the ollama to be running...';sleep 1;fi;done;
          ollama run {{`{{ .ModelName }}`}};
          while true;do sleep 60;done"
      resources:
        requests:
          cpu: 2
          memory: 4Gi
        limits:
          cpu: 2
          memory: 4Gi
{{- end }}
