[{"body":"If you want to integrate more backends into llmaz, please refer to this PR. It’s always welcomed.\nllama.cpp llama.cpp is to enable LLM inference with minimal setup and state-of-the-art performance on a wide variety of hardware - locally and in the cloud.\nollama ollama is running with Llama 3.2, Mistral, Gemma 2, and other large language models, based on llama.cpp, aims for local deploy.\nSGLang SGLang is yet another fast serving framework for large language models and vision language models.\nTensorRT-LLM TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and support state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that orchestrate the inference execution in performant way.\nText-Generation-Inference text-generation-inference is a Rust, Python and gRPC server for text generation inference. Used in production at Hugging Face to power Hugging Chat, the Inference API and Inference Endpoint.\nvLLM vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs\n","categories":"","description":"","excerpt":"If you want to integrate more backends into llmaz, please refer to …","ref":"/docs/features/broad-backends/","tags":"","title":"Broad Inference Backends Support"},{"body":"Envoy AI Gateway is an open source project for using Envoy Gateway to handle request traffic from application clients to Generative AI services.\nHow to use Enable Envoy Gateway and Envoy AI Gateway Both of them are already enabled by default in values.global.yaml and will be deployed in llmaz-system.\nenvoy-gateway: enabled: true envoy-ai-gateway: enabled: true However, Envoy Gateway and Envoy AI Gateway can be deployed standalone in case you want to deploy them in other namespaces.\nBasic Example To expose your models via Envoy Gateway, you need to create a GatewayClass, Gateway, and AIGatewayRoute. The following example shows how to do this.\nWe’ll deploy two models Qwen/Qwen2-0.5B-Instruct-GGUF and Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF with llama.cpp (cpu only) and expose them via Envoy AI Gateway.\nThe full example is here, apply it.\nkubectl apply -f https://raw.githubusercontent.com/InftyAI/llmaz/refs/heads/main/docs/examples/envoy-ai-gateway/basic.yaml Query AI Gateway APIs If Open-WebUI is enabled, you can chat via the webui (recommended), see documentation. Otherwise, following the steps below to test the Envoy AI Gateway APIs.\nI. Port-forwarding the LoadBalancer service in llmaz-system, like:\nkubectl -n llmaz-system port-forward \\ $(kubectl -n llmaz-system get svc \\ -l gateway.envoyproxy.io/owning-gateway-name=default-envoy-ai-gateway \\ -o name) \\ 8080:80 II. Query curl http://localhost:8080/v1/models | jq ., available models will be listed. Expected response will look like this:\n{ \"data\": [ { \"id\": \"qwen2-0.5b\", \"created\": 1745327294, \"object\": \"model\", \"owned_by\": \"Envoy AI Gateway\" }, { \"id\": \"qwen2.5-coder\", \"created\": 1745327294, \"object\": \"model\", \"owned_by\": \"Envoy AI Gateway\" } ], \"object\": \"list\" } III. Query http://localhost:8080/v1/chat/completions to chat with the model. Here, we ask the qwen2-0.5b model, the query will look like:\ncurl -H \"Content-Type: application/json\" -d '{ \"model\": \"qwen2-0.5b\", \"messages\": [ { \"role\": \"system\", \"content\": \"Hi.\" } ] }' http://localhost:8080/v1/chat/completions | jq . Expected response will look like this:\n{ \"choices\": [ { \"finish_reason\": \"stop\", \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"Hello! How can I assist you today?\" } } ], \"created\": 1745327371, \"model\": \"qwen2-0.5b\", \"system_fingerprint\": \"b5124-bc091a4d\", \"object\": \"chat.completion\", \"usage\": { \"completion_tokens\": 10, \"prompt_tokens\": 10, \"total_tokens\": 20 }, \"id\": \"chatcmpl-AODlT8xnf4OjJwpQH31XD4yehHLnurr0\", \"timings\": { \"prompt_n\": 1, \"prompt_ms\": 319.876, \"prompt_per_token_ms\": 319.876, \"prompt_per_second\": 3.1262114069201816, \"predicted_n\": 10, \"predicted_ms\": 1309.393, \"predicted_per_token_ms\": 130.9393, \"predicted_per_second\": 7.63712651587415 } } ","categories":"","description":"","excerpt":"Envoy AI Gateway is an open source project for using Envoy Gateway to …","ref":"/docs/integrations/envoy-ai-gateway/","tags":"","title":"Envoy AI Gateway"},{"body":"","categories":"","description":"This section contains the tutorials for llmaz.\n","excerpt":"This section contains the tutorials for llmaz.\n","ref":"/docs/getting-started/","tags":"","title":"Getting Started"},{"body":"Requirements:\nKubernetes version \u003e= 1.27.\nLWS requires Kubernetes version v1.27 or higher. If you are using a lower Kubernetes version and most of your workloads rely on single-node inference, we may consider replacing LWS with a Deployment-based approach. This fallback plan would involve using Kubernetes Deployments to manage single-node inference workloads efficiently. See #32 for more details and updates.\nHelm 3, see installation.\nNote that llmaz helm chart will by default install:\nLWS as the default inference workload in the llmaz-system, if you *already installed it * or want to deploy it in other namespaces , append --set leaderWorkerSet.enabled=false to the command below. Envoy Gateway and Envoy AI Gateway as the frontier in the llmaz-system, if you already installed these two components or want to deploy in other namespaces , append --set envoy-gateway.enabled=false --set envoy-ai-gateway.enabled=false to the command below. Open WebUI as the default chatbot, if you want to disable it, append --set open-webui.enabled=false to the command below. ","categories":"","description":"This section contains the prerequisites for llmaz.\n","excerpt":"This section contains the prerequisites for llmaz.\n","ref":"/docs/getting-started/prerequisites/","tags":"","title":"Prerequisites"},{"body":"","categories":"","description":"This section contains the advanced features of llmaz.\n","excerpt":"This section contains the advanced features of llmaz.\n","ref":"/docs/features/","tags":"","title":"Features"},{"body":"A llama2-7B model can be running on 1xA100 GPU, also on 1xA10 GPU, even on 1x4090 and a variety of other types of GPUs as well, that’s what we called resource fungibility. In practical scenarios, we may have a heterogeneous cluster with different GPU types, and high-end GPUs will stock out a lot, to meet the SLOs of the service as well as the cost, we need to schedule the workloads on different GPU types. With the ResourceFungibility in the InftyAI scheduler, we can simply achieve this with at most 8 alternative GPU types.\nHow to use Enable InftyAI scheduler Edit the values.global.yaml file to modify the following values:\nkube-scheduler: enabled: true globalConfig: configData: |- scheduler-name: inftyai-scheduler Run make helm-upgrade to install or upgrade llmaz.\n","categories":"","description":"","excerpt":"A llama2-7B model can be running on 1xA100 GPU, also on 1xA10 GPU, …","ref":"/docs/features/heterogeneous-cluster-support/","tags":"","title":"Heterogeneous Cluster Support"},{"body":"Install a released version (recommended) Install helm install llmaz oci://registry-1.docker.io/inftyai/llmaz --namespace llmaz-system --create-namespace --version 0.0.10 Uninstall helm uninstall llmaz --namespace llmaz-system kubectl delete ns llmaz-system If you want to delete the CRDs as well, run\nkubectl delete crd \\ openmodels.llmaz.io \\ backendruntimes.inference.llmaz.io \\ playgrounds.inference.llmaz.io \\ services.inference.llmaz.io Install from source Change configurations If you want to change the default configurations, please change the values in values.global.yaml.\nDo not change the values in values.yaml because it’s auto-generated and will be overwritten.\nInstall git clone https://github.com/inftyai/llmaz.git \u0026\u0026 cd llmaz kubectl create ns llmaz-system \u0026\u0026 kubens llmaz-system make helm-install Uninstall helm uninstall llmaz --namespace llmaz-system kubectl delete ns llmaz-system If you want to delete the CRDs as well, run\nkubectl delete crd \\ openmodels.llmaz.io \\ backendruntimes.inference.llmaz.io \\ playgrounds.inference.llmaz.io \\ services.inference.llmaz.io Upgrade Once you changed your code, run the command to upgrade the controller:\nIMG=\u003cimage-registry\u003e:\u003ctag\u003e make helm-upgrade ","categories":"","description":"This section introduces the installation guidance for llmaz.\n","excerpt":"This section introduces the installation guidance for llmaz.\n","ref":"/docs/getting-started/installation/","tags":"","title":"Installation"},{"body":"Karpenter automatically launches just the right compute resources to handle your cluster’s applications, but it is built to adhere to the scheduling decisions of kube-scheduler, so it’s certainly possible we would run across some cases where Karpenter makes incorrect decisions when the InftyAI scheduler is in the mix.\nWe forked the Karpenter project and re-complie the karpenter image for cloud providers like AWS, and you can find the details in this proposal. This document provides deployment steps to install and configure Customized Karpenter in an EKS cluster.\nHow to use Set environment variables export KARPENTER_NAMESPACE=\"kube-system\" export KARPENTER_VERSION=\"1.5.0\" export K8S_VERSION=\"1.32\" export AWS_PARTITION=\"aws\" # if you are not using standard partitions, you may need to configure to aws-cn / aws-us-gov export CLUSTER_NAME=\"${USER}-karpenter-demo\" export AWS_DEFAULT_REGION=\"us-west-2\" export AWS_ACCOUNT_ID=\"$(aws sts get-caller-identity --query Account --output text)\" export TEMPOUT=\"$(mktemp)\" export ALIAS_VERSION=\"$(aws ssm get-parameter --name \"/aws/service/eks/optimized-ami/${K8S_VERSION}/amazon-linux-2023/x86_64/standard/recommended/image_id\" --query Parameter.Value | xargs aws ec2 describe-images --query 'Images[0].Name' --image-ids | sed -r 's/^.*(v[[:digit:]]+).*$/\\1/')\" If you open a new shell to run steps in this procedure, you need to set some or all of the environment variables again. To remind yourself of these values, type:\necho \"${KARPENTER_NAMESPACE}\" \"${KARPENTER_VERSION}\" \"${K8S_VERSION}\" \"${CLUSTER_NAME}\" \"${AWS_DEFAULT_REGION}\" \"${AWS_ACCOUNT_ID}\" \"${TEMPOUT}\" \"${ALIAS_VERSION}\" Create a cluster and add Karpenter Please refer to the Getting Started with Karpenter to create a cluster and add Karpenter.\nInstall the gpu operator helm repo add nvidia https://helm.ngc.nvidia.com/nvidia \\ \u0026\u0026 helm repo update helm install --wait --generate-name \\ -n gpu-operator --create-namespace \\ nvidia/gpu-operator \\ --version=v25.3.0 Install llmaz with InftyAI scheduler enabled Please refer to heterogeneous cluster support.\nConfigure Karpenter with customized image We need to assign the karpenter-core-llmaz cluster role to the karpenter service account and update the karpenter image to the customized one.\ncat \u003c\u003cEOF | envsubst | kubectl apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: karpenter-core-llmaz roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: karpenter-core-llmaz subjects: - kind: ServiceAccount name: karpenter namespace: ${KARPENTER_NAMESPACE} --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: karpenter-core-llmaz rules: - apiGroups: [\"llmaz.io\"] resources: [\"openmodels\"] verbs: [\"get\", \"list\", \"watch\"] EOF helm upgrade --install karpenter oci://public.ecr.aws/karpenter/karpenter --version \"${KARPENTER_VERSION}\" --namespace \"${KARPENTER_NAMESPACE}\" --create-namespace \\ --set \"settings.clusterName=${CLUSTER_NAME}\" \\ --set \"settings.interruptionQueue=${CLUSTER_NAME}\" \\ --set controller.resources.requests.cpu=1 \\ --set controller.resources.requests.memory=1Gi \\ --set controller.resources.limits.cpu=1 \\ --set controller.resources.limits.memory=1Gi \\ --wait \\ --set controller.image.repository=inftyai/aws-karpenter \\ --set \"controller.image.tag=${KARPENTER_VERSION}\" \\ --set controller.image.digest=\"\" Basic Example Create a gpu node pool cat \u003c\u003cEOF | envsubst | kubectl apply -f - apiVersion: karpenter.k8s.aws/v1 kind: EC2NodeClass metadata: name: llmaz-demo # you can change the name to a more meaningful one, please align with the node pool's nodeClassRef. spec: amiSelectorTerms: - alias: al2023@${ALIAS_VERSION} blockDeviceMappings: # the default volume size of the selected AMI is 20Gi, it is not enough for kubelet to pull # the images and run the workloads. So we need to map a larger volume to the root device. # You can change the volume size to a larger value according to your actual needs. - deviceName: /dev/xvda ebs: deleteOnTermination: true volumeSize: 50Gi volumeType: gp3 role: KarpenterNodeRole-${CLUSTER_NAME} # replace with your cluster name securityGroupSelectorTerms: - tags: karpenter.sh/discovery: ${CLUSTER_NAME} # replace with your cluster name subnetSelectorTerms: - tags: karpenter.sh/discovery: ${CLUSTER_NAME} # replace with your cluster name --- apiVersion: karpenter.sh/v1 kind: NodePool metadata: name: llmaz-demo-gpu-nodepool # you can change the name to a more meaningful one. spec: disruption: budgets: - nodes: 10% consolidateAfter: 5m consolidationPolicy: WhenEmptyOrUnderutilized limits: # You can change the limits to match your actual needs. cpu: 1000 template: spec: expireAfter: 720h nodeClassRef: group: karpenter.k8s.aws kind: EC2NodeClass name: llmaz-demo requirements: - key: kubernetes.io/arch operator: In values: - amd64 - key: kubernetes.io/os operator: In values: - linux - key: karpenter.sh/capacity-type operator: In values: - spot - key: karpenter.k8s.aws/instance-family operator: In values: # replace with your instance-family with gpu supported - g4dn - g5g taints: - effect: NoSchedule key: nvidia.com/gpu value: \"true\" Deploy a model with flavors cat \u003c\u003cEOF | kubectl apply -f - apiVersion: llmaz.io/v1alpha1 kind: OpenModel metadata: name: qwen2-0--5b spec: familyName: qwen2 source: modelHub: modelID: Qwen/Qwen2-0.5B-Instruct inferenceConfig: flavors: # The g5g instance family in the aws cloud can provide the t4g GPU type. # we define the instance family in the node pool like llmaz-demo-gpu-nodepool. - name: t4g limits: nvidia.com/gpu: 1 # The flavorName is not recongnized by the Karpenter, so we need to specify the # instance-gpu-name via nodeSelector to match the t4g GPU type when node is provisioned # by Karpenter from multiple node pools. # # When you only have a single node pool to provision the GPU instance and the node pool # only has one GPU type, it is okay to not specify the nodeSelector. But in practice, # it is better to specify the nodeSelector to make the provisioned node more predictable. # # The available node labels for selecting the target GPU device is listed below: # karpenter.k8s.aws/instance-gpu-count # karpenter.k8s.aws/instance-gpu-manufacturer # karpenter.k8s.aws/instance-gpu-memory # karpenter.k8s.aws/instance-gpu-name nodeSelector: karpenter.k8s.aws/instance-gpu-name: t4g # The g4dn instance family in the aws cloud can provide the t4 GPU type. # we define the instance family in the node pool like llmaz-demo-gpu-nodepool. - name: t4 limits: nvidia.com/gpu: 1 # The flavorName is not recongnized by the Karpenter, so we need to specify the # instance-gpu-name via nodeSelector to match the t4 GPU type when node is provisioned # by Karpenter from multiple node pools. # # When you only have a single node pool to provision the GPU instance and the node pool # only has one GPU type, it is okay to not specify the nodeSelector. But in practice, # it is better to specify the nodeSelector to make the provisioned node more predictable. # # The available node labels for selecting the target GPU device is listed below: # karpenter.k8s.aws/instance-gpu-count # karpenter.k8s.aws/instance-gpu-manufacturer # karpenter.k8s.aws/instance-gpu-memory # karpenter.k8s.aws/instance-gpu-name nodeSelector: karpenter.k8s.aws/instance-gpu-name: t4 --- # Currently, the Playground resource type does not support to configure tolerations # for the generated pods. But luckily, when a pod with the `nvidia.com/gpu` resource # is created on the eks cluster, the generated pod will be tweaked with the following # tolerations: # - effect: NoExecute # key: node.kubernetes.io/not-ready # operator: Exists # tolerationSeconds: 300 # - effect: NoExecute # key: node.kubernetes.io/unreachable # operator: Exists # tolerationSeconds: 300 # - effect: NoSchedule # key: nvidia.com/gpu # operator: Exists apiVersion: inference.llmaz.io/v1alpha1 kind: Playground metadata: labels: llmaz.io/model-name: qwen2-0--5b name: qwen2-0--5b spec: backendRuntimeConfig: backendName: tgi # Due to the limitation of our aws account, we have to decrease the resources to match # the avaliable instance type which is g4dn.xlarge. If your account has no such limitation, # you can remove the custom resources settings below. resources: limits: cpu: \"2\" memory: 4Gi requests: cpu: \"2\" memory: 4Gi modelClaim: modelName: qwen2-0--5b replicas: 1 EOF ","categories":"","description":"","excerpt":"Karpenter automatically launches just the right compute resources to …","ref":"/docs/integrations/karpenter/","tags":"","title":"Karpenter"},{"body":"Let’s assume that you have installed the llmaz with the default settings, which means both the AI Gateway and Open WebUI are installed. Now let’s following the steps to chat with your models.\nDeploy the Services Run the following command to deploy two models (cpu only).\nkubectl apply -f https://raw.githubusercontent.com/InftyAI/llmaz/refs/heads/main/docs/examples/envoy-ai-gateway/basic.yaml Chat with Models Waiting for your services ready, generally looks like:\nNAME READY STATUS RESTARTS AGE ai-eg-route-extproc-default-envoy-ai-gateway-6ddcd49b64-ldwcd 1/1 Running 0 6m37s qwen2--5-coder-0 1/1 Running 0 6m37s qwen2-0--5b-0 1/1 Running 0 6m37s Once ready, you can access the Open WebUI by port-forwarding the service:\nkubectl port-forward svc/open-webui 8080:80 -n llmaz-system Let’s chat on http://localhost:8080 now, two models are available to you! 🎉\n","categories":"","description":"This section introduces the basic usage of llmaz.\n","excerpt":"This section introduces the basic usage of llmaz.\n","ref":"/docs/getting-started/basic-usage/","tags":"","title":"Basic Usage"},{"body":"Support multi-host \u0026 homogeneous xPyD distributed serving with LWS from day 0. Will implement the heterogeneous xPyD in the future.\n","categories":"","description":"","excerpt":"Support multi-host \u0026 homogeneous xPyD distributed serving with LWS …","ref":"/docs/features/distributed_inference/","tags":"","title":"Distributed Inference"},{"body":"","categories":"","description":"This section contains the llmaz integration information.\n","excerpt":"This section contains the llmaz integration information.\n","ref":"/docs/integrations/","tags":"","title":"Integrations"},{"body":"Open WebUI is a user-friendly AI interface with OpenAI-compatible APIs, serving as the default chatbot for llmaz.\nPrerequisites Make sure EnvoyGateway and Envoy AI Gateway are installed, both of them are installed by default in llmaz. See AI Gateway for more details. How to use Enable Open WebUI Open-WebUI is enabled by default in the values.global.yaml and will be deployed in llmaz-system.\nopen-webui: enabled: true Set the Service Address Run kubectl get svc -n llmaz-system to list out the services, the output looks like below, the LoadBalancer service name will be used later.\nenvoy-default-default-envoy-ai-gateway-dbec795a LoadBalancer 10.96.145.150 \u003cpending\u003e 80:30548/TCP 132m envoy-gateway ClusterIP 10.96.52.76 \u003cnone\u003e 18000/TCP,18001/TCP,18002/TCP,19001/TCP 172m Port forward the Open-WebUI service, and visit http://localhost:8080.\nkubectl port-forward svc/open-webui 8080:80 -n llmaz-system Click Settings -\u003e Admin Settings -\u003e Connections, set the URL to http://envoy-default-default-envoy-ai-gateway-dbec795a.llmaz-system.svc.cluster.local/v1 and save. (You can also set the openaiBaseApiUrl in the values.global.yaml)\nStart to chat now. Persistence Set the persistence=true in values.global.yaml to enable persistence.\n","categories":"","description":"","excerpt":"Open WebUI is a user-friendly AI interface with OpenAI-compatible …","ref":"/docs/integrations/open-webui/","tags":"","title":"Open-WebUI"},{"body":"Project Structure llmaz # root ├── bin # where the binaries locates, like the kustomize, ginkgo, etc. ├── chart # where the helm chart locates ├── cmd # where the main entry locates ├── docs # where all the documents locate, like examples, installation guidance, etc. ├── llmaz # where the model loader logic locates ├── pkg # where the main logic for Kubernetes controllers locates API design Core APIs See the API Reference for more details.\nInference APIs See the API Reference for more details.\n","categories":"","description":"This section contains a develop guidance for people who want to learn more about this project.\n","excerpt":"This section contains a develop guidance for people who want to learn …","ref":"/docs/develop/","tags":"","title":"Develop Guidance"},{"body":"This document provides deployment steps to install and configure Prometheus Operator in a Kubernetes cluster.\nInstall the prometheus operator Please follow the documentation to install prometheus operator or simply run the following command:\ncurl -sL https://github.com/prometheus-operator/prometheus-operator/releases/download/v0.81.0/bundle.yaml | kubectl create -f - Ensure that the Prometheus Operator Pod is running successfully.\n# Installing the prometheus operator root@VM-0-5-ubuntu:/home/ubuntu# kubectl get pods NAME READY STATUS RESTARTS AGE prometheus-operator-55b5c96cf8-jl2nx 1/1 Running 0 12s Install the ServiceMonitor CR for llmaz To enable monitoring for the llmaz system, you need to install the ServiceMonitor custom resource (CR). You can either modify the Helm chart prometheus according to the documentation or use make install-prometheus in Makefile.\nUsing Helm Chart: to modify the values.global.yaml prometheus: # -- Whether to enable Prometheus metrics exporting. enable: true Using Makefile Command: make install-prometheus root@VM-0-5-ubuntu:/home/ubuntu/llmaz# make install-prometheus kubectl apply --server-side -k config/prometheus serviceaccount/llmaz-prometheus serverside-applied clusterrole.rbac.authorization.k8s.io/llmaz-prometheus serverside-applied clusterrolebinding.rbac.authorization.k8s.io/llmaz-prometheus serverside-applied prometheus.monitoring.coreos.com/llmaz-prometheus serverside-applied servicemonitor.monitoring.coreos.com/llmaz-controller-manager-metrics-monitor serverside-applied Check Related Resources Verify that the necessary resources have been created:\nServiceMonitor root@VM-0-5-ubuntu:/home/ubuntu/llmaz# kubectl get ServiceMonitor -n llmaz-system NAME AGE llmaz-controller-manager-metrics-monitor 59s Prometheus Pods root@VM-0-5-ubuntu:/home/ubuntu/llmaz# kubectl get pods -n llmaz-system NAME READY STATUS RESTARTS AGE llmaz-controller-manager-7ff8f7d9bd-vztls 2/2 Running 0 28s prometheus-llmaz-prometheus-0 2/2 Running 0 27s Services root@VM-0-5-ubuntu:/home/ubuntu/llmaz# kubectl get svc -n llmaz-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE llmaz-controller-manager-metrics-service ClusterIP 10.96.79.226 \u003cnone\u003e 8443/TCP 46s llmaz-webhook-service ClusterIP 10.96.249.226 \u003cnone\u003e 443/TCP 46s prometheus-operated ClusterIP None \u003cnone\u003e 9090/TCP 45s View metrics using the prometheus UI Use port forwarding to access the Prometheus UI from your local machine:\nroot@VM-0-5-ubuntu:/home/ubuntu# kubectl port-forward services/prometheus-operated 9090:9090 --address 0.0.0.0 -n llmaz-system Forwarding from 0.0.0.0:9090 -\u003e 9090 If using kind, we can use port-forward, kubectl port-forward services/prometheus-operated 39090:9090 --address 0.0.0.0 -n llmaz-system This allows us to access prometheus using a browser: http://localhost:9090/query\n","categories":"","description":"","excerpt":"This document provides deployment steps to install and configure …","ref":"/docs/integrations/prometheus-operator/","tags":"","title":"Prometheus Operator"},{"body":"","categories":"","description":"This section contains the llmaz reference information.\n","excerpt":"This section contains the llmaz reference information.\n","ref":"/docs/reference/","tags":"","title":"Reference"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/inference/","tags":"","title":"Inference"},{"body":"With the GPT series models shocking the world, a new era of AI innovation has begun. Besides the model training, because of the large model size and high computational cost, the inference process is also a challenge, not only the cost, but also the performance and efficiency. So when we look back to the late of 2023, we see lots of communities are building the inference engines, like the vLLM, TGI, LMDeploy and more others less well-known. But there still lacks a platform to provide an unified interface to serve LLM workloads in cloud and it should work smoothly with these inference engines. That’s the initial idea of llmaz. However, we didn’t start the work until middle of 2024 due to some unavoidable commitments. Anyway, today we are proud to announce the first minor release v0.1.0 of llmaz.\n💙 To make sure you will not leave with disappointments, we don’t have a lot of fancy features for v0.1.0, we just did a lot of dirty work to make sure it’s a workable solution, but we promise you, we will bring more exciting features in the near future.\nArchitecture First of all, let’s take a look at the architecture of llmaz: Basically, llmaz works as a platform on top of Kubernetes and provides an unified interface for various kinds of inference engines, it has four CRDs as defined:\nOpenModel: the model specification, which defines the model source, inference configurations and other metadata. It’s a cluster scoped resource. Playground: the facade to set the inference configurations, e.g. the model name, the replicas, the scaling policies, as simple as possible. It’s a namespace scoped resource. Inference Service: the full configurations for inference workload if Playground is not enough. Most of the time, you don’t need it. A Playground will create a Service automatically and it’s a namespace scoped resource. BackendRuntime: the backend runtime represents the actual inference engines, their images, resource requirements, together with their boot configurations. It’s a namespace scoped resource. With the abstraction of these CRDs, llmaz provides a simple way to deploy and manage the inference workloads, offering features like:\nEasy of Use: People can quick deploy a LLM service with minimal configurations. Broad Backends Support: llmaz supports a wide range of advanced inference backends for different scenarios, like vLLM, Text-Generation-Inference, SGLang, llama.cpp. Find the full list of supported backends here. Accelerator Fungibility: llmaz supports serving the same LLM with various accelerators to optimize cost and performance. SOTA Inference: llmaz supports the latest cutting-edge researches like Speculative Decoding to run on Kubernetes. Various Model Providers: llmaz supports a wide range of model providers, such as HuggingFace, ModelScope, ObjectStores. llmaz will automatically handle the model loading, requiring no effort from users. Multi-hosts Support: llmaz supports both single-host and multi-hosts scenarios from day 0. Scaling Efficiency: llmaz supports horizontal scaling with just 2-3 lines. With llmaz v0.1.0, all these features are available. Next, I’ll show you how to use llmaz.\nQuick Start Installation First, you need to install the llmaz with helm charts, be note that the helm chart version is different with the llmaz version, 0.0.6 is exactly the version of llmaz v0.1.0.\nhelm repo add inftyai https://inftyai.github.io/llmaz helm repo update helm install llmaz inftyai/llmaz --namespace llmaz-system --create-namespace --version 0.0.6 You can find more installation guides here like installing from source code.\nDeploy a Model Here’s the simplest way to deploy a model with llmaz.\nFirst, you need to deploy a model with specifications: apiVersion: llmaz.io/v1alpha1 kind: OpenModel metadata: name: opt-125m spec: familyName: opt source: modelHub: modelID: facebook/opt-125m inferenceConfig: flavors: - name: default requests: nvidia.com/gpu: 1 Then deploy a Playground: apiVersion: inference.llmaz.io/v1alpha1 kind: Playground metadata: name: opt-125m spec: replicas: 1 modelClaim: modelName: opt-125m # To use elasticConfig, you need to add scaleTriggers to backendRuntime, # if not, comment the elasticConfig here. elasticConfig: minReplicas: 1 maxReplicas: 3 That’s it! llmaz will launch a opt-125m service with the replicas ranging from 1 to 3. The service is served by vLLM by default.\nDesign Philosophy We believe that the complexity of the system should be hidden from the users, we have two main roles in our system, the user, and the platform runner.\nThe user, who wants to deploy a LLM model should not know too much details of the Kubernetes (although llmaz is also deployed on Kubernetes), the only thing they need to do is to provide the model name, and llmaz should take care of the rest.\nThat’s the reason why we have the Playground, it’s a facade to the inference workload with model name, replicas configurations, we shift the complexity to the BackendRuntime instead. If you take a look at the vLLM BackendRuntime, the configuration is really long.\napiVersion: inference.llmaz.io/v1alpha1 kind: BackendRuntime metadata: labels: app.kubernetes.io/name: backendruntime app.kubernetes.io/part-of: llmaz app.kubernetes.io/created-by: llmaz name: vllm spec: commands: - python3 - -m - vllm.entrypoints.openai.api_server multiHostCommands: leader: - sh - -c - | ray start --head --disable-usage-stats --include-dashboard false i=0 while true; do active_nodes=`python3 -c 'import ray; ray.init(); print(sum(node[\"Alive\"] for node in ray.nodes()))'` if [ $active_nodes -eq $(LWS_GROUP_SIZE) ]; then echo \"All ray workers are active and the ray cluster is initialized successfully.\" break fi if [ $i -eq 60 ]; then echo \"Initialization failed. Exiting...\" exit 1 fi echo \"Wait for $active_nodes/$(LWS_GROUP_SIZE) workers to be active.\" i=$((i+1)) sleep 5s; done python3 -m vllm.entrypoints.openai.api_server worker: - sh - -c - | i=0 while true; do ray start --address=$(LWS_LEADER_ADDRESS):6379 --block if [ $? -eq 0 ]; then echo \"Worker: Ray runtime started with head address $(LWS_LEADER_ADDRESS):6379\" break fi if [ $i -eq 60 ]; then echo \"Initialization failed. Exiting...\" exit 1 fi echo \"Waiting until the ray worker is active...\" sleep 5s; done image: vllm/vllm-openai version: v0.6.0 # Do not edit the preset argument name unless you know what you're doing. # Free to add more arguments with your requirements. args: - name: default flags: - --model - \"{{ .ModelPath }}\" - --served-model-name - \"{{ .ModelName }}\" - --host - \"0.0.0.0\" - --port - \"8080\" - name: speculative-decoding flags: - --model - \"{{ .ModelPath }}\" - --served-model-name - \"{{ .ModelName }}\" - --speculative_model - \"{{ .DraftModelPath }}\" - --host - \"0.0.0.0\" - --port - \"8080\" - --num_speculative_tokens - \"5\" - -tp - \"1\" - name: model-parallelism flags: - --model - \"{{ .ModelPath }}\" - --served-model-name - \"{{ .ModelName }}\" - --host - \"0.0.0.0\" - --port - \"8080\" - --tensor-parallel-size - \"{{ .TP }}\" - --pipeline-parallel-size - \"{{ .PP }}\" resources: requests: cpu: 4 memory: 8Gi limits: cpu: 4 memory: 8Gi startupProbe: periodSeconds: 10 failureThreshold: 30 httpGet: path: /health port: 8080 livenessProbe: initialDelaySeconds: 15 periodSeconds: 10 failureThreshold: 3 httpGet: path: /health port: 8080 readinessProbe: initialDelaySeconds: 5 periodSeconds: 5 failureThreshold: 3 httpGet: path: /health port: 8080 Basically, the BackendRuntime configures the boot commands, the resource requirements, the probes, all the stuff related to the inference engine, also part of the workload’s Pod yaml. We believe it’s workable for several reasons:\nUser may not be familiar with inference engines, the parameters are really verbose and complex, the vLLM has 209 parameters in total the day we write this blog. A preset configuration template is helpful in this case. On the other hand, the platform runner can help optimize the configurations, offering the best practices. User can still override the configurations if they want to, the llmaz will merge the configurations from the Playground and the BackendRuntime. User can provide their own BackendRuntime for extensibility as well and specify the backend name in the Playground for use. Regarding to the OpenModel, we think model should be the first citizen in the cloud management, who has lots of properties, like the source address, the inference configurations, the metadata, etc.. We believe it’s a good practice to separate the model from the inference workload, and we can reuse the model in different workloads.\nFor the long-term consideration, we may support model fine-tuning and model training in the future, so the OpenModel for serving is a good start.\nAnd we would like to highlight the inference configs of OpenModel, particularly the inference flavors, in cloud, we claim a Nvidia GPU with requests like nvidia.com/gpu: 1, this is not good enough because GPU chips have different series, like P4, T4, L40S, A100, H100, H200, they have different memory bandwidth and compute capability, even the same chip series may have different types like the A100 has the 40GB and 80GB, and we can’t tolerate to use low-end GPUs like the T4 to serve the SOFT models like llama3 405B or DeepSeek V3, so we need to specify the inference requirements in the model.\nHere, I demonstrate how to deploy the llama3 405B with flavors configured, it will first try to scheduler the Pods to the nodes with the label gpu.a100-80gb: true, if not, fallback to the nodes with label gpu.h100: true (this requires to install our new written scheduler plugin, we’ll reveal it in the following posts).\napiVersion: llmaz.io/v1alpha1 kind: OpenModel metadata: name: llama3-405b-instruct spec: familyName: llama3 source: modelHub: modelID: meta-llama/Llama-3.1-405B inferenceConfig: flavors: - name: a100-80gb requests: nvidia.com/gpu: 8 # single node request params: TP: \"8\" # 8 GPUs per node PP: \"2\" # 2 nodes nodeSelector: gpu.a100-80gb: true - name: h100 requests: nvidia.com/gpu: 8 # single node request params: TP: \"8\" PP: \"2\" nodeSelector: gpu.h100: true --- apiVersion: inference.llmaz.io/v1alpha1 kind: Playground metadata: name: llama3-405b-instruct spec: replicas: 1 modelClaim: modelName: llama3-405b-instruct backendRuntimeConfig: resources: requests: cpu: 4 memory: 8Gi limits: cpu: 4 memory: 16Gi Then llmaz will launch a multi-host inference service with 2 nodes, each node has 8 GPUs of A100 80GB/H100, the tensor parallelism is 8, the pipeline parallelism is 2, running by vLLM.\nRoamMap for V0.2.0 So this is our first minor release, as we mentioned, we did a lot of dirty work to make it easy to use, but we also left some unfinished work, especially the model distribution, this is a really pain-point, we have some on-going work but not ready for v0.1.0.\nSo here’s the roadmap for v0.2.0:\nModel Distribution: Advanced model loading like model sharding, model caching, model pre-fetching etc.. Observability: We’ll provide an out-of-the-box grafana dashboard for better monitoring. LLM-Focused Capacities: We will provide more LLM-focused improvements, like Lora aware, KV-cache aware loadbalancing, disaggregated serving, etc.. And it’s also great to have features like scale-to-zero serving, python SDK for code integration.\nFinally We would like to thank all the contributors who helped us to make this release happen, it’s really happy and grateful to have you all as a new open-source project.\nAnd we are looking forward to user feedbacks as well, if you’re interested with llmaz, feel free to have a try and if you have any problems or suggestions, don’t hesitate to contact us, open an issue or PR on our GitHub repository is also welcomed.\nLast but not least, don’t forget to 🌟️ our repository if you like it, it’s a great encouragement for us.\n","categories":["inference"],"description":"A brief introduction to llmaz and the features published in the first minor release v0.1.0.","excerpt":"A brief introduction to llmaz and the features published in the first …","ref":"/blog/2025/01/26/llmaz-a-new-inference-platform-for-llms-built-for-easy-to-use/","tags":["release-note"],"title":"llmaz, a new inference platform for LLMs built for easy to use"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/release-note/","tags":"","title":"Release-Note"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/","tags":"","title":"Blog"},{"body":"Welcome to llmaz llmaz (pronounced /lima:z/), aims to provide a Production-Ready inference platform for large language models on Kubernetes. It closely integrates with the state-of-the-art inference backends to bring the leading-edge researches to cloud.\nHigh Level Overview Architecture Read to get started? ","categories":"","description":"","excerpt":"Welcome to llmaz llmaz (pronounced /lima:z/), aims to provide a …","ref":"/docs/","tags":"","title":"Documentation"},{"body":" Learn More GitHub Easy, advanced inference platform for large language models on Kubernetes.\nKey Features\nEasy of Use People can quick deploy a LLM service with minimal configurations.\nBroad Backends Support llmaz supports a wide range of advanced inference backends for different scenarios, like vLLM, Text-Generation-Inference, SGLang, llama.cpp. Find the full list of supported backends here.\nHeterogeneous Cluster Support llmaz supports serving the same LLM with various accelerators to optimize cost and performance.\nVarious Model Providers llmaz supports a wide range of model providers, such as HuggingFace, ModelScope, ObjectStores. llmaz will automatically handle the model loading, requiring no effort from users.\nDistributed Serving Multi-host \u0026 homogeneous xPyD distributed serving support with LWS from day 0. Will implement the heterogeneous xPyD in the future.\nAI Gateway Support Offering capabilities like token-based rate limiting, model routing with the integration of Envoy AI Gateway.\nScaling Efficiency Horizontal Pod scaling with HPA based on LLM-focused metrics and node(spot instance) autoscaling with Karpenter.\nBuild-in ChatUI Out-of-the-box chatbot support with the integration of Open WebUI, offering capacities like function call, RAG, web search and more, see configurations here.\nMore in the future ","categories":"","description":"","excerpt":" Learn More GitHub Easy, advanced inference platform for large …","ref":"/","tags":"","title":"llmaz"},{"body":"Resource Types OpenModel OpenModel Appears in:\nOpenModel is the Schema for the open models API\nFieldDescription apiVersionstringllmaz.io/v1alpha1 kindstringOpenModel spec [Required] ModelSpec No description provided. status [Required] ModelStatus No description provided. Flavor Appears in:\nInferenceConfig Flavor defines the accelerator requirements for a model and the necessary parameters in autoscaling. Right now, it will be used in two places:\nPod scheduling with node selectors specified. Cluster autoscaling with essential parameters provided. FieldDescription name [Required] FlavorName Name represents the flavor name, which will be used in model claim.\nlimits k8s.io/api/core/v1.ResourceList Limits defines the required accelerators to serve the model for each replica, like \u003cnvidia.com/gpu: 8\u003e. For multi-hosts cases, the limits here indicates the resource requirements for each replica, usually equals to the TP size. Not recommended to set the cpu and memory usage here:\nif using playground, you can define the cpu/mem usage at backendConfig. if using inference service, you can define the cpu/mem at the container resources. However, if you define the same accelerator resources at playground/service as well, the resources will be overwritten by the flavor limit here. nodeSelector map[string]string NodeSelector represents the node candidates for Pod placements, if a node doesn't meet the nodeSelector, it will be filtered out in the resourceFungibility scheduler plugin. If nodeSelector is empty, it means every node is a candidate.\nparams map[string]string Params stores other useful parameters and will be consumed by cluster-autoscaler / Karpenter for autoscaling or be defined as model parallelism parameters like TP or PP size. E.g. with autoscaling, when scaling up nodes with 8x Nvidia A00, the parameter can be injected with \u003cINSTANCE-TYPE: p4d.24xlarge\u003e for AWS. Preset parameters: TP, PP, INSTANCE-TYPE.\nFlavorName (Alias of string)\nAppears in:\nFlavor InferenceConfig Appears in:\nModelSpec InferenceConfig represents the inference configurations for the model.\nFieldDescription flavors []Flavor Flavors represents the accelerator requirements to serve the model. Flavors are fungible following the priority represented by the slice order.\nModelHub Appears in:\nModelSource ModelHub represents the model registry for model downloads.\nFieldDescription name string Name refers to the model registry, such as huggingface.\nmodelID [Required] string ModelID refers to the model identifier on model hub, such as meta-llama/Meta-Llama-3-8B.\nfilename [Required] string Filename refers to a specified model file rather than the whole repo. This is helpful to download a specified GGUF model rather than downloading the whole repo which includes all kinds of quantized models. TODO: this is only supported with Huggingface, add support for ModelScope in the near future. Note: once filename is set, allowPatterns and ignorePatterns should be left unset.\nrevision string Revision refers to a Git revision id which can be a branch name, a tag, or a commit hash.\nallowPatterns []string AllowPatterns refers to files matched with at least one pattern will be downloaded.\nignorePatterns []string IgnorePatterns refers to files matched with any of the patterns will not be downloaded.\nModelName (Alias of string)\nAppears in:\nModelRef\nModelSpec\nModelRef Appears in:\nModelRef refers to a created Model with it's role.\nFieldDescription name [Required] ModelName Name represents the model name.\nrole ModelRole Role represents the model role once more than one model is required. Such as a draft role, which means running with SpeculativeDecoding, and default arguments for backend will be searched in backendRuntime with the name of speculative-decoding.\nModelRole (Alias of string)\nAppears in:\nModelRef ModelSource Appears in:\nModelSpec ModelSource represents the source of the model. Only one model source will be used.\nFieldDescription modelHub ModelHub ModelHub represents the model registry for model downloads.\nuri URIProtocol URI represents a various kinds of model sources following the uri protocol, protocol://, e.g.\noss://./ ollama://llama3.3 host:// ModelSpec Appears in:\nOpenModel ModelSpec defines the desired state of Model\nFieldDescription familyName [Required] ModelName FamilyName represents the model type, like llama2, which will be auto injected to the labels with the key of llmaz.io/model-family-name.\nsource [Required] ModelSource Source represents the source of the model, there're several ways to load the model such as loading from huggingface, OCI registry, s3, host path and so on.\ninferenceConfig [Required] InferenceConfig InferenceConfig represents the inference configurations for the model.\nownedBy string OwnedBy represents the owner of the running models serving by the backends, which will be exported as the field of \"OwnedBy\" in openai-compatible API \"/models\". Default to \"llmaz\" if not set.\ncreatedAt k8s.io/apimachinery/pkg/apis/meta/v1.Time CreatedAt represents the creation timestamp of the running models serving by the backends, which will be exported as the field of \"Created\" in openai-compatible API \"/models\". It follows the format of RFC 3339, for example \"2024-05-21T10:00:00Z\".\nModelStatus Appears in:\nOpenModel ModelStatus defines the observed state of Model\nFieldDescription conditions [Required] []k8s.io/apimachinery/pkg/apis/meta/v1.Condition Conditions represents the Inference condition.\nURIProtocol (Alias of string)\nAppears in:\nModelSource URIProtocol represents the protocol of the URI.\n","categories":"","description":"Generated API reference documentation for llmaz.io/v1alpha1.","excerpt":"Generated API reference documentation for llmaz.io/v1alpha1.","ref":"/docs/reference/core.v1alpha1/","tags":"","title":"llmaz core API"},{"body":"Resource Types Playground Service Playground Appears in:\nPlayground is the Schema for the playgrounds API\nFieldDescription apiVersionstringinference.llmaz.io/v1alpha1 kindstringPlayground spec [Required] PlaygroundSpec No description provided. status [Required] PlaygroundStatus No description provided. Service Appears in:\nService is the Schema for the services API\nFieldDescription apiVersionstringinference.llmaz.io/v1alpha1 kindstringService spec [Required] ServiceSpec No description provided. status [Required] ServiceStatus No description provided. BackendName (Alias of string)\nAppears in:\nBackendRuntimeConfig BackendRuntime Appears in:\nBackendRuntime is the Schema for the backendRuntime API\nFieldDescription spec [Required] BackendRuntimeSpec No description provided. status [Required] BackendRuntimeStatus No description provided. BackendRuntimeConfig Appears in:\nPlaygroundSpec FieldDescription backendName BackendName BackendName represents the inference backend under the hood, e.g. vLLM.\nversion string Version represents the backend version if you want a different one from the default version.\nenvs []k8s.io/api/core/v1.EnvVar Envs represents the environments set to the container.\nconfigName [Required] string ConfigName represents the recommended configuration name for the backend, It will be inferred from the models in the runtime if not specified, e.g. default, speculative-decoding.\nargs []string Args defined here will \"append\" the args defined in the recommendedConfig, either explicitly configured in configName or inferred in the runtime.\nresources ResourceRequirements Resources represents the resource requirements for backend, like cpu/mem, accelerators like GPU should not be defined here, but at the model flavors, or the values here will be overwritten. Resources defined here will \"overwrite\" the resources in the recommendedConfig.\nsharedMemorySize k8s.io/apimachinery/pkg/api/resource.Quantity SharedMemorySize represents the size of /dev/shm required in the runtime of inference workload. SharedMemorySize defined here will \"overwrite\" the sharedMemorySize in the recommendedConfig.\nBackendRuntimeSpec Appears in:\nBackendRuntime BackendRuntimeSpec defines the desired state of BackendRuntime\nFieldDescription command []string Command represents the default command for the backendRuntime.\nimage [Required] string Image represents the default image registry of the backendRuntime. It will work together with version to make up a real image.\nversion [Required] string Version represents the default version of the backendRuntime. It will be appended to the image as a tag.\nenvs []k8s.io/api/core/v1.EnvVar Envs represents the environments set to the container.\nlifecycle k8s.io/api/core/v1.Lifecycle Lifecycle represents hooks executed during the lifecycle of the container.\nlivenessProbe k8s.io/api/core/v1.Probe Periodic probe of backend liveness. Backend will be restarted if the probe fails. Cannot be updated.\nreadinessProbe k8s.io/api/core/v1.Probe Periodic probe of backend readiness. Backend will be removed from service endpoints if the probe fails.\nstartupProbe k8s.io/api/core/v1.Probe StartupProbe indicates that the Backend has successfully initialized. If specified, no other probes are executed until this completes successfully. If this probe fails, the backend will be restarted, just as if the livenessProbe failed. This can be used to provide different probe parameters at the beginning of a backend's lifecycle, when it might take a long time to load data or warm a cache, than during steady-state operation.\nrecommendedConfigs []RecommendedConfig RecommendedConfigs represents the recommended configurations for the backendRuntime.\nBackendRuntimeStatus Appears in:\nBackendRuntime BackendRuntimeStatus defines the observed state of BackendRuntime\nFieldDescription conditions [Required] []k8s.io/apimachinery/pkg/apis/meta/v1.Condition Conditions represents the Inference condition.\nElasticConfig Appears in:\nPlaygroundSpec FieldDescription minReplicas int32 MinReplicas indicates the minimum number of inference workloads based on the traffic. Default to 1. MinReplicas couldn't be 0 now, will support serverless in the future.\nmaxReplicas [Required] int32 MaxReplicas indicates the maximum number of inference workloads based on the traffic. Default to nil means there's no limit for the instance number.\nscaleTrigger ScaleTrigger ScaleTrigger defines the rules to scale the workloads. Only one trigger cloud work at a time, mostly used in Playground. ScaleTrigger defined here will \"overwrite\" the scaleTrigger in the recommendedConfig.\nHPATrigger Appears in:\nScaleTrigger HPATrigger represents the configuration of the HorizontalPodAutoscaler. Inspired by kubernetes.io/pkg/apis/autoscaling/types.go#HorizontalPodAutoscalerSpec. Note: HPA component should be installed in prior.\nFieldDescription metrics []k8s.io/api/autoscaling/v2.MetricSpec metrics contains the specifications for which to use to calculate the desired replica count (the maximum replica count across all metrics will be used). The desired replica count is calculated multiplying the ratio between the target value and the current value by the current number of pods. Ergo, metrics used must decrease as the pod count is increased, and vice-versa. See the individual metric source types for more information about how each type of metric must respond.\nbehavior k8s.io/api/autoscaling/v2.HorizontalPodAutoscalerBehavior behavior configures the scaling behavior of the target in both Up and Down directions (scaleUp and scaleDown fields respectively). If not set, the default HPAScalingRules for scale up and scale down are used.\nPlaygroundSpec Appears in:\nPlayground PlaygroundSpec defines the desired state of Playground\nFieldDescription replicas int32 Replicas represents the replica number of inference workloads.\nmodelClaim ModelClaim ModelClaim represents claiming for one model, it's a simplified use case of modelClaims. Most of the time, modelClaim is enough. ModelClaim and modelClaims are exclusive configured.\nmodelClaims ModelClaims ModelClaims represents claiming for multiple models for more complicated use cases like speculative-decoding. ModelClaims and modelClaim are exclusive configured.\nbackendRuntimeConfig BackendRuntimeConfig BackendRuntimeConfig represents the inference backendRuntime configuration under the hood, e.g. vLLM, which is the default backendRuntime.\nelasticConfig ElasticConfig ElasticConfig defines the configuration for elastic usage, e.g. the max/min replicas.\nPlaygroundStatus Appears in:\nPlayground PlaygroundStatus defines the observed state of Playground\nFieldDescription conditions [Required] []k8s.io/apimachinery/pkg/apis/meta/v1.Condition Conditions represents the Inference condition.\nreplicas [Required] int32 Replicas track the replicas that have been created, whether ready or not.\nselector [Required] string Selector points to the string form of a label selector which will be used by HPA.\nRecommendedConfig Appears in:\nBackendRuntimeSpec RecommendedConfig represents the recommended configurations for the backendRuntime, user can choose one of them to apply.\nFieldDescription name [Required] string Name represents the identifier of the config.\nargs []string Args represents all the arguments for the command. Argument around with {{ .CONFIG }} is a configuration waiting for render.\nresources ResourceRequirements Resources represents the resource requirements for backend, like cpu/mem, accelerators like GPU should not be defined here, but at the model flavors, or the values here will be overwritten.\nsharedMemorySize k8s.io/apimachinery/pkg/api/resource.Quantity SharedMemorySize represents the size of /dev/shm required in the runtime of inference workload.\nscaleTrigger ScaleTrigger ScaleTrigger defines the rules to scale the workloads. Only one trigger cloud work at a time.\nResourceRequirements Appears in:\nBackendRuntimeConfig\nRecommendedConfig\nTODO: Do not support DRA yet, we can support that once needed.\nFieldDescription limits k8s.io/api/core/v1.ResourceList Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\nrequests k8s.io/api/core/v1.ResourceList Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\nScaleTrigger Appears in:\nElasticConfig\nRecommendedConfig\nScaleTrigger defines the rules to scale the workloads. Only one trigger cloud work at a time, mostly used in Playground.\nFieldDescription hpa [Required] HPATrigger HPA represents the trigger configuration of the HorizontalPodAutoscaler.\nServiceSpec Appears in:\nService ServiceSpec defines the desired state of Service. Service controller will maintain multi-flavor of workloads with different accelerators for cost or performance considerations.\nFieldDescription modelClaims [Required] ModelClaims ModelClaims represents multiple claims for different models.\nreplicas int32 Replicas represents the replica number of inference workloads.\nworkloadTemplate [Required] sigs.k8s.io/lws/api/leaderworkerset/v1.LeaderWorkerTemplate WorkloadTemplate defines the template for leader/worker pods\nrolloutStrategy sigs.k8s.io/lws/api/leaderworkerset/v1.RolloutStrategy RolloutStrategy defines the strategy that will be applied to update replicas when a revision is made to the leaderWorkerTemplate.\nServiceStatus Appears in:\nService ServiceStatus defines the observed state of Service\nFieldDescription conditions [Required] []k8s.io/apimachinery/pkg/apis/meta/v1.Condition Conditions represents the Inference condition.\nreplicas [Required] int32 Replicas track the replicas that have been created, whether ready or not.\nselector [Required] string Selector points to the string form of a label selector, the HPA will be able to autoscale your resource.\n","categories":"","description":"Generated API reference documentation for inference.llmaz.io/v1alpha1.","excerpt":"Generated API reference documentation for inference.llmaz.io/v1alpha1.","ref":"/docs/reference/inference.v1alpha1/","tags":"","title":"llmaz inference API"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Search Results"}]