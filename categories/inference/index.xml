<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Inference on llmaz</title><link>https://llmaz.inftyai.com/categories/inference/</link><description>Recent content in Inference on llmaz</description><generator>Hugo</generator><language>en</language><lastBuildDate>Tue, 17 Jun 2025 18:06:02 +0800</lastBuildDate><atom:link href="https://llmaz.inftyai.com/categories/inference/index.xml" rel="self" type="application/rss+xml"/><item><title>llmaz, a new inference platform for LLMs built for easy to use</title><link>https://llmaz.inftyai.com/blog/2025/01/26/llmaz-a-new-inference-platform-for-llms-built-for-easy-to-use/</link><pubDate>Sun, 26 Jan 2025 15:00:00 +0800</pubDate><guid>https://llmaz.inftyai.com/blog/2025/01/26/llmaz-a-new-inference-platform-for-llms-built-for-easy-to-use/</guid><description>&lt;p>With the GPT series models shocking the world, a new era of AI innovation has begun. Besides the model training, because of the large model size and high computational cost, the inference process is also a challenge, not only the cost, but also the performance and efficiency. So when we look back to the late of 2023, we see lots of communities are building the inference engines, like the vLLM, TGI, LMDeploy and more others less well-known. But there still lacks a platform to provide an unified interface to serve LLM workloads in cloud and it should work smoothly with these inference engines. That&amp;rsquo;s the initial idea of llmaz. However, we didn&amp;rsquo;t start the work until middle of 2024 due to some unavoidable commitments. Anyway, today we are proud to announce the first minor release v0.1.0 of llmaz.&lt;/p></description></item></channel></rss>