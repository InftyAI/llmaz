apiVersion: inference.llmaz.io/v1alpha1
kind: BackendRuntime
metadata:
  labels:
    app.kubernetes.io/name: backendruntime
    app.kubernetes.io/part-of: llmaz
    app.kubernetes.io/created-by: llmaz
  name: llamacpp
spec:
  commands:
    - ./llama-server
  image: ghcr.io/ggerganov/llama.cpp
  version: server
  # Do not edit the preset argument name unless you know what you're doing.
  # Free to add more arguments with your requirements.
  recommendedConfigs:
    - name: default
      args:
        - -m
        - "{{ .ModelPath }}"
        - --host
        - "0.0.0.0"
        - --port
        - "8080"
      resources:
        requests:
          cpu: 2
          memory: 4Gi
        limits:
          cpu: 2
          memory: 4Gi
    # TODO: not supported yet, see https://github.com/InftyAI/llmaz/issues/240.
    # - name: speculative-decoding
    #   args:
    #     - -m
    #     - "{{ .ModelPath }}"
    #     - -md
    #     - "{{ .DraftModelPath }}"
    #     - --host
    #     - "0.0.0.0"
    #     - --port
    #     - "8080"
    #     - --draft-max
    #     - "16"
    #     - --draft-min
    #     - "5"
  startupProbe:
    periodSeconds: 10
    failureThreshold: 30
    httpGet:
      path: /health
      port: 8080
  livenessProbe:
    initialDelaySeconds: 15
    periodSeconds: 10
    failureThreshold: 3
    httpGet:
      path: /health
      port: 8080
  readinessProbe:
    initialDelaySeconds: 5
    periodSeconds: 5
    failureThreshold: 3
    httpGet:
      path: /health
      port: 8080
