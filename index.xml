<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>llmaz</title><link>https://llmaz.inftyai.com/</link><description>Recent content on llmaz</description><generator>Hugo</generator><language>en</language><atom:link href="https://llmaz.inftyai.com/index.xml" rel="self" type="application/rss+xml"/><item><title>Envoy AI Gateway</title><link>https://llmaz.inftyai.com/docs/integrations/envoy-ai-gateway/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/integrations/envoy-ai-gateway/</guid><description>&lt;p>&lt;a href="https://aigateway.envoyproxy.io/">Envoy AI Gateway&lt;/a> is an open source project for using Envoy Gateway
to handle request traffic from application clients to Generative AI services.&lt;/p>
&lt;h2 id="how-to-use">How to use&lt;/h2>
&lt;h3 id="1-enable-envoy-gateway-and-envoy-ai-gateway">1. Enable Envoy Gateway and Envoy AI Gateway&lt;/h3>
&lt;p>Both of them are enabled by default in &lt;code>values.global.yaml&lt;/code> and will be deployed in llmaz-system.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">envoy-gateway&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">enabled&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">true&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">envoy-ai-gateway&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">enabled&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">true&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>However, &lt;a href="https://gateway.envoyproxy.io/latest/install/install-helm/">Envoy Gateway&lt;/a> and &lt;a href="https://aigateway.envoyproxy.io/docs/getting-started/">Envoy AI Gateway&lt;/a> can be deployed standalone in case you want to deploy them in other namespaces.&lt;/p></description></item><item><title>Installation</title><link>https://llmaz.inftyai.com/docs/installation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/installation/</guid><description>&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;p>&lt;strong>Requirements&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Kubernetes version &amp;gt;= 1.27.&lt;/p>
&lt;p>LWS requires Kubernetes version &lt;strong>v1.27 or higher&lt;/strong>. If you are using a lower Kubernetes version and most of your workloads rely on single-node inference, we may consider replacing LWS with a Deployment-based approach. This fallback plan would involve using Kubernetes Deployments to manage single-node inference workloads efficiently. See &lt;a href="https://github.com/InftyAI/llmaz/issues/32">#32&lt;/a> for more details and updates.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Helm 3, see &lt;a href="https://helm.sh/docs/intro/install/">installation&lt;/a>.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Note that llmaz helm chart will by default install:&lt;/p></description></item><item><title>Open WebUI</title><link>https://llmaz.inftyai.com/docs/integrations/open-webui/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/integrations/open-webui/</guid><description>&lt;p>&lt;a href="https://github.com/open-webui/open-webui">Open WebUI&lt;/a> is a user-friendly AI interface with OpenAI-compatible APIs, serving as the default chatbot for llmaz.&lt;/p>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;ul>
&lt;li>Make sure you&amp;rsquo;re located in &lt;strong>llmaz-system&lt;/strong> namespace, haven&amp;rsquo;t tested with other namespaces.&lt;/li>
&lt;li>Make sure &lt;a href="https://github.com/envoyproxy/gateway">EnvoyGateway&lt;/a> and &lt;a href="https://github.com/envoyproxy/ai-gateway">Envoy AI Gateway&lt;/a> are installed, both of them are installed by default in llmaz. See &lt;a href="docs/envoy-ai-gateway.md">AI Gateway&lt;/a> for more details.&lt;/li>
&lt;/ul>
&lt;h2 id="how-to-use">How to use&lt;/h2>
&lt;p>If open-webui already installed, what you need to do is just update the OpenAI API endpoint in the admin settings if you deployed the &lt;a href="docs/envoy-ai-gateway.md">Basic AI Gateway Example&lt;/a> to a namespace other than &amp;ldquo;default&amp;rdquo;. You can get the value from step2 &amp;amp; 3 below. Otherwise, following the steps here to install open-webui.&lt;/p></description></item><item><title>Develop Guidance</title><link>https://llmaz.inftyai.com/docs/develop/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/develop/</guid><description>&lt;h2 id="project-structure">Project Structure&lt;/h2>
&lt;pre tabindex="0">&lt;code class="language-structure" data-lang="structure">llmaz # root
├── bin # where the binaries locates, like the kustomize, ginkgo, etc.
├── chart # where the helm chart locates
├── cmd # where the main entry locates
├── docs # where all the documents locate, like examples, installation guidance, etc.
├── llmaz # where the model loader logic locates
├── pkg # where the main logic for Kubernetes controllers locates
&lt;/code>&lt;/pre>&lt;h2 id="api-design">API design&lt;/h2>
&lt;h3 id="core-apis">Core APIs&lt;/h3>
&lt;p>See the &lt;a href="https://llmaz.inftyai.com/docs/reference/core.v1alpha1/">API Reference&lt;/a> for more details.&lt;/p></description></item><item><title>Prometheus Operator</title><link>https://llmaz.inftyai.com/docs/integrations/prometheus-operator/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/integrations/prometheus-operator/</guid><description>&lt;p>This document provides deployment steps to install and configure Prometheus Operator in a Kubernetes cluster.&lt;/p>
&lt;h3 id="install-the-prometheus-operator">Install the prometheus operator&lt;/h3>
&lt;p>Please follow the &lt;a href="https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/getting-started/installation.md">documentation&lt;/a> to install prometheus operator or simply run the following command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>curl -sL https://github.com/prometheus-operator/prometheus-operator/releases/download/v0.81.0/bundle.yaml &lt;span style="color:#000;font-weight:bold">|&lt;/span> kubectl delete -f -
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Ensure that the Prometheus Operator Pod is running successfully.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic"># Installing the prometheus operator&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>root@VM-0-5-ubuntu:/home/ubuntu# kubectl get pods
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>prometheus-operator-55b5c96cf8-jl2nx 1/1 Running &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> 12s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="install-the-servicemonitor-cr-for-llmaz">Install the ServiceMonitor CR for llmaz&lt;/h3>
&lt;p>To enable monitoring for the llmaz system, you need to install the ServiceMonitor custom resource (CR).
You can either modify the Helm chart prometheus according to the &lt;a href="https://github.com/InftyAI/llmaz/blob/main/chart/values.global.yaml">documentation&lt;/a> or use &lt;code>make install-prometheus&lt;/code> in Makefile.&lt;/p></description></item><item><title>Supported Inference Backends</title><link>https://llmaz.inftyai.com/docs/integrations/support-backends/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/integrations/support-backends/</guid><description>&lt;p>If you want to integrate more backends into llmaz, please refer to this &lt;a href="https://github.com/InftyAI/llmaz/pull/182">PR&lt;/a>. It&amp;rsquo;s always welcomed.&lt;/p>
&lt;h2 id="llamacpp">llama.cpp&lt;/h2>
&lt;p>&lt;a href="https://github.com/ggerganov/llama.cpp">llama.cpp&lt;/a> is to enable LLM inference with minimal setup and state-of-the-art performance on a wide variety of hardware - locally and in the cloud.&lt;/p>
&lt;h2 id="sglang">SGLang&lt;/h2>
&lt;p>&lt;a href="https://github.com/sgl-project/sglang">SGLang&lt;/a> is yet another fast serving framework for large language models and vision language models.&lt;/p>
&lt;h2 id="tensorrt-llm">TensorRT-LLM&lt;/h2>
&lt;p>&lt;a href="https://github.com/NVIDIA/TensorRT-LLM">TensorRT-LLM&lt;/a> provides users with an easy-to-use Python API to define Large Language Models (LLMs) and support state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that orchestrate the inference execution in performant way.&lt;/p></description></item><item><title>llmaz core API</title><link>https://llmaz.inftyai.com/docs/reference/core.v1alpha1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/reference/core.v1alpha1/</guid><description>&lt;h2 id="resource-types">Resource Types&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://llmaz.inftyai.com/docs/reference/core.v1alpha1/#llmaz-io-v1alpha1-OpenModel">OpenModel&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="llmaz-io-v1alpha1-OpenModel">&lt;code>OpenModel&lt;/code> &lt;/h2>
&lt;p>&lt;strong>Appears in:&lt;/strong>&lt;/p>
&lt;p>OpenModel is the Schema for the open models API&lt;/p>
&lt;table class="table">
&lt;thead>&lt;tr>&lt;th width="30%">Field&lt;/th>&lt;th>Description&lt;/th>&lt;/tr>&lt;/thead>
&lt;tbody>
&lt;tr>&lt;td>&lt;code>apiVersion&lt;/code>&lt;br/>string&lt;/td>&lt;td>&lt;code>llmaz.io/v1alpha1&lt;/code>&lt;/td>&lt;/tr>
&lt;tr>&lt;td>&lt;code>kind&lt;/code>&lt;br/>string&lt;/td>&lt;td>&lt;code>OpenModel&lt;/code>&lt;/td>&lt;/tr>
&lt;tr>&lt;td>&lt;code>spec&lt;/code> &lt;B>[Required]&lt;/B>&lt;br/>
&lt;a href="#llmaz-io-v1alpha1-ModelSpec">&lt;code>ModelSpec&lt;/code>&lt;/a>
&lt;/td>
&lt;td>
 &lt;span class="text-muted">No description provided.&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>&lt;td>&lt;code>status&lt;/code> &lt;B>[Required]&lt;/B>&lt;br/>
&lt;a href="#llmaz-io-v1alpha1-ModelStatus">&lt;code>ModelStatus&lt;/code>&lt;/a>
&lt;/td>
&lt;td>
 &lt;span class="text-muted">No description provided.&lt;/span>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="llmaz-io-v1alpha1-Flavor">&lt;code>Flavor&lt;/code> &lt;/h2>
&lt;p>&lt;strong>Appears in:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://llmaz.inftyai.com/docs/reference/core.v1alpha1/#llmaz-io-v1alpha1-InferenceConfig">InferenceConfig&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Flavor defines the accelerator requirements for a model and the necessary parameters
in autoscaling. Right now, it will be used in two places:&lt;/p>
&lt;ul>
&lt;li>Pod scheduling with node selectors specified.&lt;/li>
&lt;li>Cluster autoscaling with essential parameters provided.&lt;/li>
&lt;/ul>
&lt;table class="table">
&lt;thead>&lt;tr>&lt;th width="30%">Field&lt;/th>&lt;th>Description&lt;/th>&lt;/tr>&lt;/thead>
&lt;tbody>
&lt;tr>&lt;td>&lt;code>name&lt;/code> &lt;B>[Required]&lt;/B>&lt;br/>
&lt;a href="#llmaz-io-v1alpha1-FlavorName">&lt;code>FlavorName&lt;/code>&lt;/a>
&lt;/td>
&lt;td>
 &lt;p>Name represents the flavor name, which will be used in model claim.&lt;/p></description></item><item><title>llmaz inference API</title><link>https://llmaz.inftyai.com/docs/reference/inference.v1alpha1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/reference/inference.v1alpha1/</guid><description>&lt;h2 id="resource-types">Resource Types&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://llmaz.inftyai.com/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-Playground">Playground&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://llmaz.inftyai.com/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-Service">Service&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="inference-llmaz-io-v1alpha1-Playground">&lt;code>Playground&lt;/code> &lt;/h2>
&lt;p>&lt;strong>Appears in:&lt;/strong>&lt;/p>
&lt;p>Playground is the Schema for the playgrounds API&lt;/p>
&lt;table class="table">
&lt;thead>&lt;tr>&lt;th width="30%">Field&lt;/th>&lt;th>Description&lt;/th>&lt;/tr>&lt;/thead>
&lt;tbody>
&lt;tr>&lt;td>&lt;code>apiVersion&lt;/code>&lt;br/>string&lt;/td>&lt;td>&lt;code>inference.llmaz.io/v1alpha1&lt;/code>&lt;/td>&lt;/tr>
&lt;tr>&lt;td>&lt;code>kind&lt;/code>&lt;br/>string&lt;/td>&lt;td>&lt;code>Playground&lt;/code>&lt;/td>&lt;/tr>
&lt;tr>&lt;td>&lt;code>spec&lt;/code> &lt;B>[Required]&lt;/B>&lt;br/>
&lt;a href="#inference-llmaz-io-v1alpha1-PlaygroundSpec">&lt;code>PlaygroundSpec&lt;/code>&lt;/a>
&lt;/td>
&lt;td>
 &lt;span class="text-muted">No description provided.&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>&lt;td>&lt;code>status&lt;/code> &lt;B>[Required]&lt;/B>&lt;br/>
&lt;a href="#inference-llmaz-io-v1alpha1-PlaygroundStatus">&lt;code>PlaygroundStatus&lt;/code>&lt;/a>
&lt;/td>
&lt;td>
 &lt;span class="text-muted">No description provided.&lt;/span>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="inference-llmaz-io-v1alpha1-Service">&lt;code>Service&lt;/code> &lt;/h2>
&lt;p>&lt;strong>Appears in:&lt;/strong>&lt;/p>
&lt;p>Service is the Schema for the services API&lt;/p>
&lt;table class="table">
&lt;thead>&lt;tr>&lt;th width="30%">Field&lt;/th>&lt;th>Description&lt;/th>&lt;/tr>&lt;/thead>
&lt;tbody>
&lt;tr>&lt;td>&lt;code>apiVersion&lt;/code>&lt;br/>string&lt;/td>&lt;td>&lt;code>inference.llmaz.io/v1alpha1&lt;/code>&lt;/td>&lt;/tr>
&lt;tr>&lt;td>&lt;code>kind&lt;/code>&lt;br/>string&lt;/td>&lt;td>&lt;code>Service&lt;/code>&lt;/td>&lt;/tr>
&lt;tr>&lt;td>&lt;code>spec&lt;/code> &lt;B>[Required]&lt;/B>&lt;br/>
&lt;a href="#inference-llmaz-io-v1alpha1-ServiceSpec">&lt;code>ServiceSpec&lt;/code>&lt;/a>
&lt;/td>
&lt;td>
 &lt;span class="text-muted">No description provided.&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>&lt;td>&lt;code>status&lt;/code> &lt;B>[Required]&lt;/B>&lt;br/>
&lt;a href="#inference-llmaz-io-v1alpha1-ServiceStatus">&lt;code>ServiceStatus&lt;/code>&lt;/a>
&lt;/td>
&lt;td>
 &lt;span class="text-muted">No description provided.&lt;/span>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="inference-llmaz-io-v1alpha1-BackendName">&lt;code>BackendName&lt;/code> &lt;/h2>
&lt;p>(Alias of &lt;code>string&lt;/code>)&lt;/p>
&lt;p>&lt;strong>Appears in:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://llmaz.inftyai.com/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-BackendRuntimeConfig">BackendRuntimeConfig&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="inference-llmaz-io-v1alpha1-BackendRuntime">&lt;code>BackendRuntime&lt;/code> &lt;/h2>
&lt;p>&lt;strong>Appears in:&lt;/strong>&lt;/p>
&lt;p>BackendRuntime is the Schema for the backendRuntime API&lt;/p></description></item><item><title>Search Results</title><link>https://llmaz.inftyai.com/search/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/search/</guid><description/></item></channel></rss>