<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>llmaz</title><link>https://llmaz.inftyai.com/</link><description>Recent content on llmaz</description><generator>Hugo</generator><language>en</language><atom:link href="https://llmaz.inftyai.com/index.xml" rel="self" type="application/rss+xml"/><item><title>Broad Inference Backends Support</title><link>https://llmaz.inftyai.com/docs/features/broad-backends/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/features/broad-backends/</guid><description>&lt;p>If you want to integrate more backends into llmaz, please refer to this &lt;a href="https://github.com/InftyAI/llmaz/pull/182">PR&lt;/a>. It&amp;rsquo;s always welcomed.&lt;/p>
&lt;h2 id="llamacpp">llama.cpp&lt;/h2>
&lt;p>&lt;a href="https://github.com/ggerganov/llama.cpp">llama.cpp&lt;/a> is to enable LLM inference with minimal setup and state-of-the-art performance on a wide variety of hardware - locally and in the cloud.&lt;/p>
&lt;h2 id="ollama">ollama&lt;/h2>
&lt;p>&lt;a href="https://github.com/ollama/ollama">ollama&lt;/a> is running with Llama 3.2, Mistral, Gemma 2, and other large language models, based on llama.cpp, aims for local deploy.&lt;/p>
&lt;h2 id="sglang">SGLang&lt;/h2>
&lt;p>&lt;a href="https://github.com/sgl-project/sglang">SGLang&lt;/a> is yet another fast serving framework for large language models and vision language models.&lt;/p></description></item><item><title>Envoy AI Gateway</title><link>https://llmaz.inftyai.com/docs/integrations/envoy-ai-gateway/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/integrations/envoy-ai-gateway/</guid><description>&lt;p>&lt;a href="https://aigateway.envoyproxy.io/">Envoy AI Gateway&lt;/a> is an open source project for using Envoy Gateway
to handle request traffic from application clients to Generative AI services.&lt;/p>
&lt;h2 id="how-to-use">How to use&lt;/h2>
&lt;h3 id="enable-envoy-gateway-and-envoy-ai-gateway">Enable Envoy Gateway and Envoy AI Gateway&lt;/h3>
&lt;p>Both of them are already enabled by default in &lt;code>values.global.yaml&lt;/code> and will be deployed in llmaz-system.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">envoy-gateway&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">enabled&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">true&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">envoy-ai-gateway&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">enabled&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">true&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>However, &lt;a href="https://gateway.envoyproxy.io/latest/install/install-helm/">Envoy Gateway&lt;/a> and &lt;a href="https://aigateway.envoyproxy.io/docs/getting-started/">Envoy AI Gateway&lt;/a> can be deployed standalone in case you want to deploy them in other namespaces.&lt;/p></description></item><item><title>Prerequisites</title><link>https://llmaz.inftyai.com/docs/getting-started/prerequisites/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/getting-started/prerequisites/</guid><description>&lt;p>&lt;strong>Requirements&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Kubernetes version &amp;gt;= 1.27.&lt;/p>
&lt;p>LWS requires Kubernetes version &lt;strong>v1.27 or higher&lt;/strong>. If you are using a lower Kubernetes version and most of your workloads rely on single-node inference, we may consider replacing LWS with a Deployment-based approach. This fallback plan would involve using Kubernetes Deployments to manage single-node inference workloads efficiently. See &lt;a href="https://github.com/InftyAI/llmaz/issues/32">#32&lt;/a> for more details and updates.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Helm 3, see &lt;a href="https://helm.sh/docs/intro/install/">installation&lt;/a>.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Note that llmaz helm chart will by default install:&lt;/p></description></item><item><title>Heterogeneous Cluster Support</title><link>https://llmaz.inftyai.com/docs/features/heterogeneous-cluster-support/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/features/heterogeneous-cluster-support/</guid><description>&lt;p>A &lt;code>llama2-7B&lt;/code> model can be running on &lt;strong>1xA100&lt;/strong> GPU, also on &lt;strong>1xA10&lt;/strong> GPU, even on &lt;strong>1x4090&lt;/strong> and a variety of other types of GPUs as well, that&amp;rsquo;s what we called resource fungibility. In practical scenarios, we may have a heterogeneous cluster with different GPU types, and high-end GPUs will stock out a lot, to meet the SLOs of the service as well as the cost, we need to schedule the workloads on different GPU types. With the &lt;a href="https://github.com/InftyAI/scheduler-plugins/blob/main/pkg/plugins/resource_fungibility">ResourceFungibility&lt;/a> in the InftyAI scheduler, we can simply achieve this with at most 8 alternative GPU types.&lt;/p></description></item><item><title>Installation</title><link>https://llmaz.inftyai.com/docs/getting-started/installation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/getting-started/installation/</guid><description>&lt;h2 id="install-a-released-version-recommended">Install a released version (recommended)&lt;/h2>
&lt;h3 id="install">Install&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-cmd" data-lang="cmd">&lt;span style="display:flex;">&lt;span>helm install llmaz oci://registry-1.docker.io/inftyai/llmaz --namespace llmaz-system --create-namespace --version 0.0.10
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="uninstall">Uninstall&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-cmd" data-lang="cmd">&lt;span style="display:flex;">&lt;span>helm uninstall llmaz --namespace llmaz-system
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl delete ns llmaz-system
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you want to delete the CRDs as well, run&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-cmd" data-lang="cmd">&lt;span style="display:flex;">&lt;span>kubectl delete crd \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> openmodels.llmaz.io \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> backendruntimes.inference.llmaz.io \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> playgrounds.inference.llmaz.io \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> services.inference.llmaz.io
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="install-from-source">Install from source&lt;/h2>
&lt;h3 id="change-configurations">Change configurations&lt;/h3>
&lt;p>If you want to change the default configurations, please change the values in &lt;a href="https://github.com/InftyAI/llmaz/blob/main/chart/values.global.yaml">values.global.yaml&lt;/a>.&lt;/p>
&lt;p>&lt;strong>Do not change&lt;/strong> the values in &lt;em>values.yaml&lt;/em> because it&amp;rsquo;s auto-generated and will be overwritten.&lt;/p></description></item><item><title>Karpenter</title><link>https://llmaz.inftyai.com/docs/integrations/karpenter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/integrations/karpenter/</guid><description>&lt;p>&lt;a href="https://github.com/kubernetes-sigs/karpenter">Karpenter&lt;/a> automatically launches just the right compute resources to handle your cluster&amp;rsquo;s applications, but it is built to adhere to the scheduling decisions of kube-scheduler, so it&amp;rsquo;s certainly possible we would run across some cases where Karpenter makes incorrect decisions when the InftyAI scheduler is in the mix.&lt;/p>
&lt;p>We forked the Karpenter project and re-complie the karpenter image for cloud providers like AWS, and you can find the details in &lt;a href="https://github.com/InftyAI/llmaz/blob/main/docs/proposals/106-spot-instance-karpenter/README.md">this proposal&lt;/a>. This document provides deployment steps to install and configure Customized Karpenter in an EKS cluster.&lt;/p></description></item><item><title>Basic Usage</title><link>https://llmaz.inftyai.com/docs/getting-started/basic-usage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/getting-started/basic-usage/</guid><description>&lt;p>Let&amp;rsquo;s assume that you have installed the llmaz with the default settings, which means both the &lt;a href="https://llmaz.inftyai.com/docs/integrations/envoy-ai-gateway/">AI Gateway&lt;/a> and &lt;a href="https://llmaz.inftyai.com/docs/integrations/open-webui/">Open WebUI&lt;/a> are installed. Now let&amp;rsquo;s following the steps to chat with your models.&lt;/p>
&lt;h3 id="deploy-the-services">Deploy the Services&lt;/h3>
&lt;p>Run the following command to deploy two models (cpu only).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl apply -f https://raw.githubusercontent.com/InftyAI/llmaz/refs/heads/main/docs/examples/envoy-ai-gateway/basic.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="chat-with-models">Chat with Models&lt;/h3>
&lt;p>Waiting for your services ready, generally looks like:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ai-eg-route-extproc-default-envoy-ai-gateway-6ddcd49b64-ldwcd 1/1 Running &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> 6m37s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>qwen2--5-coder-0 1/1 Running &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> 6m37s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>qwen2-0--5b-0 1/1 Running &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> 6m37s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Once ready, you can access the Open WebUI by port-forwarding the service:&lt;/p></description></item><item><title>Distributed Inference</title><link>https://llmaz.inftyai.com/docs/features/distributed_inference/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/features/distributed_inference/</guid><description>&lt;p>Support multi-host &amp;amp; homogeneous xPyD distributed serving with &lt;a href="https://github.com/kubernetes-sigs/lws">LWS&lt;/a> from day 0. Will implement the heterogeneous xPyD in the future.&lt;/p></description></item><item><title>Open-WebUI</title><link>https://llmaz.inftyai.com/docs/integrations/open-webui/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/integrations/open-webui/</guid><description>&lt;p>&lt;a href="https://github.com/open-webui/open-webui">Open WebUI&lt;/a> is a user-friendly AI interface with OpenAI-compatible APIs, serving as the default chatbot for llmaz.&lt;/p>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;ul>
&lt;li>Make sure &lt;a href="https://github.com/envoyproxy/gateway">EnvoyGateway&lt;/a> and &lt;a href="https://github.com/envoyproxy/ai-gateway">Envoy AI Gateway&lt;/a> are installed, both of them are installed by default in llmaz. See &lt;a href="docs/envoy-ai-gateway.md">AI Gateway&lt;/a> for more details.&lt;/li>
&lt;/ul>
&lt;h2 id="how-to-use">How to use&lt;/h2>
&lt;h3 id="enable-open-webui">Enable Open WebUI&lt;/h3>
&lt;p>Open-WebUI is enabled by default in the &lt;code>values.global.yaml&lt;/code> and will be deployed in llmaz-system.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">open-webui&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">enabled&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">true&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="set-the-service-address">Set the Service Address&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>Run &lt;code>kubectl get svc -n llmaz-system&lt;/code> to list out the services, the output looks like below, the LoadBalancer service name will be used later.&lt;/p></description></item><item><title>Develop Guidance</title><link>https://llmaz.inftyai.com/docs/develop/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/develop/</guid><description>&lt;h2 id="project-structure">Project Structure&lt;/h2>
&lt;pre tabindex="0">&lt;code class="language-structure" data-lang="structure">llmaz # root
├── bin # where the binaries locates, like the kustomize, ginkgo, etc.
├── chart # where the helm chart locates
├── cmd # where the main entry locates
├── docs # where all the documents locate, like examples, installation guidance, etc.
├── llmaz # where the model loader logic locates
├── pkg # where the main logic for Kubernetes controllers locates
&lt;/code>&lt;/pre>&lt;h2 id="api-design">API design&lt;/h2>
&lt;h3 id="core-apis">Core APIs&lt;/h3>
&lt;p>See the &lt;a href="https://llmaz.inftyai.com/docs/reference/core.v1alpha1/">API Reference&lt;/a> for more details.&lt;/p></description></item><item><title>Prometheus Operator</title><link>https://llmaz.inftyai.com/docs/integrations/prometheus-operator/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/integrations/prometheus-operator/</guid><description>&lt;p>This document provides deployment steps to install and configure Prometheus Operator in a Kubernetes cluster.&lt;/p>
&lt;h3 id="install-the-prometheus-operator">Install the prometheus operator&lt;/h3>
&lt;p>Please follow the &lt;a href="https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/getting-started/installation.md">documentation&lt;/a> to install prometheus operator or simply run the following command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>curl -sL https://github.com/prometheus-operator/prometheus-operator/releases/download/v0.81.0/bundle.yaml &lt;span style="color:#000;font-weight:bold">|&lt;/span> kubectl create -f -
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Ensure that the Prometheus Operator Pod is running successfully.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic"># Installing the prometheus operator&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>root@VM-0-5-ubuntu:/home/ubuntu# kubectl get pods
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>prometheus-operator-55b5c96cf8-jl2nx 1/1 Running &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> 12s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="install-the-servicemonitor-cr-for-llmaz">Install the ServiceMonitor CR for llmaz&lt;/h3>
&lt;p>To enable monitoring for the llmaz system, you need to install the ServiceMonitor custom resource (CR).
You can either modify the Helm chart prometheus according to the &lt;a href="https://github.com/InftyAI/llmaz/blob/main/chart/values.global.yaml">documentation&lt;/a> or use &lt;code>make install-prometheus&lt;/code> in Makefile.&lt;/p></description></item><item><title>llmaz, a new inference platform for LLMs built for easy to use</title><link>https://llmaz.inftyai.com/blog/2025/01/26/llmaz-a-new-inference-platform-for-llms-built-for-easy-to-use/</link><pubDate>Sun, 26 Jan 2025 15:00:00 +0800</pubDate><guid>https://llmaz.inftyai.com/blog/2025/01/26/llmaz-a-new-inference-platform-for-llms-built-for-easy-to-use/</guid><description>&lt;p>With the GPT series models shocking the world, a new era of AI innovation has begun. Besides the model training, because of the large model size and high computational cost, the inference process is also a challenge, not only the cost, but also the performance and efficiency. So when we look back to the late of 2023, we see lots of communities are building the inference engines, like the vLLM, TGI, LMDeploy and more others less well-known. But there still lacks a platform to provide an unified interface to serve LLM workloads in cloud and it should work smoothly with these inference engines. That&amp;rsquo;s the initial idea of llmaz. However, we didn&amp;rsquo;t start the work until middle of 2024 due to some unavoidable commitments. Anyway, today we are proud to announce the first minor release v0.1.0 of llmaz.&lt;/p></description></item><item><title>llmaz core API</title><link>https://llmaz.inftyai.com/docs/reference/core.v1alpha1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/reference/core.v1alpha1/</guid><description>&lt;h2 id="resource-types">Resource Types&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://llmaz.inftyai.com/docs/reference/core.v1alpha1/#llmaz-io-v1alpha1-OpenModel">OpenModel&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="llmaz-io-v1alpha1-OpenModel">&lt;code>OpenModel&lt;/code> &lt;/h2>
&lt;p>&lt;strong>Appears in:&lt;/strong>&lt;/p>
&lt;p>OpenModel is the Schema for the open models API&lt;/p>
&lt;table class="table">
&lt;thead>&lt;tr>&lt;th width="30%">Field&lt;/th>&lt;th>Description&lt;/th>&lt;/tr>&lt;/thead>
&lt;tbody>
&lt;tr>&lt;td>&lt;code>apiVersion&lt;/code>&lt;br/>string&lt;/td>&lt;td>&lt;code>llmaz.io/v1alpha1&lt;/code>&lt;/td>&lt;/tr>
&lt;tr>&lt;td>&lt;code>kind&lt;/code>&lt;br/>string&lt;/td>&lt;td>&lt;code>OpenModel&lt;/code>&lt;/td>&lt;/tr>
&lt;tr>&lt;td>&lt;code>spec&lt;/code> &lt;B>[Required]&lt;/B>&lt;br/>
&lt;a href="#llmaz-io-v1alpha1-ModelSpec">&lt;code>ModelSpec&lt;/code>&lt;/a>
&lt;/td>
&lt;td>
 &lt;span class="text-muted">No description provided.&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>&lt;td>&lt;code>status&lt;/code> &lt;B>[Required]&lt;/B>&lt;br/>
&lt;a href="#llmaz-io-v1alpha1-ModelStatus">&lt;code>ModelStatus&lt;/code>&lt;/a>
&lt;/td>
&lt;td>
 &lt;span class="text-muted">No description provided.&lt;/span>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="llmaz-io-v1alpha1-Flavor">&lt;code>Flavor&lt;/code> &lt;/h2>
&lt;p>&lt;strong>Appears in:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://llmaz.inftyai.com/docs/reference/core.v1alpha1/#llmaz-io-v1alpha1-InferenceConfig">InferenceConfig&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Flavor defines the accelerator requirements for a model and the necessary parameters
in autoscaling. Right now, it will be used in two places:&lt;/p>
&lt;ul>
&lt;li>Pod scheduling with node selectors specified.&lt;/li>
&lt;li>Cluster autoscaling with essential parameters provided.&lt;/li>
&lt;/ul>
&lt;table class="table">
&lt;thead>&lt;tr>&lt;th width="30%">Field&lt;/th>&lt;th>Description&lt;/th>&lt;/tr>&lt;/thead>
&lt;tbody>
&lt;tr>&lt;td>&lt;code>name&lt;/code> &lt;B>[Required]&lt;/B>&lt;br/>
&lt;a href="#llmaz-io-v1alpha1-FlavorName">&lt;code>FlavorName&lt;/code>&lt;/a>
&lt;/td>
&lt;td>
 &lt;p>Name represents the flavor name, which will be used in model claim.&lt;/p></description></item><item><title>llmaz inference API</title><link>https://llmaz.inftyai.com/docs/reference/inference.v1alpha1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/reference/inference.v1alpha1/</guid><description>&lt;h2 id="resource-types">Resource Types&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://llmaz.inftyai.com/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-Playground">Playground&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://llmaz.inftyai.com/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-Service">Service&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="inference-llmaz-io-v1alpha1-Playground">&lt;code>Playground&lt;/code> &lt;/h2>
&lt;p>&lt;strong>Appears in:&lt;/strong>&lt;/p>
&lt;p>Playground is the Schema for the playgrounds API&lt;/p>
&lt;table class="table">
&lt;thead>&lt;tr>&lt;th width="30%">Field&lt;/th>&lt;th>Description&lt;/th>&lt;/tr>&lt;/thead>
&lt;tbody>
&lt;tr>&lt;td>&lt;code>apiVersion&lt;/code>&lt;br/>string&lt;/td>&lt;td>&lt;code>inference.llmaz.io/v1alpha1&lt;/code>&lt;/td>&lt;/tr>
&lt;tr>&lt;td>&lt;code>kind&lt;/code>&lt;br/>string&lt;/td>&lt;td>&lt;code>Playground&lt;/code>&lt;/td>&lt;/tr>
&lt;tr>&lt;td>&lt;code>spec&lt;/code> &lt;B>[Required]&lt;/B>&lt;br/>
&lt;a href="#inference-llmaz-io-v1alpha1-PlaygroundSpec">&lt;code>PlaygroundSpec&lt;/code>&lt;/a>
&lt;/td>
&lt;td>
 &lt;span class="text-muted">No description provided.&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>&lt;td>&lt;code>status&lt;/code> &lt;B>[Required]&lt;/B>&lt;br/>
&lt;a href="#inference-llmaz-io-v1alpha1-PlaygroundStatus">&lt;code>PlaygroundStatus&lt;/code>&lt;/a>
&lt;/td>
&lt;td>
 &lt;span class="text-muted">No description provided.&lt;/span>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="inference-llmaz-io-v1alpha1-Service">&lt;code>Service&lt;/code> &lt;/h2>
&lt;p>&lt;strong>Appears in:&lt;/strong>&lt;/p>
&lt;p>Service is the Schema for the services API&lt;/p>
&lt;table class="table">
&lt;thead>&lt;tr>&lt;th width="30%">Field&lt;/th>&lt;th>Description&lt;/th>&lt;/tr>&lt;/thead>
&lt;tbody>
&lt;tr>&lt;td>&lt;code>apiVersion&lt;/code>&lt;br/>string&lt;/td>&lt;td>&lt;code>inference.llmaz.io/v1alpha1&lt;/code>&lt;/td>&lt;/tr>
&lt;tr>&lt;td>&lt;code>kind&lt;/code>&lt;br/>string&lt;/td>&lt;td>&lt;code>Service&lt;/code>&lt;/td>&lt;/tr>
&lt;tr>&lt;td>&lt;code>spec&lt;/code> &lt;B>[Required]&lt;/B>&lt;br/>
&lt;a href="#inference-llmaz-io-v1alpha1-ServiceSpec">&lt;code>ServiceSpec&lt;/code>&lt;/a>
&lt;/td>
&lt;td>
 &lt;span class="text-muted">No description provided.&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>&lt;td>&lt;code>status&lt;/code> &lt;B>[Required]&lt;/B>&lt;br/>
&lt;a href="#inference-llmaz-io-v1alpha1-ServiceStatus">&lt;code>ServiceStatus&lt;/code>&lt;/a>
&lt;/td>
&lt;td>
 &lt;span class="text-muted">No description provided.&lt;/span>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="inference-llmaz-io-v1alpha1-BackendName">&lt;code>BackendName&lt;/code> &lt;/h2>
&lt;p>(Alias of &lt;code>string&lt;/code>)&lt;/p>
&lt;p>&lt;strong>Appears in:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://llmaz.inftyai.com/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-BackendRuntimeConfig">BackendRuntimeConfig&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="inference-llmaz-io-v1alpha1-BackendRuntime">&lt;code>BackendRuntime&lt;/code> &lt;/h2>
&lt;p>&lt;strong>Appears in:&lt;/strong>&lt;/p>
&lt;p>BackendRuntime is the Schema for the backendRuntime API&lt;/p></description></item><item><title>Search Results</title><link>https://llmaz.inftyai.com/search/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/search/</guid><description/></item></channel></rss>