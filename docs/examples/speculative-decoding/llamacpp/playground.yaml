# This is just an toy example, because it doesn't make any sense
# in real world, drafting tokens for the model with similar size.
apiVersion: llmaz.io/v1alpha1
kind: OpenModel
metadata:
  name: llama2-7b-q8-gguf
spec:
  familyName: llama2
  source:
    modelHub:
      modelID: TheBloke/Llama-2-7B-GGUF
      filename: llama-2-7b.Q8_0.gguf
---
apiVersion: llmaz.io/v1alpha1
kind: OpenModel
metadata:
  name: llama2-7b-q2-k-gguf
spec:
  familyName: llama2
  source:
    modelHub:
      modelID: TheBloke/Llama-2-7B-GGUF
      filename: llama-2-7b.Q2_K.gguf
---
apiVersion: inference.llmaz.io/v1alpha1
kind: Playground
metadata:
  name: llamacpp-speculator
spec:
  replicas: 1
  modelClaims:
    models:
      - name: llama2-7b-q8-gguf # the target model
        role: main
      - name: llama2-7b-q2-k-gguf # the draft model
        role: draft
  backendRuntimeConfig:
    backendName: llamacpp
    args:
      - -fa # use flash attention
    resources:
      requests:
        cpu: 4
        memory: "8Gi"
      limits:
        cpu: 4
        memory: "8Gi"
