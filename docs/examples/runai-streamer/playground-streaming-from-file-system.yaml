# This example demonstrates how to use the Run:ai Model Streamer to load models from the local file system.
# The model-loader initContainer first downloads the model from Hugging Face.
# By using `--load-format runai_streamer`, vLLM leverages the Run:ai Model Streamer to stream models from the local file system.
# While this approach may be slightly slower than streaming directly from S3 (due to the initial download to local disk),
# it still offers faster model loading compared to not using the Streamer, 
# as it utilizes multiple threads to concurrently read tensor data from files into a dedicated CPU buffer,
# and then transfers the tensors to GPU memory.
apiVersion: llmaz.io/v1alpha1
kind: OpenModel
metadata:
  name: deepseek-r1-distill-qwen-1-5b
spec:
  familyName: deepseek
  source:
    modelHub:
      modelID: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
  inferenceConfig:
    flavors:
      - name: t4 # GPU type
        limits:
          nvidia.com/gpu: 1
---
apiVersion: inference.llmaz.io/v1alpha1
kind: Playground
metadata:
  name: deepseek-r1-distill-qwen-1-5b
spec:
  replicas: 1
  modelClaim:
    modelName: deepseek-r1-distill-qwen-1-5b
  backendRuntimeConfig:
    backendName: vllm # currently, only vllm supports runai streamer
    args:
      - --load-format
      - runai_streamer
    resources:
      limits:
        cpu: "4"
        memory: 16Gi
        nvidia.com/gpu: "1"
      requests:
        cpu: "4"
        memory: 16Gi
        nvidia.com/gpu: "1"
