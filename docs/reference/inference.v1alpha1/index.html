<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>llmaz inference API | llmaz</title>
<meta name=description content="Generated API reference documentation for inference.llmaz.io/v1alpha1."><meta property="og:url" content="https://llmaz.inftyai.com/docs/reference/inference.v1alpha1/"><meta property="og:site_name" content="llmaz"><meta property="og:title" content="llmaz inference API"><meta property="og:description" content="Generated API reference documentation for inference.llmaz.io/v1alpha1."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2025-05-15T13:01:47+08:00"><meta itemprop=name content="llmaz inference API"><meta itemprop=description content="Generated API reference documentation for inference.llmaz.io/v1alpha1."><meta itemprop=dateModified content="2025-05-15T13:01:47+08:00"><meta itemprop=wordCount content="1217"><meta name=twitter:card content="summary"><meta name=twitter:title content="llmaz inference API"><meta name=twitter:description content="Generated API reference documentation for inference.llmaz.io/v1alpha1."><link rel=preload href=/scss/main.min.df756438a7e020f7456327e32660cfd018c35e49c250d72b637e5a0fdc7f595c.css as=style integrity="sha256-33VkOKfgIPdFYyfjJmDP0BjDXknCUNcrY35aD9x/WVw=" crossorigin=anonymous><link href=/scss/main.min.df756438a7e020f7456327e32660cfd018c35e49c250d72b637e5a0fdc7f595c.css rel=stylesheet integrity="sha256-33VkOKfgIPdFYyfjJmDP0BjDXknCUNcrY35aD9x/WVw=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script></head><body class=td-page><header><nav class="td-navbar js-navbar-scroll" data-bs-theme=dark><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"><svg version="1.2" viewBox="0 0 29 30" width="29" height="30"><style>.a{fill:#fff;stroke:#e94751;stroke-linecap:round;stroke-linejoin:round;stroke-width:.4}</style><path class="a" d="m14.3.0-13.7 6.5c-.2.0-.2.3.0.4L3 8.1q.2.1.4.0l11-5.5q.2.0.3.0l1.9.9c.3.2.2.3.0.4L6 9.3c-.2.1-.2.3.0.4L8.5 11q.1.1.3.0l10.8-6q.1.0.2.0l2.5 1.2c.2.0.2.3.0.4l-10.1 5.5c-.4.1-.6.5-.2.8l2.3 1.2c.2.1.4.0.5.0L28.2 7c.4-.2.4-.7.0-.9L14.7.0q-.2.0-.4.0z"/><path class="a" d="m29 8.8v3.3l-.1.1-9.3 12.5 9.1-4.9c.1.0.3.0.3.2v3l-13.2 7c-.2.1-.6-.1-.6-.4v-3.5q0-.1.1-.1l9.3-12.2s0-.1-.1.0l-9 5.4c-.1.1-.3.0-.3-.2v-3c0-.3.2-.5.4-.6l12.9-6.9c.2-.1.5.0.5.3z"/><path class="a" d="m13.6 15.4-12.9-6.6c-.3-.2-.7.1-.7.4v13.3q0 .1.1.2l2.1 1c.2.1.5-.1.5-.4v-3.2l7.5 3.8v4.3q0 .2.2.3l2.7 1.5c.3.1.7-.1.7-.5V15.9c0-.2-.1-.4-.2-.5zM2.7 17.3v-3.4c0-.1.2-.2.3-.1l7 3.4c.1.1.2.3.2.5V21z"/></svg></span><span class=navbar-brand__name>llmaz</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class=nav-item><a class="nav-link active" href=/docs/><span>Documentation</span></a></li><li class=nav-item><a class="nav-link active" href=/docs/reference/><span>Reference</span></a></li><li class="nav-item dropdown d-none d-lg-block"><div class=dropdown><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>Versions</a><ul class=dropdown-menu><li><a class=dropdown-item href=/docs>latest</a></li></ul></div></li></ul></div><div class="d-none d-lg-block"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.686c2a63022b0b6581ba6fdb34428441.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><aside class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"><div id=td-sidebar-menu class=td-sidebar__inner><div id=content-mobile><form class="td-sidebar__search d-flex align-items-center"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.686c2a63022b0b6581ba6fdb34428441.json data-offline-search-base-href=/ data-offline-search-max-results=10></div><button class="btn btn-link td-sidebar__toggle d-md-none p-0 ms-3 fas fa-bars" type=button data-bs-toggle=collapse data-bs-target=#td-section-nav aria-controls=td-section-nav aria-expanded=false aria-label="Toggle section navigation"></button></form></div><div id=content-desktop></div><nav class="td-sidebar-nav collapse td-sidebar-nav--search-disabled" id=td-section-nav><ul class="td-sidebar-nav__section pe-md-3 ul-0"><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child active-path" id=m-docs-li><a href=/docs/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section tree-root" id=m-docs><span>Documentation</span></a><ul class=ul-1><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsinstallation-li><a href=/docs/installation/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsinstallation><span>Installation</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id=m-docsintegrations-li><a href=/docs/integrations/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id=m-docsintegrations><span>Integrations</span></a><ul class="ul-2 foldable"><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsintegrationsenvoy-ai-gateway-li><a href=/docs/integrations/envoy-ai-gateway/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsintegrationsenvoy-ai-gateway><span>Envoy AI Gateway</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsintegrationsopen-webui-li><a href=/docs/integrations/open-webui/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsintegrationsopen-webui><span>Open WebUI</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsintegrationsprometheus-operator-li><a href=/docs/integrations/prometheus-operator/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsintegrationsprometheus-operator><span>Prometheus Operator</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsintegrationssupport-backends-li><a href=/docs/integrations/support-backends/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsintegrationssupport-backends><span>Supported Inference Backends</span></a></li></ul></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsdevelop-li><a href=/docs/develop/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsdevelop><span>Develop Guidance</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child active-path" id=m-docsreference-li><a href=/docs/reference/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id=m-docsreference><span>Reference</span></a><ul class="ul-2 foldable"><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsreferencecorev1alpha1-li><a href=/docs/reference/core.v1alpha1/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsreferencecorev1alpha1><span>llmaz core API</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child active-path" id=m-docsreferenceinferencev1alpha1-li><a href=/docs/reference/inference.v1alpha1/ class="align-left ps-0 active td-sidebar-link td-sidebar-link__page" id=m-docsreferenceinferencev1alpha1><span class=td-sidebar-nav-active-item>llmaz inference API</span></a></li></ul></li></ul></li></ul></nav></div></aside><aside class="d-none d-xl-block col-xl-2 td-sidebar-toc d-print-none"><div class="td-page-meta ms-2 pb-1 pt-2 mb-0"><a href=https://github.com/InftyAI/llmaz/tree/main/site/content/en/docs/reference/inference.v1alpha1.md class="td-page-meta--view td-page-meta__view" target=_blank rel=noopener><i class="fa-solid fa-file-lines fa-fw"></i> View page source</a>
<a href=https://github.com/InftyAI/llmaz/edit/main/site/content/en/docs/reference/inference.v1alpha1.md class="td-page-meta--edit td-page-meta__edit" target=_blank rel=noopener><i class="fa-solid fa-pen-to-square fa-fw"></i> Edit this page</a>
<a href="https://github.com/InftyAI/llmaz/new/main/site/content/en/docs/reference?filename=change-me.md&amp;value=---%0Atitle%3A+%22Long+Page+Title%22%0AlinkTitle%3A+%22Short+Nav+Title%22%0Aweight%3A+100%0Adescription%3A+%3E-%0A+++++Page+description+for+heading+and+indexes.%0A---%0A%0A%23%23+Heading%0A%0AEdit+this+template+to+create+your+new+page.%0A%0A%2A+Give+it+a+good+name%2C+ending+in+%60.md%60+-+e.g.+%60getting-started.md%60%0A%2A+Edit+the+%22front+matter%22+section+at+the+top+of+the+page+%28weight+controls+how+its+ordered+amongst+other+pages+in+the+same+directory%3B+lowest+number+first%29.%0A%2A+Add+a+good+commit+message+at+the+bottom+of+the+page+%28%3C80+characters%3B+use+the+extended+description+field+for+more+detail%29.%0A%2A+Create+a+new+branch+so+you+can+preview+your+new+file+and+request+a+review+via+Pull+Request.%0A" class="td-page-meta--child td-page-meta__child" target=_blank rel=noopener><i class="fa-solid fa-pen-to-square fa-fw"></i> Create child page</a>
<a href="https://github.com/InftyAI/llmaz/issues/new?title=llmaz%20inference%20API" class="td-page-meta--issue td-page-meta__issue" target=_blank rel=noopener><i class="fa-solid fa-list-check fa-fw"></i> Create documentation issue</a>
<a href=https://github.com/InftyAI/llmaz/issues/new class="td-page-meta--project td-page-meta__project-issue" target=_blank rel=noopener><i class="fa-solid fa-list-check fa-fw"></i> Create project issue</a>
<a id=print href=/docs/reference/_print/><i class="fa-solid fa-print fa-fw"></i> Print entire section</a></div><div class=td-toc><nav id=TableOfContents><ul><li><a href=#resource-types>Resource Types</a></li><li><a href=#inference-llmaz-io-v1alpha1-Playground><code>Playground</code></a></li><li><a href=#inference-llmaz-io-v1alpha1-Service><code>Service</code></a></li><li><a href=#inference-llmaz-io-v1alpha1-BackendName><code>BackendName</code></a></li><li><a href=#inference-llmaz-io-v1alpha1-BackendRuntime><code>BackendRuntime</code></a></li><li><a href=#inference-llmaz-io-v1alpha1-BackendRuntimeConfig><code>BackendRuntimeConfig</code></a></li><li><a href=#inference-llmaz-io-v1alpha1-BackendRuntimeSpec><code>BackendRuntimeSpec</code></a></li><li><a href=#inference-llmaz-io-v1alpha1-BackendRuntimeStatus><code>BackendRuntimeStatus</code></a></li><li><a href=#inference-llmaz-io-v1alpha1-ElasticConfig><code>ElasticConfig</code></a></li><li><a href=#inference-llmaz-io-v1alpha1-HPATrigger><code>HPATrigger</code></a></li><li><a href=#inference-llmaz-io-v1alpha1-PlaygroundSpec><code>PlaygroundSpec</code></a></li><li><a href=#inference-llmaz-io-v1alpha1-PlaygroundStatus><code>PlaygroundStatus</code></a></li><li><a href=#inference-llmaz-io-v1alpha1-RecommendedConfig><code>RecommendedConfig</code></a></li><li><a href=#inference-llmaz-io-v1alpha1-ResourceRequirements><code>ResourceRequirements</code></a></li><li><a href=#inference-llmaz-io-v1alpha1-ScaleTrigger><code>ScaleTrigger</code></a></li><li><a href=#inference-llmaz-io-v1alpha1-ServiceSpec><code>ServiceSpec</code></a></li><li><a href=#inference-llmaz-io-v1alpha1-ServiceStatus><code>ServiceStatus</code></a></li></ul></nav></div></aside><main class="col-12 col-md-9 col-xl-8 ps-md-5" role=main><nav aria-label=breadcrumb class=td-breadcrumbs><ol class=breadcrumb><li class=breadcrumb-item><a href=/docs/>Documentation</a></li><li class=breadcrumb-item><a href=/docs/reference/>Reference</a></li><li class="breadcrumb-item active" aria-current=page>llmaz inference API</li></ol></nav><div class=td-content><h1>llmaz inference API</h1><div class=lead>Generated API reference documentation for inference.llmaz.io/v1alpha1.</div><header class=article-meta><p class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>&nbsp; 6 minute read &nbsp;</p></header><h2 id=resource-types>Resource Types</h2><ul><li><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-Playground>Playground</a></li><li><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-Service>Service</a></li></ul><h2 id=inference-llmaz-io-v1alpha1-Playground><code>Playground</code></h2><p><strong>Appears in:</strong></p><p>Playground is the Schema for the playgrounds API</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code><br>string</td><td><code>inference.llmaz.io/v1alpha1</code></td></tr><tr><td><code>kind</code><br>string</td><td><code>Playground</code></td></tr><tr><td><code>spec</code> <b>[Required]</b><br><a href=#inference-llmaz-io-v1alpha1-PlaygroundSpec><code>PlaygroundSpec</code></a></td><td><span class=text-muted>No description provided.</span></td></tr><tr><td><code>status</code> <b>[Required]</b><br><a href=#inference-llmaz-io-v1alpha1-PlaygroundStatus><code>PlaygroundStatus</code></a></td><td><span class=text-muted>No description provided.</span></td></tr></tbody></table><h2 id=inference-llmaz-io-v1alpha1-Service><code>Service</code></h2><p><strong>Appears in:</strong></p><p>Service is the Schema for the services API</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code><br>string</td><td><code>inference.llmaz.io/v1alpha1</code></td></tr><tr><td><code>kind</code><br>string</td><td><code>Service</code></td></tr><tr><td><code>spec</code> <b>[Required]</b><br><a href=#inference-llmaz-io-v1alpha1-ServiceSpec><code>ServiceSpec</code></a></td><td><span class=text-muted>No description provided.</span></td></tr><tr><td><code>status</code> <b>[Required]</b><br><a href=#inference-llmaz-io-v1alpha1-ServiceStatus><code>ServiceStatus</code></a></td><td><span class=text-muted>No description provided.</span></td></tr></tbody></table><h2 id=inference-llmaz-io-v1alpha1-BackendName><code>BackendName</code></h2><p>(Alias of <code>string</code>)</p><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-BackendRuntimeConfig>BackendRuntimeConfig</a></li></ul><h2 id=inference-llmaz-io-v1alpha1-BackendRuntime><code>BackendRuntime</code></h2><p><strong>Appears in:</strong></p><p>BackendRuntime is the Schema for the backendRuntime API</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>spec</code> <b>[Required]</b><br><a href=#inference-llmaz-io-v1alpha1-BackendRuntimeSpec><code>BackendRuntimeSpec</code></a></td><td><span class=text-muted>No description provided.</span></td></tr><tr><td><code>status</code> <b>[Required]</b><br><a href=#inference-llmaz-io-v1alpha1-BackendRuntimeStatus><code>BackendRuntimeStatus</code></a></td><td><span class=text-muted>No description provided.</span></td></tr></tbody></table><h2 id=inference-llmaz-io-v1alpha1-BackendRuntimeConfig><code>BackendRuntimeConfig</code></h2><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-PlaygroundSpec>PlaygroundSpec</a></li></ul><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>backendName</code><br><a href=#inference-llmaz-io-v1alpha1-BackendName><code>BackendName</code></a></td><td><p>BackendName represents the inference backend under the hood, e.g. vLLM.</p></td></tr><tr><td><code>version</code><br><code>string</code></td><td><p>Version represents the backend version if you want a different one
from the default version.</p></td></tr><tr><td><code>envs</code><br><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#envvar-v1-core><code>[]k8s.io/api/core/v1.EnvVar</code></a></td><td><p>Envs represents the environments set to the container.</p></td></tr><tr><td><code>configName</code> <b>[Required]</b><br><code>string</code></td><td><p>ConfigName represents the recommended configuration name for the backend,
It will be inferred from the models in the runtime if not specified, e.g. default,
speculative-decoding.</p></td></tr><tr><td><code>args</code><br><code>[]string</code></td><td><p>Args defined here will "append" the args defined in the recommendedConfig,
either explicitly configured in configName or inferred in the runtime.</p></td></tr><tr><td><code>resources</code><br><a href=#inference-llmaz-io-v1alpha1-ResourceRequirements><code>ResourceRequirements</code></a></td><td><p>Resources represents the resource requirements for backend, like cpu/mem,
accelerators like GPU should not be defined here, but at the model flavors,
or the values here will be overwritten.
Resources defined here will "overwrite" the resources in the recommendedConfig.</p></td></tr><tr><td><code>sharedMemorySize</code><br><a href=https://pkg.go.dev/k8s.io/apimachinery/pkg/api/resource#Quantity><code>k8s.io/apimachinery/pkg/api/resource.Quantity</code></a></td><td><p>SharedMemorySize represents the size of /dev/shm required in the runtime of
inference workload.
SharedMemorySize defined here will "overwrite" the sharedMemorySize in the recommendedConfig.</p></td></tr></tbody></table><h2 id=inference-llmaz-io-v1alpha1-BackendRuntimeSpec><code>BackendRuntimeSpec</code></h2><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-BackendRuntime>BackendRuntime</a></li></ul><p>BackendRuntimeSpec defines the desired state of BackendRuntime</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>command</code><br><code>[]string</code></td><td><p>Command represents the default command for the backendRuntime.</p></td></tr><tr><td><code>image</code> <b>[Required]</b><br><code>string</code></td><td><p>Image represents the default image registry of the backendRuntime.
It will work together with version to make up a real image.</p></td></tr><tr><td><code>version</code> <b>[Required]</b><br><code>string</code></td><td><p>Version represents the default version of the backendRuntime.
It will be appended to the image as a tag.</p></td></tr><tr><td><code>envs</code><br><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#envvar-v1-core><code>[]k8s.io/api/core/v1.EnvVar</code></a></td><td><p>Envs represents the environments set to the container.</p></td></tr><tr><td><code>lifecycle</code><br><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#lifecycle-v1-core><code>k8s.io/api/core/v1.Lifecycle</code></a></td><td><p>Lifecycle represents hooks executed during the lifecycle of the container.</p></td></tr><tr><td><code>livenessProbe</code><br><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#probe-v1-core><code>k8s.io/api/core/v1.Probe</code></a></td><td><p>Periodic probe of backend liveness.
Backend will be restarted if the probe fails.
Cannot be updated.</p></td></tr><tr><td><code>readinessProbe</code><br><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#probe-v1-core><code>k8s.io/api/core/v1.Probe</code></a></td><td><p>Periodic probe of backend readiness.
Backend will be removed from service endpoints if the probe fails.</p></td></tr><tr><td><code>startupProbe</code><br><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#probe-v1-core><code>k8s.io/api/core/v1.Probe</code></a></td><td><p>StartupProbe indicates that the Backend has successfully initialized.
If specified, no other probes are executed until this completes successfully.
If this probe fails, the backend will be restarted, just as if the livenessProbe failed.
This can be used to provide different probe parameters at the beginning of a backend's lifecycle,
when it might take a long time to load data or warm a cache, than during steady-state operation.</p></td></tr><tr><td><code>recommendedConfigs</code><br><a href=#inference-llmaz-io-v1alpha1-RecommendedConfig><code>[]RecommendedConfig</code></a></td><td><p>RecommendedConfigs represents the recommended configurations for the backendRuntime.</p></td></tr></tbody></table><h2 id=inference-llmaz-io-v1alpha1-BackendRuntimeStatus><code>BackendRuntimeStatus</code></h2><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-BackendRuntime>BackendRuntime</a></li></ul><p>BackendRuntimeStatus defines the observed state of BackendRuntime</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>conditions</code> <b>[Required]</b><br><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#condition-v1-meta><code>[]k8s.io/apimachinery/pkg/apis/meta/v1.Condition</code></a></td><td><p>Conditions represents the Inference condition.</p></td></tr></tbody></table><h2 id=inference-llmaz-io-v1alpha1-ElasticConfig><code>ElasticConfig</code></h2><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-PlaygroundSpec>PlaygroundSpec</a></li></ul><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>minReplicas</code><br><code>int32</code></td><td><p>MinReplicas indicates the minimum number of inference workloads based on the traffic.
Default to 1.
MinReplicas couldn't be 0 now, will support serverless in the future.</p></td></tr><tr><td><code>maxReplicas</code><br><code>int32</code></td><td><p>MaxReplicas indicates the maximum number of inference workloads based on the traffic.
Default to nil means there's no limit for the instance number.</p></td></tr><tr><td><code>scaleTrigger</code><br><a href=#inference-llmaz-io-v1alpha1-ScaleTrigger><code>ScaleTrigger</code></a></td><td><p>ScaleTrigger defines the rules to scale the workloads.
Only one trigger cloud work at a time, mostly used in Playground.
ScaleTrigger defined here will "overwrite" the scaleTrigger in the recommendedConfig.</p></td></tr></tbody></table><h2 id=inference-llmaz-io-v1alpha1-HPATrigger><code>HPATrigger</code></h2><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-ScaleTrigger>ScaleTrigger</a></li></ul><p>HPATrigger represents the configuration of the HorizontalPodAutoscaler.
Inspired by kubernetes.io/pkg/apis/autoscaling/types.go#HorizontalPodAutoscalerSpec.
Note: HPA component should be installed in prior.</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>metrics</code><br><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#metricspec-v2-autoscaling><code>[]k8s.io/api/autoscaling/v2.MetricSpec</code></a></td><td><p>metrics contains the specifications for which to use to calculate the
desired replica count (the maximum replica count across all metrics will
be used). The desired replica count is calculated multiplying the
ratio between the target value and the current value by the current
number of pods. Ergo, metrics used must decrease as the pod count is
increased, and vice-versa. See the individual metric source types for
more information about how each type of metric must respond.</p></td></tr><tr><td><code>behavior</code><br><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#horizontalpodautoscalerbehavior-v2-autoscaling><code>k8s.io/api/autoscaling/v2.HorizontalPodAutoscalerBehavior</code></a></td><td><p>behavior configures the scaling behavior of the target
in both Up and Down directions (scaleUp and scaleDown fields respectively).
If not set, the default HPAScalingRules for scale up and scale down are used.</p></td></tr></tbody></table><h2 id=inference-llmaz-io-v1alpha1-PlaygroundSpec><code>PlaygroundSpec</code></h2><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-Playground>Playground</a></li></ul><p>PlaygroundSpec defines the desired state of Playground</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>replicas</code><br><code>int32</code></td><td><p>Replicas represents the replica number of inference workloads.</p></td></tr><tr><td><code>modelClaim</code><br><a href=#llmaz-io-v1alpha1-ModelClaim><code>ModelClaim</code></a></td><td><p>ModelClaim represents claiming for one model, it's a simplified use case
of modelClaims. Most of the time, modelClaim is enough.
ModelClaim and modelClaims are exclusive configured.</p></td></tr><tr><td><code>modelClaims</code><br><a href=#llmaz-io-v1alpha1-ModelClaims><code>ModelClaims</code></a></td><td><p>ModelClaims represents claiming for multiple models for more complicated
use cases like speculative-decoding.
ModelClaims and modelClaim are exclusive configured.</p></td></tr><tr><td><code>backendRuntimeConfig</code><br><a href=#inference-llmaz-io-v1alpha1-BackendRuntimeConfig><code>BackendRuntimeConfig</code></a></td><td><p>BackendRuntimeConfig represents the inference backendRuntime configuration
under the hood, e.g. vLLM, which is the default backendRuntime.</p></td></tr><tr><td><code>elasticConfig</code> <b>[Required]</b><br><a href=#inference-llmaz-io-v1alpha1-ElasticConfig><code>ElasticConfig</code></a></td><td><p>ElasticConfig defines the configuration for elastic usage,
e.g. the max/min replicas.</p></td></tr></tbody></table><h2 id=inference-llmaz-io-v1alpha1-PlaygroundStatus><code>PlaygroundStatus</code></h2><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-Playground>Playground</a></li></ul><p>PlaygroundStatus defines the observed state of Playground</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>conditions</code> <b>[Required]</b><br><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#condition-v1-meta><code>[]k8s.io/apimachinery/pkg/apis/meta/v1.Condition</code></a></td><td><p>Conditions represents the Inference condition.</p></td></tr><tr><td><code>replicas</code> <b>[Required]</b><br><code>int32</code></td><td><p>Replicas track the replicas that have been created, whether ready or not.</p></td></tr><tr><td><code>selector</code> <b>[Required]</b><br><code>string</code></td><td><p>Selector points to the string form of a label selector which will be used by HPA.</p></td></tr></tbody></table><h2 id=inference-llmaz-io-v1alpha1-RecommendedConfig><code>RecommendedConfig</code></h2><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-BackendRuntimeSpec>BackendRuntimeSpec</a></li></ul><p>RecommendedConfig represents the recommended configurations for the backendRuntime,
user can choose one of them to apply.</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code> <b>[Required]</b><br><code>string</code></td><td><p>Name represents the identifier of the config.</p></td></tr><tr><td><code>args</code><br><code>[]string</code></td><td><p>Args represents all the arguments for the command.
Argument around with {{ .CONFIG }} is a configuration waiting for render.</p></td></tr><tr><td><code>resources</code><br><a href=#inference-llmaz-io-v1alpha1-ResourceRequirements><code>ResourceRequirements</code></a></td><td><p>Resources represents the resource requirements for backend, like cpu/mem,
accelerators like GPU should not be defined here, but at the model flavors,
or the values here will be overwritten.</p></td></tr><tr><td><code>sharedMemorySize</code><br><a href=https://pkg.go.dev/k8s.io/apimachinery/pkg/api/resource#Quantity><code>k8s.io/apimachinery/pkg/api/resource.Quantity</code></a></td><td><p>SharedMemorySize represents the size of /dev/shm required in the runtime of
inference workload.</p></td></tr><tr><td><code>scaleTrigger</code><br><a href=#inference-llmaz-io-v1alpha1-ScaleTrigger><code>ScaleTrigger</code></a></td><td><p>ScaleTrigger defines the rules to scale the workloads.
Only one trigger cloud work at a time.</p></td></tr></tbody></table><h2 id=inference-llmaz-io-v1alpha1-ResourceRequirements><code>ResourceRequirements</code></h2><p><strong>Appears in:</strong></p><ul><li><p><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-BackendRuntimeConfig>BackendRuntimeConfig</a></p></li><li><p><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-RecommendedConfig>RecommendedConfig</a></p></li></ul><p>TODO: Do not support DRA yet, we can support that once needed.</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>limits</code><br><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#resourcelist-v1-core><code>k8s.io/api/core/v1.ResourceList</code></a></td><td><p>Limits describes the maximum amount of compute resources allowed.
More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/</p></td></tr><tr><td><code>requests</code><br><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#resourcelist-v1-core><code>k8s.io/api/core/v1.ResourceList</code></a></td><td><p>Requests describes the minimum amount of compute resources required.
If Requests is omitted for a container, it defaults to Limits if that is explicitly specified,
otherwise to an implementation-defined value. Requests cannot exceed Limits.
More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/</p></td></tr></tbody></table><h2 id=inference-llmaz-io-v1alpha1-ScaleTrigger><code>ScaleTrigger</code></h2><p><strong>Appears in:</strong></p><ul><li><p><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-ElasticConfig>ElasticConfig</a></p></li><li><p><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-RecommendedConfig>RecommendedConfig</a></p></li></ul><p>ScaleTrigger defines the rules to scale the workloads.
Only one trigger cloud work at a time, mostly used in Playground.</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>hpa</code> <b>[Required]</b><br><a href=#inference-llmaz-io-v1alpha1-HPATrigger><code>HPATrigger</code></a></td><td><p>HPA represents the trigger configuration of the HorizontalPodAutoscaler.</p></td></tr></tbody></table><h2 id=inference-llmaz-io-v1alpha1-ServiceSpec><code>ServiceSpec</code></h2><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-Service>Service</a></li></ul><p>ServiceSpec defines the desired state of Service.
Service controller will maintain multi-flavor of workloads with
different accelerators for cost or performance considerations.</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>modelClaims</code> <b>[Required]</b><br><a href=#llmaz-io-v1alpha1-ModelClaims><code>ModelClaims</code></a></td><td><p>ModelClaims represents multiple claims for different models.</p></td></tr><tr><td><code>replicas</code><br><code>int32</code></td><td><p>Replicas represents the replica number of inference workloads.</p></td></tr><tr><td><code>workloadTemplate</code> <b>[Required]</b><br><code>sigs.k8s.io/lws/api/leaderworkerset/v1.LeaderWorkerTemplate</code></td><td><p>WorkloadTemplate defines the template for leader/worker pods</p></td></tr><tr><td><code>rolloutStrategy</code><br><code>sigs.k8s.io/lws/api/leaderworkerset/v1.RolloutStrategy</code></td><td><p>RolloutStrategy defines the strategy that will be applied to update replicas
when a revision is made to the leaderWorkerTemplate.</p></td></tr></tbody></table><h2 id=inference-llmaz-io-v1alpha1-ServiceStatus><code>ServiceStatus</code></h2><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-Service>Service</a></li></ul><p>ServiceStatus defines the observed state of Service</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>conditions</code> <b>[Required]</b><br><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#condition-v1-meta><code>[]k8s.io/apimachinery/pkg/apis/meta/v1.Condition</code></a></td><td><p>Conditions represents the Inference condition.</p></td></tr><tr><td><code>replicas</code> <b>[Required]</b><br><code>int32</code></td><td><p>Replicas track the replicas that have been created, whether ready or not.</p></td></tr><tr><td><code>selector</code> <b>[Required]</b><br><code>string</code></td><td><p>Selector points to the string form of a label selector, the HPA will be
able to autoscale your resource.</p></td></tr></tbody></table><style>.feedback--answer{display:inline-block}.feedback--answer-no{margin-left:1em}.feedback--response{display:none;margin-top:1em}.feedback--response__visible{display:block}</style><div class=d-print-none><h2 class=feedback--title>Feedback</h2><p class=feedback--question>Was this page helpful?</p><button class="btn btn-primary mb-4 feedback--answer feedback--answer-yes">Yes</button>
<button class="btn btn-primary mb-4 feedback--answer feedback--answer-no">No</button><p class="feedback--response feedback--response-yes">Glad to hear it! Please <a href=https://github.com/USERNAME/REPOSITORY/issues/new>tell us how we can improve</a>.</p><p class="feedback--response feedback--response-no">Sorry to hear that. Please <a href=https://github.com/USERNAME/REPOSITORY/issues/new>tell us how we can improve</a>.</p></div><script>const yesButton=document.querySelector(".feedback--answer-yes"),noButton=document.querySelector(".feedback--answer-no"),yesResponse=document.querySelector(".feedback--response-yes"),noResponse=document.querySelector(".feedback--response-no"),disableButtons=()=>{yesButton.disabled=!0,noButton.disabled=!0},sendFeedback=e=>{if(typeof gtag!="function")return;gtag("event","page_helpful",{event_category:"Helpful",event_label:window.location.pathname,value:e})};yesButton.addEventListener("click",()=>{yesResponse.classList.add("feedback--response__visible"),disableButtons(),sendFeedback(100)}),noButton.addEventListener("click",()=>{noResponse.classList.add("feedback--response__visible"),disableButtons(),sendFeedback(0)})</script><br><div class=td-page-meta__lastmod>Last modified May 15, 2025: <a href=https://github.com/InftyAI/llmaz/commit/0fa89a0df8543c9669b5b8737036a4956c314705>Add Metrics Aggregator (#413) (0fa89a0)</a></div></div></main></div></div><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class="td-footer__left col-6 col-sm-4 order-sm-1"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title aria-label><a target=_blank rel=noopener href aria-label><i></i></a></li></ul></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=GitHub aria-label=GitHub><a target=_blank rel=noopener href=https://github.com/InftyAI/llmaz aria-label=GitHub><i class="fab fa-github"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=X aria-label=X><a target=_blank rel=noopener href=https://x.com/InftyAI aria-label=X><i class="fab fa-x-twitter"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=Slack aria-label=Slack><a target=_blank rel=noopener href=https://inftyai.slack.com/ aria-label=Slack><i class="fab fa-slack"></i></a></li></ul></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"><span class=td-footer__copyright>&copy;
2025
<span class=td-footer__authors>The InftyAI Team</span></span><span class=td-footer__all_rights_reserved>All Rights Reserved</span></div></div></div></footer></div><script src=/js/main.min.69e2c1ae9320465ab10236d9ef752c6a4442c54b48b883b17c497b7c7d96a796.js integrity="sha256-aeLBrpMgRlqxAjbZ73UsakRCxUtIuIOxfEl7fH2Wp5Y=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script></body></html>