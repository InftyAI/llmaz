<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Integrations on llmaz</title><link>https://llmaz.inftyai.com/docs/integrations/</link><description>Recent content in Integrations on llmaz</description><generator>Hugo</generator><language>en</language><atom:link href="https://llmaz.inftyai.com/docs/integrations/index.xml" rel="self" type="application/rss+xml"/><item><title>Envoy AI Gateway</title><link>https://llmaz.inftyai.com/docs/integrations/envoy-ai-gateway/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/integrations/envoy-ai-gateway/</guid><description>&lt;p>&lt;a href="https://aigateway.envoyproxy.io/">Envoy AI Gateway&lt;/a> is an open source project for using Envoy Gateway
to handle request traffic from application clients to Generative AI services.&lt;/p>
&lt;h2 id="how-to-use">How to use&lt;/h2>
&lt;h3 id="enable-envoy-gateway-and-envoy-ai-gateway">Enable Envoy Gateway and Envoy AI Gateway&lt;/h3>
&lt;p>Both of them are already enabled by default in &lt;code>values.global.yaml&lt;/code> and will be deployed in llmaz-system.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">envoy-gateway&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">enabled&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">true&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">envoy-ai-gateway&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">enabled&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">true&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>However, &lt;a href="https://gateway.envoyproxy.io/latest/install/install-helm/">Envoy Gateway&lt;/a> and &lt;a href="https://aigateway.envoyproxy.io/docs/getting-started/">Envoy AI Gateway&lt;/a> can be deployed standalone in case you want to deploy them in other namespaces.&lt;/p></description></item><item><title>Open-WebUI</title><link>https://llmaz.inftyai.com/docs/integrations/open-webui/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/integrations/open-webui/</guid><description>&lt;p>&lt;a href="https://github.com/open-webui/open-webui">Open WebUI&lt;/a> is a user-friendly AI interface with OpenAI-compatible APIs, serving as the default chatbot for llmaz.&lt;/p>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;ul>
&lt;li>Make sure &lt;a href="https://github.com/envoyproxy/gateway">EnvoyGateway&lt;/a> and &lt;a href="https://github.com/envoyproxy/ai-gateway">Envoy AI Gateway&lt;/a> are installed, both of them are installed by default in llmaz. See &lt;a href="docs/envoy-ai-gateway.md">AI Gateway&lt;/a> for more details.&lt;/li>
&lt;/ul>
&lt;h2 id="how-to-use">How to use&lt;/h2>
&lt;h3 id="enable-open-webui">Enable Open WebUI&lt;/h3>
&lt;p>Open-WebUI is enabled by default in the &lt;code>values.global.yaml&lt;/code> and will be deployed in llmaz-system.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">open-webui&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">enabled&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">true&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="set-the-service-address">Set the Service Address&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>Run &lt;code>kubectl get svc -n llmaz-system&lt;/code> to list out the services, the output looks like below, the LoadBalancer service name will be used later.&lt;/p></description></item><item><title>Prometheus Operator</title><link>https://llmaz.inftyai.com/docs/integrations/prometheus-operator/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/integrations/prometheus-operator/</guid><description>&lt;p>This document provides deployment steps to install and configure Prometheus Operator in a Kubernetes cluster.&lt;/p>
&lt;h3 id="install-the-prometheus-operator">Install the prometheus operator&lt;/h3>
&lt;p>Please follow the &lt;a href="https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/getting-started/installation.md">documentation&lt;/a> to install prometheus operator or simply run the following command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>curl -sL https://github.com/prometheus-operator/prometheus-operator/releases/download/v0.81.0/bundle.yaml &lt;span style="color:#000;font-weight:bold">|&lt;/span> kubectl create -f -
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Ensure that the Prometheus Operator Pod is running successfully.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic"># Installing the prometheus operator&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>root@VM-0-5-ubuntu:/home/ubuntu# kubectl get pods
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>prometheus-operator-55b5c96cf8-jl2nx 1/1 Running &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> 12s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="install-the-servicemonitor-cr-for-llmaz">Install the ServiceMonitor CR for llmaz&lt;/h3>
&lt;p>To enable monitoring for the llmaz system, you need to install the ServiceMonitor custom resource (CR).
You can either modify the Helm chart prometheus according to the &lt;a href="https://github.com/InftyAI/llmaz/blob/main/chart/values.global.yaml">documentation&lt;/a> or use &lt;code>make install-prometheus&lt;/code> in Makefile.&lt;/p></description></item><item><title>Supported Inference Backends</title><link>https://llmaz.inftyai.com/docs/integrations/support-backends/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/integrations/support-backends/</guid><description>&lt;p>If you want to integrate more backends into llmaz, please refer to this &lt;a href="https://github.com/InftyAI/llmaz/pull/182">PR&lt;/a>. It&amp;rsquo;s always welcomed.&lt;/p>
&lt;h2 id="llamacpp">llama.cpp&lt;/h2>
&lt;p>&lt;a href="https://github.com/ggerganov/llama.cpp">llama.cpp&lt;/a> is to enable LLM inference with minimal setup and state-of-the-art performance on a wide variety of hardware - locally and in the cloud.&lt;/p>
&lt;h2 id="sglang">SGLang&lt;/h2>
&lt;p>&lt;a href="https://github.com/sgl-project/sglang">SGLang&lt;/a> is yet another fast serving framework for large language models and vision language models.&lt;/p>
&lt;h2 id="tensorrt-llm">TensorRT-LLM&lt;/h2>
&lt;p>&lt;a href="https://github.com/NVIDIA/TensorRT-LLM">TensorRT-LLM&lt;/a> provides users with an easy-to-use Python API to define Large Language Models (LLMs) and support state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that orchestrate the inference execution in performant way.&lt;/p></description></item></channel></rss>