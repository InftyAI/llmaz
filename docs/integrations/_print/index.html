<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=canonical type=text/html href=https://llmaz.inftyai.com/docs/integrations/><link rel=alternate type=application/rss+xml href=https://llmaz.inftyai.com/docs/integrations/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Integrations | llmaz</title>
<meta name=description content="This section contains the llmaz integration information."><meta property="og:url" content="https://llmaz.inftyai.com/docs/integrations/"><meta property="og:site_name" content="llmaz"><meta property="og:title" content="Integrations"><meta property="og:description" content="This section contains the llmaz integration information."><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta itemprop=name content="Integrations"><meta itemprop=description content="This section contains the llmaz integration information."><meta itemprop=dateModified content="2025-05-02T22:44:11+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Integrations"><meta name=twitter:description content="This section contains the llmaz integration information."><link rel=preload href=/scss/main.min.e26a880677830bd0f4860cb4ab2e650214ac89917d6eaa97183417553e4b2aed.css as=style integrity="sha256-4mqIBneDC9D0hgy0qy5lAhSsiZF9bqqXGDQXVT5LKu0=" crossorigin=anonymous><link href=/scss/main.min.e26a880677830bd0f4860cb4ab2e650214ac89917d6eaa97183417553e4b2aed.css rel=stylesheet integrity="sha256-4mqIBneDC9D0hgy0qy5lAhSsiZF9bqqXGDQXVT5LKu0=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script></head><body class=td-section><header><nav class="td-navbar js-navbar-scroll" data-bs-theme=dark><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"><svg id="_图层_2" data-name="图层 2" viewBox="0 0 616.66 634.81"><defs><style>.cls-1{fill:#ee4c1e;stroke:#e94751;stroke-linecap:round;stroke-linejoin:round;stroke-width:8px}</style></defs><g id="_图层_1-2" data-name="图层 1"><g><path class="cls-1" d="M304.81 4.96 13.82 140.69c-3.97 1.85-4.04 7.46-.13 9.42l50.2 25.1c2.92 1.46 6.36 1.47 9.29.03l234.7-115.29c1.86-.87 4.01-.87 5.87-.01l40.85 18.85c5.18 2.82 3.86 6.09-.05 8.06L127.99 201.19c-3.17 1.6-3.2 6.11-.06 7.75l53.28 27.91c2.07 1.08 4.54 1.06 6.58-.06l230.76-126.06c1.44-.76 3.14-.81 4.62-.13l51.6 23.58c3.95 1.82 4.08 7.4.21 9.4L260.37 259.52c-8.72 3.49-13.08 12.21-3.49 17.44l48.83 25.29c3.49 1.74 6.98.87 9.43-.46L601.5 151.15c7.52-3.97 7.15-14.86-.62-18.32L313.26 4.88c-2.7-1.2-5.78-1.17-8.45.08z"/><path class="cls-1" d="M612.55 182.12l.11 69.07c0 .87-.28 1.71-.8 2.4l-198.02 264.9 193.3-103c2.42-1.35 5.41.4 5.41 3.18l.11 63.2-280.79 148.24c-4.85 2.56-12.1-2.24-12.1-7.72v-74.9c0-.81.27-1.59.76-2.23l198.21-256.29c.68-.87-.35-2.04-1.3-1.47L325.25 402.07c-2.43 1.42-5.48-.33-5.48-3.14v-65.01c0-5.34 2.93-10.26 7.63-12.8l274.89-145.15c4.65-2.47 10.25.9 10.25 6.16z"/><path class="cls-1" d="M292.7 321.89 19.16 180.45C12.14 176.84 4 182.71 4 190.61v280.88c0 1.54.89 2.95 2.28 3.61l44.51 21.22c5.31 2.53 11.44-1.34 11.44-7.21l.06-68.13 159.64 80.45v90.66c0 2.62 1.47 5.02 3.81 6.22l57.35 31.02c7.02 3.58 15.34-1.52 15.34-9.4V331.28c0-3.95-2.21-7.57-5.73-9.38zM62.35 361.66l.07-72.14c0-2.58 2.62-4.73 5.9-3.13l147.58 73.39c3.49 1.74 6.03 4.69 6.03 8.81v71.88L62.35 361.66z"/></g></g></svg></span><span class=navbar-brand__name>llmaz</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class=nav-item><a class="nav-link active" href=/docs/><span>Documentation</span></a></li><li class=nav-item><a class=nav-link href=/docs/reference/><span>Reference</span></a></li></ul></div><div class="d-none d-lg-block"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.077338254ed649e9430aa7ba95cefecc.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 ps-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/integrations/>Return to the regular view of this page</a>.</p></div><h1 class=title>Integrations</h1><div class=lead>This section contains the llmaz integration information.</div><ul><li>1: <a href=#pg-4883ef24b3c01ce986dd46f3062ba8ef>Envoy AI Gateway</a></li><li>2: <a href=#pg-f489e01e2e14261978166c1d55f39718>Open WebUI</a></li><li>3: <a href=#pg-7d3a7a2ca4bbfe3c7b0f023d3eac77c6>Prometheus Operator</a></li><li>4: <a href=#pg-7b38a55cdf7f3c862f4b985f4bac4241>Supported Inference Backends</a></li></ul><div class=content></div></div><div class=td-content><h1 id=pg-4883ef24b3c01ce986dd46f3062ba8ef>1 - Envoy AI Gateway</h1><p><a href=https://aigateway.envoyproxy.io/>Envoy AI Gateway</a> is an open source project for using Envoy Gateway
to handle request traffic from application clients to Generative AI services.</p><h2 id=how-to-use>How to use</h2><h3 id=1-enable-envoy-gateway-and-envoy-ai-gateway>1. Enable Envoy Gateway and Envoy AI Gateway</h3><p>Both of them are enabled by default in <code>values.global.yaml</code> and will be deployed in llmaz-system.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>envoy-gateway</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>    </span><span style=color:#204a87;font-weight:700>enabled</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#204a87;font-weight:700>true</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline></span><span style=color:#204a87;font-weight:700>envoy-ai-gateway</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>    </span><span style=color:#204a87;font-weight:700>enabled</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#204a87;font-weight:700>true</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span></code></pre></div><p>However, <a href=https://gateway.envoyproxy.io/latest/install/install-helm/>Envoy Gateway</a> and <a href=https://aigateway.envoyproxy.io/docs/getting-started/>Envoy AI Gateway</a> can be deployed standalone in case you want to deploy them in other namespaces.</p><h3 id=2-basic-ai-gateway-example>2. Basic AI Gateway Example</h3><p>To expose your models via Envoy Gateway, you need to create a GatewayClass, Gateway, and AIGatewayRoute. The following example shows how to do this.</p><p>We&rsquo;ll deploy two models <code>Qwen/Qwen2-0.5B-Instruct-GGUF</code> and <code>Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF</code> with llama.cpp (cpu only) and expose them via Envoy AI Gateway.</p><p>The full example is <a href=https://github.com/InftyAI/llmaz/blob/main/docs/examples/envoy-ai-gateway/basic.yaml>here</a>, apply it.</p><h3 id=3-check-envoy-ai-gateway-apis>3. Check Envoy AI Gateway APIs</h3><p>If Open-WebUI is enabled, you can chat via the webui (recommended), see <a href=/docs/integrations/open-webui/>documentation</a>. Otherwise, following the steps below to test the Envoy AI Gateway APIs.</p><p>I. Port-forwarding the <code>LoadBalancer</code> service in llmaz-system, like:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl port-forward svc/envoy-default-default-envoy-ai-gateway-dbec795a 8080:80
</span></span></code></pre></div><p>II. Query <code>http://localhost:8008/v1/models | jq .</code>, available models will be listed. Expected response will look like this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span><span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>  <span style=color:#204a87;font-weight:700>&#34;data&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000;font-weight:700>[</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>      <span style=color:#204a87;font-weight:700>&#34;id&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;qwen2-0.5b&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>      <span style=color:#204a87;font-weight:700>&#34;created&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>1745327294</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>      <span style=color:#204a87;font-weight:700>&#34;object&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;model&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>      <span style=color:#204a87;font-weight:700>&#34;owned_by&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;Envoy AI Gateway&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>},</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>      <span style=color:#204a87;font-weight:700>&#34;id&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;qwen2.5-coder&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>      <span style=color:#204a87;font-weight:700>&#34;created&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>1745327294</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>      <span style=color:#204a87;font-weight:700>&#34;object&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;model&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>      <span style=color:#204a87;font-weight:700>&#34;owned_by&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;Envoy AI Gateway&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>}</span>
</span></span><span style=display:flex><span>  <span style=color:#000;font-weight:700>],</span>
</span></span><span style=display:flex><span>  <span style=color:#204a87;font-weight:700>&#34;object&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;list&#34;</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>}</span>
</span></span></code></pre></div><p>III. Query <code>http://localhost:8080/v1/chat/completions</code> to chat with the model. Here, we ask the <code>qwen2-0.5b</code> model, the query will look like:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>curl -H <span style=color:#4e9a06>&#34;Content-Type: application/json&#34;</span>     -d <span style=color:#4e9a06>&#39;{
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        &#34;model&#34;: &#34;qwen2-0.5b&#34;,
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        &#34;messages&#34;: [
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>            {
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>                &#34;role&#34;: &#34;system&#34;,
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>                &#34;content&#34;: &#34;Hi.&#34;
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>            }
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        ]
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>    }&#39;</span>     http://localhost:8080/v1/chat/completions <span style=color:#000;font-weight:700>|</span> jq .
</span></span></code></pre></div><p>Expected response will look like this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span><span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>  <span style=color:#204a87;font-weight:700>&#34;choices&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000;font-weight:700>[</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>      <span style=color:#204a87;font-weight:700>&#34;finish_reason&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;stop&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>      <span style=color:#204a87;font-weight:700>&#34;index&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>0</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>      <span style=color:#204a87;font-weight:700>&#34;message&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>&#34;role&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;assistant&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>&#34;content&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;Hello! How can I assist you today?&#34;</span>
</span></span><span style=display:flex><span>      <span style=color:#000;font-weight:700>}</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>}</span>
</span></span><span style=display:flex><span>  <span style=color:#000;font-weight:700>],</span>
</span></span><span style=display:flex><span>  <span style=color:#204a87;font-weight:700>&#34;created&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>1745327371</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>  <span style=color:#204a87;font-weight:700>&#34;model&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;qwen2-0.5b&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>  <span style=color:#204a87;font-weight:700>&#34;system_fingerprint&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;b5124-bc091a4d&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>  <span style=color:#204a87;font-weight:700>&#34;object&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;chat.completion&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>  <span style=color:#204a87;font-weight:700>&#34;usage&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>&#34;completion_tokens&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>10</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>&#34;prompt_tokens&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>10</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>&#34;total_tokens&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>20</span>
</span></span><span style=display:flex><span>  <span style=color:#000;font-weight:700>},</span>
</span></span><span style=display:flex><span>  <span style=color:#204a87;font-weight:700>&#34;id&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;chatcmpl-AODlT8xnf4OjJwpQH31XD4yehHLnurr0&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>  <span style=color:#204a87;font-weight:700>&#34;timings&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>&#34;prompt_n&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>1</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>&#34;prompt_ms&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>319.876</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>&#34;prompt_per_token_ms&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>319.876</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>&#34;prompt_per_second&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>3.1262114069201816</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>&#34;predicted_n&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>10</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>&#34;predicted_ms&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>1309.393</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>&#34;predicted_per_token_ms&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>130.9393</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>&#34;predicted_per_second&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>7.63712651587415</span>
</span></span><span style=display:flex><span>  <span style=color:#000;font-weight:700>}</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>}</span>
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-f489e01e2e14261978166c1d55f39718>2 - Open WebUI</h1><p><a href=https://github.com/open-webui/open-webui>Open WebUI</a> is a user-friendly AI interface with OpenAI-compatible APIs, serving as the default chatbot for llmaz.</p><h2 id=prerequisites>Prerequisites</h2><ul><li>Make sure you&rsquo;re located in <strong>llmaz-system</strong> namespace, haven&rsquo;t tested with other namespaces.</li><li>Make sure <a href=https://github.com/envoyproxy/gateway>EnvoyGateway</a> and <a href=https://github.com/envoyproxy/ai-gateway>Envoy AI Gateway</a> are installed, both of them are installed by default in llmaz. See <a href=docs/envoy-ai-gateway.md>AI Gateway</a> for more details.</li></ul><h2 id=how-to-use>How to use</h2><p>If open-webui already installed, what you need to do is just update the OpenAI API endpoint in the admin settings. You can get the value from step2 & 3 below. Otherwise, following the steps here to install open-webui.</p><ol><li><p>Enable Open WebUI in the <code>values.global.yaml</code> file, open-webui is enabled by default.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>open-webui</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>enabled</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#204a87;font-weight:700>true</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span></code></pre></div><blockquote><p>Optional to set the <code>persistence=true</code> to persist the data, recommended for production.</p></blockquote></li><li><p>Run <code>kubectl get svc -n llmaz-system</code> to list out the services, the output looks like:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cmd data-lang=cmd><span style=display:flex><span>envoy-default-default-envoy-ai-gateway-dbec795a   LoadBalancer   10.96.145.150   <span style=color:#000;font-weight:700>&lt;</span>pending<span style=color:#000;font-weight:700>&gt;</span>     80:30548/TCP                              132m
</span></span><span style=display:flex><span>envoy-gateway                                     ClusterIP      10.96.52.76     <span style=color:#000;font-weight:700>&lt;</span>none<span style=color:#000;font-weight:700>&gt;</span>        18000/TCP,18001/TCP,18002/TCP,19001/TCP   172m
</span></span></code></pre></div></li><li><p>Set <code>openaiBaseApiUrl</code> in the <code>values.global.yaml</code> like:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>open-webui</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>enabled</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#204a87;font-weight:700>true</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>openaiBaseApiUrl</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>http://envoy-default-default-envoy-ai-gateway-dbec795a.llmaz-system.svc.cluster.local/v1</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span></code></pre></div></li><li><p>Run <code>make install-chatbot</code> to install the chatbot.</p></li><li><p>Port forwarding by:</p><pre tabindex=0><code>kubectl port-forward svc/open-webui 8080:80
</code></pre></li><li><p>Visit <a href=http://localhost:8080>http://localhost:8080</a> to access the Open WebUI.</p></li><li><p>Configure the administrator for the first time.</p></li></ol><p><strong>That&rsquo;s it! You can now chat with llmaz models with Open WebUI.</strong></p></div><div class=td-content style=page-break-before:always><h1 id=pg-7d3a7a2ca4bbfe3c7b0f023d3eac77c6>3 - Prometheus Operator</h1><p>Currently, llmaz has already integrated metrics. This document provides deployment steps explaining how to install and configure Prometheus Operator in a Kubernetes cluster.</p><h3 id=install-the-prometheus-operator>Install the prometheus operator</h3><p>Please follow the <a href=https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/getting-started/installation.md>documentation</a> to install</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Installing the prometheus operator</span>
</span></span><span style=display:flex><span>root@VM-0-5-ubuntu:/home/ubuntu# kubectl get pods
</span></span><span style=display:flex><span>NAME                                   READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>prometheus-operator-55b5c96cf8-jl2nx   1/1     Running   <span style=color:#0000cf;font-weight:700>0</span>          12s
</span></span></code></pre></div><p>Ensure that the Prometheus Operator Pod is running successfully.</p><h3 id=install-the-servicemonitor-cr-for-llmaz>Install the ServiceMonitor CR for llmaz</h3><p>To enable monitoring for the llmaz system, you need to install the ServiceMonitor custom resource (CR).
You can either modify the Helm chart prometheus according to the <a href=https://github.com/InftyAI/llmaz/blob/main/chart/values.global.yaml>documentation</a> or use <code>make install-prometheus</code> in Makefile.</p><ul><li>Using Helm Chart: to modify the values.global.yaml</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>prometheus</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#8f5902;font-style:italic># -- Whether to enable Prometheus metrics exporting.</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>enable</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#204a87;font-weight:700>true</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span></code></pre></div><ul><li>Using Makefile Command: <code>make install-prometheus</code></li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>root@VM-0-5-ubuntu:/home/ubuntu/llmaz# make install-prometheus
</span></span><span style=display:flex><span>kubectl apply --server-side -k config/prometheus
</span></span><span style=display:flex><span>serviceaccount/llmaz-prometheus serverside-applied
</span></span><span style=display:flex><span>clusterrole.rbac.authorization.k8s.io/llmaz-prometheus serverside-applied
</span></span><span style=display:flex><span>clusterrolebinding.rbac.authorization.k8s.io/llmaz-prometheus serverside-applied
</span></span><span style=display:flex><span>prometheus.monitoring.coreos.com/llmaz-prometheus serverside-applied
</span></span><span style=display:flex><span>servicemonitor.monitoring.coreos.com/llmaz-controller-manager-metrics-monitor serverside-applied
</span></span></code></pre></div><h3 id=check-related-resources>Check Related Resources</h3><p>Verify that the necessary resources have been created:</p><ul><li>ServiceMonitor</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>root@VM-0-5-ubuntu:/home/ubuntu/llmaz# kubectl get ServiceMonitor -n llmaz-system
</span></span><span style=display:flex><span>NAME                                       AGE
</span></span><span style=display:flex><span>llmaz-controller-manager-metrics-monitor   59s
</span></span></code></pre></div><ul><li>Prometheus Pods</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>root@VM-0-5-ubuntu:/home/ubuntu/llmaz# kubectl get pods -n llmaz-system
</span></span><span style=display:flex><span>NAME                                        READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>llmaz-controller-manager-7ff8f7d9bd-vztls   2/2     Running   <span style=color:#0000cf;font-weight:700>0</span>          28s
</span></span><span style=display:flex><span>prometheus-llmaz-prometheus-0               2/2     Running   <span style=color:#0000cf;font-weight:700>0</span>          27s
</span></span></code></pre></div><ul><li>Services</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>root@VM-0-5-ubuntu:/home/ubuntu/llmaz# kubectl get svc -n llmaz-system
</span></span><span style=display:flex><span>NAME                                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT<span style=color:#ce5c00;font-weight:700>(</span>S<span style=color:#ce5c00;font-weight:700>)</span>    AGE
</span></span><span style=display:flex><span>llmaz-controller-manager-metrics-service   ClusterIP   10.96.79.226    &lt;none&gt;        8443/TCP   46s
</span></span><span style=display:flex><span>llmaz-webhook-service                      ClusterIP   10.96.249.226   &lt;none&gt;        443/TCP    46s
</span></span><span style=display:flex><span>prometheus-operated                        ClusterIP   None            &lt;none&gt;        9090/TCP   45s
</span></span></code></pre></div><h3 id=view-metrics-using-the-prometheus-ui>View metrics using the prometheus UI</h3><p>Use port forwarding to access the Prometheus UI from your local machine:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>root@VM-0-5-ubuntu:/home/ubuntu# kubectl port-forward services/prometheus-operated 9090:9090 --address 0.0.0.0 -n llmaz-system
</span></span><span style=display:flex><span>Forwarding from 0.0.0.0:9090 -&gt; <span style=color:#0000cf;font-weight:700>9090</span>
</span></span></code></pre></div><p>If using kind, we can use port-forward, <code>kubectl port-forward services/prometheus-operated 39090:9090 --address 0.0.0.0 -n llmaz-system</code>
This allows us to access prometheus using a browser: <code>http://localhost:9090/query</code></p><p><img alt=prometheus src="/images/prometheus.png?raw=true"></p></div><div class=td-content style=page-break-before:always><h1 id=pg-7b38a55cdf7f3c862f4b985f4bac4241>4 - Supported Inference Backends</h1><p>If you want to integrate more backends into llmaz, please refer to this <a href=https://github.com/InftyAI/llmaz/pull/182>PR</a>. It&rsquo;s always welcomed.</p><h2 id=llamacpp>llama.cpp</h2><p><a href=https://github.com/ggerganov/llama.cpp>llama.cpp</a> is to enable LLM inference with minimal setup and state-of-the-art performance on a wide variety of hardware - locally and in the cloud.</p><h2 id=sglang>SGLang</h2><p><a href=https://github.com/sgl-project/sglang>SGLang</a> is yet another fast serving framework for large language models and vision language models.</p><h2 id=text-generation-inference>Text-Generation-Inference</h2><p><a href=https://github.com/huggingface/text-generation-inference>text-generation-inference</a> is a Rust, Python and gRPC server for text generation inference. Used in production at Hugging Face to power Hugging Chat, the Inference API and Inference Endpoint.</p><h2 id=ollama>ollama</h2><p><a href=https://github.com/ollama/ollama>ollama</a> is running with Llama 3.2, Mistral, Gemma 2, and other large language models, based on llama.cpp, aims for local deploy.</p><h2 id=vllm>vLLM</h2><p><a href=https://github.com/vllm-project/vllm>vLLM</a> is a high-throughput and memory-efficient inference and serving engine for LLMs</p></div></main></div></div><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class="td-footer__left col-6 col-sm-4 order-sm-1"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=Twitter aria-label=Twitter><a target=_blank rel=noopener href=https://x.com/InftyAI aria-label=Twitter><i class="fab fa-x-twitter"></i></a></li></ul></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=GitHub aria-label=GitHub><a target=_blank rel=noopener href=https://github.com/InftyAI/llmaz aria-label=GitHub><i class="fab fa-github"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=Slack aria-label=Slack><a target=_blank rel=noopener href=https://inftyai.slack.com/ aria-label=Slack><i class="fab fa-slack"></i></a></li></ul></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"><span class=td-footer__copyright>&copy;
2025
<span class=td-footer__authors>The InftyAI Team</span></span><span class=td-footer__all_rights_reserved>All Rights Reserved</span></div></div></div></footer></div><script src=/js/main.min.69e2c1ae9320465ab10236d9ef752c6a4442c54b48b883b17c497b7c7d96a796.js integrity="sha256-aeLBrpMgRlqxAjbZ73UsakRCxUtIuIOxfEl7fH2Wp5Y=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script></body></html>