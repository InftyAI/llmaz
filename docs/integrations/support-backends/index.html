<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Supported Inference Backends | llmaz</title>
<meta name=description content="If you want to integrate more backends into llmaz, please refer to this PR. It’s always welcomed.
llama.cpp llama.cpp is to enable LLM inference with minimal setup and state-of-the-art performance on a wide variety of hardware - locally and in the cloud.
SGLang SGLang is yet another fast serving framework for large language models and vision language models.
Text-Generation-Inference text-generation-inference is a Rust, Python and gRPC server for text generation inference. Used in production at Hugging Face to power Hugging Chat, the Inference API and Inference Endpoint."><meta property="og:url" content="https://llmaz.inftyai.com/docs/integrations/support-backends/"><meta property="og:site_name" content="llmaz"><meta property="og:title" content="Supported Inference Backends"><meta property="og:description" content="If you want to integrate more backends into llmaz, please refer to this PR. It’s always welcomed.
llama.cpp llama.cpp is to enable LLM inference with minimal setup and state-of-the-art performance on a wide variety of hardware - locally and in the cloud.
SGLang SGLang is yet another fast serving framework for large language models and vision language models.
Text-Generation-Inference text-generation-inference is a Rust, Python and gRPC server for text generation inference. Used in production at Hugging Face to power Hugging Chat, the Inference API and Inference Endpoint."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2025-05-02T00:34:51+08:00"><meta itemprop=name content="Supported Inference Backends"><meta itemprop=description content="If you want to integrate more backends into llmaz, please refer to this PR. It’s always welcomed.
llama.cpp llama.cpp is to enable LLM inference with minimal setup and state-of-the-art performance on a wide variety of hardware - locally and in the cloud.
SGLang SGLang is yet another fast serving framework for large language models and vision language models.
Text-Generation-Inference text-generation-inference is a Rust, Python and gRPC server for text generation inference. Used in production at Hugging Face to power Hugging Chat, the Inference API and Inference Endpoint."><meta itemprop=dateModified content="2025-05-02T00:34:51+08:00"><meta itemprop=wordCount content="122"><meta name=twitter:card content="summary"><meta name=twitter:title content="Supported Inference Backends"><meta name=twitter:description content="If you want to integrate more backends into llmaz, please refer to this PR. It’s always welcomed.
llama.cpp llama.cpp is to enable LLM inference with minimal setup and state-of-the-art performance on a wide variety of hardware - locally and in the cloud.
SGLang SGLang is yet another fast serving framework for large language models and vision language models.
Text-Generation-Inference text-generation-inference is a Rust, Python and gRPC server for text generation inference. Used in production at Hugging Face to power Hugging Chat, the Inference API and Inference Endpoint."><link rel=preload href=/scss/main.min.e26a880677830bd0f4860cb4ab2e650214ac89917d6eaa97183417553e4b2aed.css as=style integrity="sha256-4mqIBneDC9D0hgy0qy5lAhSsiZF9bqqXGDQXVT5LKu0=" crossorigin=anonymous><link href=/scss/main.min.e26a880677830bd0f4860cb4ab2e650214ac89917d6eaa97183417553e4b2aed.css rel=stylesheet integrity="sha256-4mqIBneDC9D0hgy0qy5lAhSsiZF9bqqXGDQXVT5LKu0=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script></head><body class=td-page><header><nav class="td-navbar js-navbar-scroll" data-bs-theme=dark><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"><svg id="_图层_2" data-name="图层 2" viewBox="0 0 616.66 634.81"><defs><style>.cls-1{fill:#ee4c1e;stroke:#e94751;stroke-linecap:round;stroke-linejoin:round;stroke-width:8px}</style></defs><g id="_图层_1-2" data-name="图层 1"><g><path class="cls-1" d="M304.81 4.96 13.82 140.69c-3.97 1.85-4.04 7.46-.13 9.42l50.2 25.1c2.92 1.46 6.36 1.47 9.29.03l234.7-115.29c1.86-.87 4.01-.87 5.87-.01l40.85 18.85c5.18 2.82 3.86 6.09-.05 8.06L127.99 201.19c-3.17 1.6-3.2 6.11-.06 7.75l53.28 27.91c2.07 1.08 4.54 1.06 6.58-.06l230.76-126.06c1.44-.76 3.14-.81 4.62-.13l51.6 23.58c3.95 1.82 4.08 7.4.21 9.4L260.37 259.52c-8.72 3.49-13.08 12.21-3.49 17.44l48.83 25.29c3.49 1.74 6.98.87 9.43-.46L601.5 151.15c7.52-3.97 7.15-14.86-.62-18.32L313.26 4.88c-2.7-1.2-5.78-1.17-8.45.08z"/><path class="cls-1" d="M612.55 182.12l.11 69.07c0 .87-.28 1.71-.8 2.4l-198.02 264.9 193.3-103c2.42-1.35 5.41.4 5.41 3.18l.11 63.2-280.79 148.24c-4.85 2.56-12.1-2.24-12.1-7.72v-74.9c0-.81.27-1.59.76-2.23l198.21-256.29c.68-.87-.35-2.04-1.3-1.47L325.25 402.07c-2.43 1.42-5.48-.33-5.48-3.14v-65.01c0-5.34 2.93-10.26 7.63-12.8l274.89-145.15c4.65-2.47 10.25.9 10.25 6.16z"/><path class="cls-1" d="M292.7 321.89 19.16 180.45C12.14 176.84 4 182.71 4 190.61v280.88c0 1.54.89 2.95 2.28 3.61l44.51 21.22c5.31 2.53 11.44-1.34 11.44-7.21l.06-68.13 159.64 80.45v90.66c0 2.62 1.47 5.02 3.81 6.22l57.35 31.02c7.02 3.58 15.34-1.52 15.34-9.4V331.28c0-3.95-2.21-7.57-5.73-9.38zM62.35 361.66l.07-72.14c0-2.58 2.62-4.73 5.9-3.13l147.58 73.39c3.49 1.74 6.03 4.69 6.03 8.81v71.88L62.35 361.66z"/></g></g></svg></span><span class=navbar-brand__name>llmaz</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class=nav-item><a class="nav-link active" href=/docs/><span>Documentation</span></a></li><li class=nav-item><a class=nav-link href=/docs/reference/><span>Reference</span></a></li></ul></div><div class="d-none d-lg-block"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.e4875cb77225514789150bf5e8bd3110.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><aside class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"><div id=td-sidebar-menu class=td-sidebar__inner><div id=content-mobile><form class="td-sidebar__search d-flex align-items-center"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.e4875cb77225514789150bf5e8bd3110.json data-offline-search-base-href=/ data-offline-search-max-results=10></div><button class="btn btn-link td-sidebar__toggle d-md-none p-0 ms-3 fas fa-bars" type=button data-bs-toggle=collapse data-bs-target=#td-section-nav aria-controls=td-section-nav aria-expanded=false aria-label="Toggle section navigation"></button></form></div><div id=content-desktop></div><nav class="td-sidebar-nav collapse td-sidebar-nav--search-disabled" id=td-section-nav><ul class="td-sidebar-nav__section pe-md-3 ul-0"><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child active-path" id=m-docs-li><a href=/docs/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section tree-root" id=m-docs><span>Documentation</span></a><ul class=ul-1><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsinstallation-li><a href=/docs/installation/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsinstallation><span>Installation</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child active-path" id=m-docsintegrations-li><a href=/docs/integrations/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id=m-docsintegrations><span>Integrations</span></a><ul class="ul-2 foldable"><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsintegrationsenvoy-ai-gateway-li><a href=/docs/integrations/envoy-ai-gateway/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsintegrationsenvoy-ai-gateway><span>Envoy AI Gateway</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsintegrationsopen-webui-li><a href=/docs/integrations/open-webui/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsintegrationsopen-webui><span>Open WebUI</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsintegrationsprometheus-operator-li><a href=/docs/integrations/prometheus-operator/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsintegrationsprometheus-operator><span>Prometheus Operator</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child active-path" id=m-docsintegrationssupport-backends-li><a href=/docs/integrations/support-backends/ class="align-left ps-0 active td-sidebar-link td-sidebar-link__page" id=m-docsintegrationssupport-backends><span class=td-sidebar-nav-active-item>Supported Inference Backends</span></a></li></ul></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsdevelop-li><a href=/docs/develop/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsdevelop><span>Develop Guidance</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id=m-docsreference-li><a href=/docs/reference/ class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id=m-docsreference><span>Reference</span></a><ul class="ul-2 foldable"><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsreferencecorev1alpha1-li><a href=/docs/reference/core.v1alpha1/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsreferencecorev1alpha1><span>llmaz core API</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-docsreferenceinferencev1alpha1-li><a href=/docs/reference/inference.v1alpha1/ class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id=m-docsreferenceinferencev1alpha1><span>llmaz inference API</span></a></li></ul></li></ul></li></ul></nav></div></aside><aside class="d-none d-xl-block col-xl-2 td-sidebar-toc d-print-none"><div class="td-page-meta ms-2 pb-1 pt-2 mb-0"><a href=https://github.com/InftyAI/llmaz/tree/main/site/content/en/docs/integrations/support-backends.md class="td-page-meta--view td-page-meta__view" target=_blank rel=noopener><i class="fa-solid fa-file-lines fa-fw"></i> View page source</a>
<a href=https://github.com/InftyAI/llmaz/edit/main/site/content/en/docs/integrations/support-backends.md class="td-page-meta--edit td-page-meta__edit" target=_blank rel=noopener><i class="fa-solid fa-pen-to-square fa-fw"></i> Edit this page</a>
<a href="https://github.com/InftyAI/llmaz/new/main/site/content/en/docs/integrations?filename=change-me.md&amp;value=---%0Atitle%3A+%22Long+Page+Title%22%0AlinkTitle%3A+%22Short+Nav+Title%22%0Aweight%3A+100%0Adescription%3A+%3E-%0A+++++Page+description+for+heading+and+indexes.%0A---%0A%0A%23%23+Heading%0A%0AEdit+this+template+to+create+your+new+page.%0A%0A%2A+Give+it+a+good+name%2C+ending+in+%60.md%60+-+e.g.+%60getting-started.md%60%0A%2A+Edit+the+%22front+matter%22+section+at+the+top+of+the+page+%28weight+controls+how+its+ordered+amongst+other+pages+in+the+same+directory%3B+lowest+number+first%29.%0A%2A+Add+a+good+commit+message+at+the+bottom+of+the+page+%28%3C80+characters%3B+use+the+extended+description+field+for+more+detail%29.%0A%2A+Create+a+new+branch+so+you+can+preview+your+new+file+and+request+a+review+via+Pull+Request.%0A" class="td-page-meta--child td-page-meta__child" target=_blank rel=noopener><i class="fa-solid fa-pen-to-square fa-fw"></i> Create child page</a>
<a href="https://github.com/InftyAI/llmaz/issues/new?title=Supported%20Inference%20Backends" class="td-page-meta--issue td-page-meta__issue" target=_blank rel=noopener><i class="fa-solid fa-list-check fa-fw"></i> Create documentation issue</a>
<a href=https://github.com/InftyAI/llmaz/issues/new class="td-page-meta--project td-page-meta__project-issue" target=_blank rel=noopener><i class="fa-solid fa-list-check fa-fw"></i> Create project issue</a>
<a id=print href=/docs/integrations/_print/><i class="fa-solid fa-print fa-fw"></i> Print entire section</a></div><div class=td-toc><nav id=TableOfContents><ul><li><a href=#llamacpp>llama.cpp</a></li><li><a href=#sglang>SGLang</a></li><li><a href=#text-generation-inference>Text-Generation-Inference</a></li><li><a href=#ollama>ollama</a></li><li><a href=#vllm>vLLM</a></li></ul></nav></div></aside><main class="col-12 col-md-9 col-xl-8 ps-md-5" role=main><nav aria-label=breadcrumb class=td-breadcrumbs><ol class=breadcrumb><li class=breadcrumb-item><a href=/docs/>Documentation</a></li><li class=breadcrumb-item><a href=/docs/integrations/>Integrations</a></li><li class="breadcrumb-item active" aria-current=page>Supported Inference Backends</li></ol></nav><div class=td-content><h1>Supported Inference Backends</h1><header class=article-meta><p class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>&nbsp; less than a minute &nbsp;</p></header><p>If you want to integrate more backends into llmaz, please refer to this <a href=https://github.com/InftyAI/llmaz/pull/182>PR</a>. It&rsquo;s always welcomed.</p><h2 id=llamacpp>llama.cpp</h2><p><a href=https://github.com/ggerganov/llama.cpp>llama.cpp</a> is to enable LLM inference with minimal setup and state-of-the-art performance on a wide variety of hardware - locally and in the cloud.</p><h2 id=sglang>SGLang</h2><p><a href=https://github.com/sgl-project/sglang>SGLang</a> is yet another fast serving framework for large language models and vision language models.</p><h2 id=text-generation-inference>Text-Generation-Inference</h2><p><a href=https://github.com/huggingface/text-generation-inference>text-generation-inference</a> is a Rust, Python and gRPC server for text generation inference. Used in production at Hugging Face to power Hugging Chat, the Inference API and Inference Endpoint.</p><h2 id=ollama>ollama</h2><p><a href=https://github.com/ollama/ollama>ollama</a> is running with Llama 3.2, Mistral, Gemma 2, and other large language models, based on llama.cpp, aims for local deploy.</p><h2 id=vllm>vLLM</h2><p><a href=https://github.com/vllm-project/vllm>vLLM</a> is a high-throughput and memory-efficient inference and serving engine for LLMs</p><style>.feedback--answer{display:inline-block}.feedback--answer-no{margin-left:1em}.feedback--response{display:none;margin-top:1em}.feedback--response__visible{display:block}</style><div class=d-print-none><h2 class=feedback--title>Feedback</h2><p class=feedback--question>Was this page helpful?</p><button class="btn btn-primary mb-4 feedback--answer feedback--answer-yes">Yes</button>
<button class="btn btn-primary mb-4 feedback--answer feedback--answer-no">No</button><p class="feedback--response feedback--response-yes">Glad to hear it! Please <a href=https://github.com/USERNAME/REPOSITORY/issues/new>tell us how we can improve</a>.</p><p class="feedback--response feedback--response-no">Sorry to hear that. Please <a href=https://github.com/USERNAME/REPOSITORY/issues/new>tell us how we can improve</a>.</p></div><script>const yesButton=document.querySelector(".feedback--answer-yes"),noButton=document.querySelector(".feedback--answer-no"),yesResponse=document.querySelector(".feedback--response-yes"),noResponse=document.querySelector(".feedback--response-no"),disableButtons=()=>{yesButton.disabled=!0,noButton.disabled=!0},sendFeedback=e=>{if(typeof gtag!="function")return;gtag("event","page_helpful",{event_category:"Helpful",event_label:window.location.pathname,value:e})};yesButton.addEventListener("click",()=>{yesResponse.classList.add("feedback--response__visible"),disableButtons(),sendFeedback(100)}),noButton.addEventListener("click",()=>{noResponse.classList.add("feedback--response__visible"),disableButtons(),sendFeedback(0)})</script><br><div class=td-page-meta__lastmod>Last modified May 2, 2025: <a href=https://github.com/InftyAI/llmaz/commit/3e76e05998b2155912f71d9e95f06d359d6642c2>Update deploy.yaml (3e76e05)</a></div></div></main></div></div><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class="td-footer__left col-6 col-sm-4 order-sm-1"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=Twitter aria-label=Twitter><a target=_blank rel=noopener href=https://x.com/InftyAI aria-label=Twitter><i class="fab fa-x-twitter"></i></a></li></ul></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=GitHub aria-label=GitHub><a target=_blank rel=noopener href=https://github.com/InftyAI/llmaz aria-label=GitHub><i class="fab fa-github"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=Slack aria-label=Slack><a target=_blank rel=noopener href=https://inftyai.slack.com/ aria-label=Slack><i class="fab fa-slack"></i></a></li></ul></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"><span class=td-footer__copyright>&copy;
2025
<span class=td-footer__authors>The InftyAI Team</span></span><span class=td-footer__all_rights_reserved>All Rights Reserved</span></div></div></div></footer></div><script src=/js/main.min.69e2c1ae9320465ab10236d9ef752c6a4442c54b48b883b17c497b7c7d96a796.js integrity="sha256-aeLBrpMgRlqxAjbZ73UsakRCxUtIuIOxfEl7fH2Wp5Y=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script></body></html>