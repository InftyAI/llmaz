<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Features on llmaz</title><link>https://llmaz.inftyai.com/docs/features/</link><description>Recent content in Features on llmaz</description><generator>Hugo</generator><language>en</language><atom:link href="https://llmaz.inftyai.com/docs/features/index.xml" rel="self" type="application/rss+xml"/><item><title>Heterogeneous Cluster Support</title><link>https://llmaz.inftyai.com/docs/features/heterogeneous-cluster-support/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/features/heterogeneous-cluster-support/</guid><description>&lt;p>A &lt;code>llama2-7B&lt;/code> model can be running on &lt;strong>1xA100&lt;/strong> GPU, also on &lt;strong>1xA10&lt;/strong> GPU, even on &lt;strong>1x4090&lt;/strong> and a variety of other types of GPUs as well, that&amp;rsquo;s what we called resource fungibility. In practical scenarios, we may have a heterogeneous cluster with different GPU types, and high-end GPUs will stock out a lot, to meet the SLOs of the service as well as the cost, we need to schedule the workloads on different GPU types. With the &lt;a href="https://github.com/InftyAI/scheduler-plugins/blob/main/pkg/plugins/resource_fungibility">ResourceFungibility&lt;/a> in the InftyAI scheduler, we can simply achieve this with at most 8 alternative GPU types.&lt;/p></description></item></channel></rss>