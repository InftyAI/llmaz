<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Features on llmaz</title><link>https://llmaz.inftyai.com/docs/features/</link><description>Recent content in Features on llmaz</description><generator>Hugo</generator><language>en</language><atom:link href="https://llmaz.inftyai.com/docs/features/index.xml" rel="self" type="application/rss+xml"/><item><title>Broad Inference Backends Support</title><link>https://llmaz.inftyai.com/docs/features/broad-backends/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/features/broad-backends/</guid><description>&lt;p>If you want to integrate more backends into llmaz, please refer to this &lt;a href="https://github.com/InftyAI/llmaz/pull/182">PR&lt;/a>. It&amp;rsquo;s always welcomed.&lt;/p>
&lt;h2 id="llamacpp">llama.cpp&lt;/h2>
&lt;p>&lt;a href="https://github.com/ggerganov/llama.cpp">llama.cpp&lt;/a> is to enable LLM inference with minimal setup and state-of-the-art performance on a wide variety of hardware - locally and in the cloud.&lt;/p>
&lt;h2 id="ollama">ollama&lt;/h2>
&lt;p>&lt;a href="https://github.com/ollama/ollama">ollama&lt;/a> is running with Llama 3.2, Mistral, Gemma 2, and other large language models, based on llama.cpp, aims for local deploy.&lt;/p>
&lt;h2 id="sglang">SGLang&lt;/h2>
&lt;p>&lt;a href="https://github.com/sgl-project/sglang">SGLang&lt;/a> is yet another fast serving framework for large language models and vision language models.&lt;/p></description></item><item><title>Heterogeneous Cluster Support</title><link>https://llmaz.inftyai.com/docs/features/heterogeneous-cluster-support/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/features/heterogeneous-cluster-support/</guid><description>&lt;p>A &lt;code>llama2-7B&lt;/code> model can be running on &lt;strong>1xA100&lt;/strong> GPU, also on &lt;strong>1xA10&lt;/strong> GPU, even on &lt;strong>1x4090&lt;/strong> and a variety of other types of GPUs as well, that&amp;rsquo;s what we called resource fungibility. In practical scenarios, we may have a heterogeneous cluster with different GPU types, and high-end GPUs will stock out a lot, to meet the SLOs of the service as well as the cost, we need to schedule the workloads on different GPU types. With the &lt;a href="https://github.com/InftyAI/scheduler-plugins/blob/main/pkg/plugins/resource_fungibility">ResourceFungibility&lt;/a> in the InftyAI scheduler, we can simply achieve this with at most 8 alternative GPU types.&lt;/p></description></item><item><title>Distributed Inference</title><link>https://llmaz.inftyai.com/docs/features/distributed_inference/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/features/distributed_inference/</guid><description>&lt;p>Support multi-host &amp;amp; homogeneous xPyD distributed serving with &lt;a href="https://github.com/kubernetes-sigs/lws">LWS&lt;/a> from day 0. Will implement the heterogeneous xPyD in the future.&lt;/p></description></item></channel></rss>