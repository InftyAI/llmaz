<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Documentation on llmaz</title><link>https://llmaz.inftyai.com/docs/</link><description>Recent content in Documentation on llmaz</description><generator>Hugo</generator><language>en</language><atom:link href="https://llmaz.inftyai.com/docs/index.xml" rel="self" type="application/rss+xml"/><item><title>Installation</title><link>https://llmaz.inftyai.com/docs/installation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/installation/</guid><description>&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;p>&lt;strong>Requirements&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>Kubernetes version &amp;gt;= 1.26. LWS requires Kubernetes version &lt;strong>v1.26 or higher&lt;/strong>. If you are using a lower Kubernetes version and most of your workloads rely on single-node inference, we may consider replacing LWS with a Deployment-based approach. This fallback plan would involve using Kubernetes Deployments to manage single-node inference workloads efficiently. See &lt;a href="https://github.com/InftyAI/llmaz/issues/32">#32&lt;/a> for more details and updates.&lt;/li>
&lt;li>Helm 3, see &lt;a href="https://helm.sh/docs/intro/install/">installation&lt;/a>.&lt;/li>
&lt;li>Prometheus, see &lt;a href="https://github.com/InftyAI/llmaz/tree/main/docs/prometheus-operator#install-the-prometheus-operator">installation&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>Note: llmaz helm chart will by default install&lt;/p></description></item><item><title>Develop Guidance</title><link>https://llmaz.inftyai.com/docs/develop/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmaz.inftyai.com/docs/develop/</guid><description>&lt;h2 id="project-structure">Project Structure&lt;/h2>
&lt;pre tabindex="0">&lt;code class="language-structure" data-lang="structure">llmaz # root
├── bin # where the binaries locates, like the kustomize, ginkgo, etc.
├── chart # where the helm chart locates
├── cmd # where the main entry locates
├── docs # where all the documents locate, like examples, installation guidance, etc.
├── llmaz # where the model loader logic locates
├── pkg # where the main logic for Kubernetes controllers locates
&lt;/code>&lt;/pre>&lt;h2 id="api-design">API design&lt;/h2>
&lt;h3 id="core-apis">Core APIs&lt;/h3>
&lt;p>See the &lt;a href="https://llmaz.inftyai.com/docs/reference/core.v1alpha1/">API Reference&lt;/a> for more details.&lt;/p></description></item></channel></rss>