<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=canonical type=text/html href=https://llmaz.inftyai.com/docs/><link rel=alternate type=application/rss+xml href=https://llmaz.inftyai.com/docs/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Documentation | llmaz</title>
<meta name=description content="Welcome to llmaz llmaz (pronounced /lima:z/), aims to provide a Production-Ready inference platform for large language models on Kubernetes. It closely integrates with the state-of-the-art inference backends to bring the leading-edge researches to cloud.
High Level Overview Architecture Read to get started?"><meta property="og:url" content="https://llmaz.inftyai.com/docs/"><meta property="og:site_name" content="llmaz"><meta property="og:title" content="Documentation"><meta property="og:description" content="Welcome to llmaz llmaz (pronounced /lima:z/), aims to provide a Production-Ready inference platform for large language models on Kubernetes. It closely integrates with the state-of-the-art inference backends to bring the leading-edge researches to cloud.
High Level Overview Architecture Read to get started?"><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta itemprop=name content="Documentation"><meta itemprop=description content="Welcome to llmaz llmaz (pronounced /lima:z/), aims to provide a Production-Ready inference platform for large language models on Kubernetes. It closely integrates with the state-of-the-art inference backends to bring the leading-edge researches to cloud.
High Level Overview Architecture Read to get started?"><meta itemprop=dateModified content="2025-06-19T16:13:47+08:00"><meta itemprop=wordCount content="42"><meta name=twitter:card content="summary"><meta name=twitter:title content="Documentation"><meta name=twitter:description content="Welcome to llmaz llmaz (pronounced /lima:z/), aims to provide a Production-Ready inference platform for large language models on Kubernetes. It closely integrates with the state-of-the-art inference backends to bring the leading-edge researches to cloud.
High Level Overview Architecture Read to get started?"><link rel=preload href=/scss/main.min.df756438a7e020f7456327e32660cfd018c35e49c250d72b637e5a0fdc7f595c.css as=style integrity="sha256-33VkOKfgIPdFYyfjJmDP0BjDXknCUNcrY35aD9x/WVw=" crossorigin=anonymous><link href=/scss/main.min.df756438a7e020f7456327e32660cfd018c35e49c250d72b637e5a0fdc7f595c.css rel=stylesheet integrity="sha256-33VkOKfgIPdFYyfjJmDP0BjDXknCUNcrY35aD9x/WVw=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script></head><body class=td-section><header><nav class="td-navbar js-navbar-scroll" data-bs-theme=dark><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"><svg version="1.2" viewBox="0 0 29 30" width="29" height="30"><style>.a{fill:#fff;stroke:#e94751;stroke-linecap:round;stroke-linejoin:round;stroke-width:.4}</style><path class="a" d="m14.3.0-13.7 6.5c-.2.0-.2.3.0.4L3 8.1q.2.1.4.0l11-5.5q.2.0.3.0l1.9.9c.3.2.2.3.0.4L6 9.3c-.2.1-.2.3.0.4L8.5 11q.1.1.3.0l10.8-6q.1.0.2.0l2.5 1.2c.2.0.2.3.0.4l-10.1 5.5c-.4.1-.6.5-.2.8l2.3 1.2c.2.1.4.0.5.0L28.2 7c.4-.2.4-.7.0-.9L14.7.0q-.2.0-.4.0z"/><path class="a" d="m29 8.8v3.3l-.1.1-9.3 12.5 9.1-4.9c.1.0.3.0.3.2v3l-13.2 7c-.2.1-.6-.1-.6-.4v-3.5q0-.1.1-.1l9.3-12.2s0-.1-.1.0l-9 5.4c-.1.1-.3.0-.3-.2v-3c0-.3.2-.5.4-.6l12.9-6.9c.2-.1.5.0.5.3z"/><path class="a" d="m13.6 15.4-12.9-6.6c-.3-.2-.7.1-.7.4v13.3q0 .1.1.2l2.1 1c.2.1.5-.1.5-.4v-3.2l7.5 3.8v4.3q0 .2.2.3l2.7 1.5c.3.1.7-.1.7-.5V15.9c0-.2-.1-.4-.2-.5zM2.7 17.3v-3.4c0-.1.2-.2.3-.1l7 3.4c.1.1.2.3.2.5V21z"/></svg></span><span class=navbar-brand__name>llmaz</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class=nav-item><a class="nav-link active" href=/docs/><span>Documentation</span></a></li><li class=nav-item><a class=nav-link href=/docs/reference/><span>Reference</span></a></li><li class=nav-item><a class=nav-link href=/blog/><span>Blog</span></a></li><li class="nav-item dropdown d-none d-lg-block"><div class=dropdown><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>Versions</a><ul class=dropdown-menu><li><a class=dropdown-item href=/docs>latest</a></li></ul></div></li></ul></div><div class="d-none d-lg-block"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this siteâ€¦" aria-label="Search this siteâ€¦" autocomplete=off data-offline-search-index-json-src=/offline-search-index.24adf07bddfc25b8b15923d1fc77bcc4.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 ps-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/>Return to the regular view of this page</a>.</p></div><h1 class=title>Documentation</h1><ul><li>1: <a href=#pg-17b09a9dc5200475d8a306f18e63a0f9>Getting Started</a></li><ul><li>1.1: <a href=#pg-a2716267bec59c4a777c09f6b91d541f>Prerequisites</a></li><li>1.2: <a href=#pg-83a6a494c6472794a9acaa7a5e8e053a>Installation</a></li><li>1.3: <a href=#pg-61068e28480b54611e5e155729a0567a>Basic Usage</a></li></ul><li>2: <a href=#pg-56dc2b1c45b2c6b0983526530ba75d01>Features</a></li><ul><li>2.1: <a href=#pg-7afe021e926732de8a3b9e20344722de>Broad Inference Backends Support</a></li><li>2.2: <a href=#pg-78d451d5394217e164dcadb63dec59a2>Heterogeneous Cluster Support</a></li><li>2.3: <a href=#pg-fc6166a7b67387bdda118f26008b3928>Distributed Inference</a></li></ul><li>3: <a href=#pg-cab39e4cd04ac7231668569ffa90446c>Integrations</a></li><ul><li>3.1: <a href=#pg-4883ef24b3c01ce986dd46f3062ba8ef>Envoy AI Gateway</a></li><li>3.2: <a href=#pg-906a54d18dbae3015ce13346aa74506c>Karpenter</a></li><li>3.3: <a href=#pg-f489e01e2e14261978166c1d55f39718>Open-WebUI</a></li><li>3.4: <a href=#pg-7d3a7a2ca4bbfe3c7b0f023d3eac77c6>Prometheus Operator</a></li></ul><li>4: <a href=#pg-cbaa9992f08293ae721bb3068493a2ba>Develop Guidance</a></li><li>5: <a href=#pg-b00a88a07ceb21b1a83e5822e0c86c1d>Reference</a></li><ul><li>5.1: <a href=#pg-93feb518cca605ed55bd958508b1faf9>llmaz core API</a></li><li>5.2: <a href=#pg-0b7e8a3664294b2677b095762fd03a79>llmaz inference API</a></li></ul></ul><div class=content><h2 id=welcome-to-llmaz>Welcome to llmaz</h2><p><strong>llmaz</strong> (pronounced <code>/lima:z/</code>), aims to provide a <strong>Production-Ready</strong> inference platform for large language models on Kubernetes. It closely integrates with the state-of-the-art inference backends to bring the leading-edge researches to cloud.</p><h3 id=high-level-overview>High Level Overview</h3><p align=center><picture><img alt=infrastructure src=https://raw.githubusercontent.com/inftyai/llmaz/main/site/static/images/infra.png width=70%></picture></p><h3 id=architecture>Architecture</h3><p align=center><picture><img alt=architecture src=https://raw.githubusercontent.com/inftyai/llmaz/main/site/static/images/arch.png width=100%></picture></p><h2 id=read-to-get-started>Read to get started?</h2></div></div><div class=td-content><h1 id=pg-17b09a9dc5200475d8a306f18e63a0f9>1 - Getting Started</h1><div class=lead>This section contains the tutorials for llmaz.</div></div><div class=td-content><h1 id=pg-a2716267bec59c4a777c09f6b91d541f>1.1 - Prerequisites</h1><div class=lead>This section contains the prerequisites for llmaz.</div><p><strong>Requirements</strong>:</p><ul><li><p>Kubernetes version >= 1.27.</p><p>LWS requires Kubernetes version <strong>v1.27 or higher</strong>. If you are using a lower Kubernetes version and most of your workloads rely on single-node inference, we may consider replacing LWS with a Deployment-based approach. This fallback plan would involve using Kubernetes Deployments to manage single-node inference workloads efficiently. See <a href=https://github.com/InftyAI/llmaz/issues/32>#32</a> for more details and updates.</p></li><li><p>Helm 3, see <a href=https://helm.sh/docs/intro/install/>installation</a>.</p></li></ul><p>Note that llmaz helm chart will by default install:</p><ul><li><a href=https://github.com/kubernetes-sigs/lws>LWS</a> as the default inference workload in the llmaz-system, if you *already installed it * or <em>want to deploy it in other namespaces</em> , append <code>--set leaderWorkerSet.enabled=false</code> to the command below.</li><li><a href=https://github.com/envoyproxy/gateway>Envoy Gateway</a> and <a href=https://github.com/envoyproxy/ai-gateway>Envoy AI Gateway</a> as the frontier in the llmaz-system, if you <em>already installed these two components</em> or <em>want to deploy in other namespaces</em> , append <code>--set envoy-gateway.enabled=false --set envoy-ai-gateway.enabled=false</code> to the command below.</li><li><a href=https://github.com/open-webui/open-webui>Open WebUI</a> as the default chatbot, if you want to disable it, append <code>--set open-webui.enabled=false</code> to the command below.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-83a6a494c6472794a9acaa7a5e8e053a>1.2 - Installation</h1><div class=lead>This section introduces the installation guidance for llmaz.</div><h2 id=install-a-released-version-recommended>Install a released version (recommended)</h2><h3 id=install>Install</h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cmd data-lang=cmd><span style=display:flex><span>helm install llmaz oci://registry-1.docker.io/inftyai/llmaz --namespace llmaz-system --create-namespace --version 0.0.10
</span></span></code></pre></div><h3 id=uninstall>Uninstall</h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cmd data-lang=cmd><span style=display:flex><span>helm uninstall llmaz --namespace llmaz-system
</span></span><span style=display:flex><span>kubectl delete ns llmaz-system
</span></span></code></pre></div><p>If you want to delete the CRDs as well, run</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cmd data-lang=cmd><span style=display:flex><span>kubectl delete crd \
</span></span><span style=display:flex><span>    openmodels.llmaz.io \
</span></span><span style=display:flex><span>    backendruntimes.inference.llmaz.io \
</span></span><span style=display:flex><span>    playgrounds.inference.llmaz.io \
</span></span><span style=display:flex><span>    services.inference.llmaz.io
</span></span></code></pre></div><h2 id=install-from-source>Install from source</h2><h3 id=change-configurations>Change configurations</h3><p>If you want to change the default configurations, please change the values in <a href=https://github.com/InftyAI/llmaz/blob/main/chart/values.global.yaml>values.global.yaml</a>.</p><p><strong>Do not change</strong> the values in <em>values.yaml</em> because it&rsquo;s auto-generated and will be overwritten.</p><h3 id=install-1>Install</h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cmd data-lang=cmd><span style=display:flex><span>git clone https://github.com/inftyai/llmaz.git <span style=color:#000;font-weight:700>&amp;&amp;</span> <span style=color:#204a87;font-weight:700>cd</span> llmaz
</span></span><span style=display:flex><span>kubectl create ns llmaz-system <span style=color:#000;font-weight:700>&amp;&amp;</span> kubens llmaz-system
</span></span><span style=display:flex><span>make helm-install
</span></span></code></pre></div><h3 id=uninstall-1>Uninstall</h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cmd data-lang=cmd><span style=display:flex><span>helm uninstall llmaz --namespace llmaz-system
</span></span><span style=display:flex><span>kubectl delete ns llmaz-system
</span></span></code></pre></div><p>If you want to delete the CRDs as well, run</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cmd data-lang=cmd><span style=display:flex><span>kubectl delete crd \
</span></span><span style=display:flex><span>    openmodels.llmaz.io \
</span></span><span style=display:flex><span>    backendruntimes.inference.llmaz.io \
</span></span><span style=display:flex><span>    playgrounds.inference.llmaz.io \
</span></span><span style=display:flex><span>    services.inference.llmaz.io
</span></span></code></pre></div><h2 id=upgrade>Upgrade</h2><p>Once you changed your code, run the command to upgrade the controller:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cmd data-lang=cmd><span style=display:flex><span>IMG=<span style=color:#000;font-weight:700>&lt;</span>image-registry<span style=color:#000;font-weight:700>&gt;</span>:<span style=color:#000;font-weight:700>&lt;</span>tag<span style=color:#000;font-weight:700>&gt;</span> make helm-upgrade
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-61068e28480b54611e5e155729a0567a>1.3 - Basic Usage</h1><div class=lead>This section introduces the basic usage of llmaz.</div><p>Let&rsquo;s assume that you have installed the llmaz with the default settings, which means both the <a href=/docs/integrations/envoy-ai-gateway/>AI Gateway</a> and <a href=/docs/integrations/open-webui/>Open WebUI</a> are installed. Now let&rsquo;s following the steps to chat with your models.</p><h3 id=deploy-the-services>Deploy the Services</h3><p>Run the following command to deploy two models (cpu only).</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f https://raw.githubusercontent.com/InftyAI/llmaz/refs/heads/main/docs/examples/envoy-ai-gateway/basic.yaml
</span></span></code></pre></div><h3 id=chat-with-models>Chat with Models</h3><p>Waiting for your services ready, generally looks like:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>NAME                                                            READY   STATUS            RESTARTS   AGE
</span></span><span style=display:flex><span>ai-eg-route-extproc-default-envoy-ai-gateway-6ddcd49b64-ldwcd   1/1     Running           <span style=color:#0000cf;font-weight:700>0</span>          6m37s
</span></span><span style=display:flex><span>qwen2--5-coder-0                                                1/1     Running           <span style=color:#0000cf;font-weight:700>0</span>          6m37s
</span></span><span style=display:flex><span>qwen2-0--5b-0                                                   1/1     Running           <span style=color:#0000cf;font-weight:700>0</span>          6m37s
</span></span></code></pre></div><p>Once ready, you can access the Open WebUI by port-forwarding the service:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl port-forward svc/open-webui 8080:80 -n llmaz-system
</span></span></code></pre></div><p>Let&rsquo;s chat on <code>http://localhost:8080</code> now, two models are available to you! ðŸŽ‰</p></div><div class=td-content style=page-break-before:always><h1 id=pg-56dc2b1c45b2c6b0983526530ba75d01>2 - Features</h1><div class=lead>This section contains the advanced features of llmaz.</div></div><div class=td-content><h1 id=pg-7afe021e926732de8a3b9e20344722de>2.1 - Broad Inference Backends Support</h1><p>If you want to integrate more backends into llmaz, please refer to this <a href=https://github.com/InftyAI/llmaz/pull/182>PR</a>. It&rsquo;s always welcomed.</p><h2 id=llamacpp>llama.cpp</h2><p><a href=https://github.com/ggerganov/llama.cpp>llama.cpp</a> is to enable LLM inference with minimal setup and state-of-the-art performance on a wide variety of hardware - locally and in the cloud.</p><h2 id=ollama>ollama</h2><p><a href=https://github.com/ollama/ollama>ollama</a> is running with Llama 3.2, Mistral, Gemma 2, and other large language models, based on llama.cpp, aims for local deploy.</p><h2 id=sglang>SGLang</h2><p><a href=https://github.com/sgl-project/sglang>SGLang</a> is yet another fast serving framework for large language models and vision language models.</p><h2 id=tensorrt-llm>TensorRT-LLM</h2><p><a href=https://github.com/NVIDIA/TensorRT-LLM>TensorRT-LLM</a> provides users with an easy-to-use Python API to define Large Language Models (LLMs) and support state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that orchestrate the inference execution in performant way.</p><h2 id=text-generation-inference>Text-Generation-Inference</h2><p><a href=https://github.com/huggingface/text-generation-inference>text-generation-inference</a> is a Rust, Python and gRPC server for text generation inference. Used in production at Hugging Face to power Hugging Chat, the Inference API and Inference Endpoint.</p><h2 id=vllm>vLLM</h2><p><a href=https://github.com/vllm-project/vllm>vLLM</a> is a high-throughput and memory-efficient inference and serving engine for LLMs</p></div><div class=td-content style=page-break-before:always><h1 id=pg-78d451d5394217e164dcadb63dec59a2>2.2 - Heterogeneous Cluster Support</h1><p>A <code>llama2-7B</code> model can be running on <strong>1xA100</strong> GPU, also on <strong>1xA10</strong> GPU, even on <strong>1x4090</strong> and a variety of other types of GPUs as well, that&rsquo;s what we called resource fungibility. In practical scenarios, we may have a heterogeneous cluster with different GPU types, and high-end GPUs will stock out a lot, to meet the SLOs of the service as well as the cost, we need to schedule the workloads on different GPU types. With the <a href=https://github.com/InftyAI/scheduler-plugins/blob/main/pkg/plugins/resource_fungibility>ResourceFungibility</a> in the InftyAI scheduler, we can simply achieve this with at most 8 alternative GPU types.</p><h2 id=how-to-use>How to use</h2><h3 id=enable-inftyai-scheduler>Enable InftyAI scheduler</h3><p>Edit the <code>values.global.yaml</code> file to modify the following values:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>kube-scheduler</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>enabled</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#204a87;font-weight:700>true</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline></span><span style=color:#204a87;font-weight:700>globalConfig</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>configData</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000;font-weight:700>|-</span><span style=color:#8f5902;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>    scheduler-name: inftyai-scheduler</span><span style=color:#f8f8f8;text-decoration:underline>    
</span></span></span></code></pre></div><p>Run <code>make helm-upgrade</code> to install or upgrade llmaz.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-fc6166a7b67387bdda118f26008b3928>2.3 - Distributed Inference</h1><p>Support multi-host & homogeneous xPyD distributed serving with <a href=https://github.com/kubernetes-sigs/lws>LWS</a> from day 0. Will implement the heterogeneous xPyD in the future.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-cab39e4cd04ac7231668569ffa90446c>3 - Integrations</h1><div class=lead>This section contains the llmaz integration information.</div></div><div class=td-content><h1 id=pg-4883ef24b3c01ce986dd46f3062ba8ef>3.1 - Envoy AI Gateway</h1><p><a href=https://aigateway.envoyproxy.io/>Envoy AI Gateway</a> is an open source project for using Envoy Gateway
to handle request traffic from application clients to Generative AI services.</p><h2 id=how-to-use>How to use</h2><h3 id=enable-envoy-gateway-and-envoy-ai-gateway>Enable Envoy Gateway and Envoy AI Gateway</h3><p>Both of them are already enabled by default in <code>values.global.yaml</code> and will be deployed in llmaz-system.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>envoy-gateway</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>    </span><span style=color:#204a87;font-weight:700>enabled</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#204a87;font-weight:700>true</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline></span><span style=color:#204a87;font-weight:700>envoy-ai-gateway</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>    </span><span style=color:#204a87;font-weight:700>enabled</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#204a87;font-weight:700>true</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span></code></pre></div><p>However, <a href=https://gateway.envoyproxy.io/latest/install/install-helm/>Envoy Gateway</a> and <a href=https://aigateway.envoyproxy.io/docs/getting-started/>Envoy AI Gateway</a> can be deployed standalone in case you want to deploy them in other namespaces.</p><h3 id=basic-example>Basic Example</h3><p>To expose your models via Envoy Gateway, you need to create a GatewayClass, Gateway, and AIGatewayRoute. The following example shows how to do this.</p><p>We&rsquo;ll deploy two models <code>Qwen/Qwen2-0.5B-Instruct-GGUF</code> and <code>Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF</code> with llama.cpp (cpu only) and expose them via Envoy AI Gateway.</p><p>The full example is <a href=https://github.com/InftyAI/llmaz/blob/main/docs/examples/envoy-ai-gateway/basic.yaml>here</a>, apply it.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f https://raw.githubusercontent.com/InftyAI/llmaz/refs/heads/main/docs/examples/envoy-ai-gateway/basic.yaml
</span></span></code></pre></div><h3 id=query-ai-gateway-apis>Query AI Gateway APIs</h3><p>If Open-WebUI is enabled, you can chat via the webui (recommended), see <a href=/docs/integrations/open-webui/>documentation</a>. Otherwise, following the steps below to test the Envoy AI Gateway APIs.</p><p>I. Port-forwarding the <code>LoadBalancer</code> service in llmaz-system, like:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n llmaz-system port-forward <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  <span style=color:#204a87;font-weight:700>$(</span>kubectl -n llmaz-system get svc <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>    -l gateway.envoyproxy.io/owning-gateway-name<span style=color:#ce5c00;font-weight:700>=</span>default-envoy-ai-gateway <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>    -o name<span style=color:#204a87;font-weight:700>)</span> <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  8080:80
</span></span></code></pre></div><p>II. Query <code>curl http://localhost:8080/v1/models | jq .</code>, available models will be listed. Expected response will look like this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span><span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>  <span style=color:#204a87;font-weight:700>&#34;data&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000;font-weight:700>[</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>      <span style=color:#204a87;font-weight:700>&#34;id&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;qwen2-0.5b&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>      <span style=color:#204a87;font-weight:700>&#34;created&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>1745327294</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>      <span style=color:#204a87;font-weight:700>&#34;object&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;model&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>      <span style=color:#204a87;font-weight:700>&#34;owned_by&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;Envoy AI Gateway&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>},</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>      <span style=color:#204a87;font-weight:700>&#34;id&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;qwen2.5-coder&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>      <span style=color:#204a87;font-weight:700>&#34;created&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>1745327294</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>      <span style=color:#204a87;font-weight:700>&#34;object&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;model&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>      <span style=color:#204a87;font-weight:700>&#34;owned_by&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;Envoy AI Gateway&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>}</span>
</span></span><span style=display:flex><span>  <span style=color:#000;font-weight:700>],</span>
</span></span><span style=display:flex><span>  <span style=color:#204a87;font-weight:700>&#34;object&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;list&#34;</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>}</span>
</span></span></code></pre></div><p>III. Query <code>http://localhost:8080/v1/chat/completions</code> to chat with the model. Here, we ask the <code>qwen2-0.5b</code> model, the query will look like:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>curl -H <span style=color:#4e9a06>&#34;Content-Type: application/json&#34;</span>     -d <span style=color:#4e9a06>&#39;{
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        &#34;model&#34;: &#34;qwen2-0.5b&#34;,
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        &#34;messages&#34;: [
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>            {
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>                &#34;role&#34;: &#34;system&#34;,
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>                &#34;content&#34;: &#34;Hi.&#34;
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>            }
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        ]
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>    }&#39;</span>     http://localhost:8080/v1/chat/completions <span style=color:#000;font-weight:700>|</span> jq .
</span></span></code></pre></div><p>Expected response will look like this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span><span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>  <span style=color:#204a87;font-weight:700>&#34;choices&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000;font-weight:700>[</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>      <span style=color:#204a87;font-weight:700>&#34;finish_reason&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;stop&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>      <span style=color:#204a87;font-weight:700>&#34;index&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>0</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>      <span style=color:#204a87;font-weight:700>&#34;message&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>&#34;role&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;assistant&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>&#34;content&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;Hello! How can I assist you today?&#34;</span>
</span></span><span style=display:flex><span>      <span style=color:#000;font-weight:700>}</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>}</span>
</span></span><span style=display:flex><span>  <span style=color:#000;font-weight:700>],</span>
</span></span><span style=display:flex><span>  <span style=color:#204a87;font-weight:700>&#34;created&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>1745327371</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>  <span style=color:#204a87;font-weight:700>&#34;model&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;qwen2-0.5b&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>  <span style=color:#204a87;font-weight:700>&#34;system_fingerprint&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;b5124-bc091a4d&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>  <span style=color:#204a87;font-weight:700>&#34;object&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;chat.completion&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>  <span style=color:#204a87;font-weight:700>&#34;usage&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>&#34;completion_tokens&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>10</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>&#34;prompt_tokens&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>10</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>&#34;total_tokens&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>20</span>
</span></span><span style=display:flex><span>  <span style=color:#000;font-weight:700>},</span>
</span></span><span style=display:flex><span>  <span style=color:#204a87;font-weight:700>&#34;id&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;chatcmpl-AODlT8xnf4OjJwpQH31XD4yehHLnurr0&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>  <span style=color:#204a87;font-weight:700>&#34;timings&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>&#34;prompt_n&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>1</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>&#34;prompt_ms&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>319.876</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>&#34;prompt_per_token_ms&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>319.876</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>&#34;prompt_per_second&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>3.1262114069201816</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>&#34;predicted_n&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>10</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>&#34;predicted_ms&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>1309.393</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>&#34;predicted_per_token_ms&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>130.9393</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>&#34;predicted_per_second&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>7.63712651587415</span>
</span></span><span style=display:flex><span>  <span style=color:#000;font-weight:700>}</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>}</span>
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-906a54d18dbae3015ce13346aa74506c>3.2 - Karpenter</h1><p><a href=https://github.com/kubernetes-sigs/karpenter>Karpenter</a> automatically launches just the right compute resources to handle your cluster&rsquo;s applications, but it is built to adhere to the scheduling decisions of kube-scheduler, so it&rsquo;s certainly possible we would run across some cases where Karpenter makes incorrect decisions when the InftyAI scheduler is in the mix.</p><p>We forked the Karpenter project and re-complie the karpenter image for cloud providers like AWS, and you can find the details in <a href=https://github.com/InftyAI/llmaz/blob/main/docs/proposals/106-spot-instance-karpenter/README.md>this proposal</a>. This document provides deployment steps to install and configure Customized Karpenter in an EKS cluster.</p><h2 id=how-to-use>How to use</h2><h3 id=set-environment-variables>Set environment variables</h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>KARPENTER_NAMESPACE</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;kube-system&#34;</span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>KARPENTER_VERSION</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;1.5.0&#34;</span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>K8S_VERSION</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;1.32&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>AWS_PARTITION</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;aws&#34;</span> <span style=color:#8f5902;font-style:italic># if you are not using standard partitions, you may need to configure to aws-cn / aws-us-gov</span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>CLUSTER_NAME</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;</span><span style=color:#4e9a06>${</span><span style=color:#000>USER</span><span style=color:#4e9a06>}</span><span style=color:#4e9a06>-karpenter-demo&#34;</span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>AWS_DEFAULT_REGION</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;us-west-2&#34;</span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>AWS_ACCOUNT_ID</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;</span><span style=color:#204a87;font-weight:700>$(</span>aws sts get-caller-identity --query Account --output text<span style=color:#204a87;font-weight:700>)</span><span style=color:#4e9a06>&#34;</span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>TEMPOUT</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;</span><span style=color:#204a87;font-weight:700>$(</span>mktemp<span style=color:#204a87;font-weight:700>)</span><span style=color:#4e9a06>&#34;</span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>ALIAS_VERSION</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;</span><span style=color:#204a87;font-weight:700>$(</span>aws ssm get-parameter --name <span style=color:#4e9a06>&#34;/aws/service/eks/optimized-ami/</span><span style=color:#4e9a06>${</span><span style=color:#000>K8S_VERSION</span><span style=color:#4e9a06>}</span><span style=color:#4e9a06>/amazon-linux-2023/x86_64/standard/recommended/image_id&#34;</span> --query Parameter.Value <span style=color:#000;font-weight:700>|</span> xargs aws ec2 describe-images --query <span style=color:#4e9a06>&#39;Images[0].Name&#39;</span> --image-ids <span style=color:#000;font-weight:700>|</span> sed -r <span style=color:#4e9a06>&#39;s/^.*(v[[:digit:]]+).*$/\1/&#39;</span><span style=color:#204a87;font-weight:700>)</span><span style=color:#4e9a06>&#34;</span>
</span></span></code></pre></div><p>If you open a new shell to run steps in this procedure, you need to set some or all of the environment variables again. To remind yourself of these values, type:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#204a87>echo</span> <span style=color:#4e9a06>&#34;</span><span style=color:#4e9a06>${</span><span style=color:#000>KARPENTER_NAMESPACE</span><span style=color:#4e9a06>}</span><span style=color:#4e9a06>&#34;</span> <span style=color:#4e9a06>&#34;</span><span style=color:#4e9a06>${</span><span style=color:#000>KARPENTER_VERSION</span><span style=color:#4e9a06>}</span><span style=color:#4e9a06>&#34;</span> <span style=color:#4e9a06>&#34;</span><span style=color:#4e9a06>${</span><span style=color:#000>K8S_VERSION</span><span style=color:#4e9a06>}</span><span style=color:#4e9a06>&#34;</span> <span style=color:#4e9a06>&#34;</span><span style=color:#4e9a06>${</span><span style=color:#000>CLUSTER_NAME</span><span style=color:#4e9a06>}</span><span style=color:#4e9a06>&#34;</span> <span style=color:#4e9a06>&#34;</span><span style=color:#4e9a06>${</span><span style=color:#000>AWS_DEFAULT_REGION</span><span style=color:#4e9a06>}</span><span style=color:#4e9a06>&#34;</span> <span style=color:#4e9a06>&#34;</span><span style=color:#4e9a06>${</span><span style=color:#000>AWS_ACCOUNT_ID</span><span style=color:#4e9a06>}</span><span style=color:#4e9a06>&#34;</span> <span style=color:#4e9a06>&#34;</span><span style=color:#4e9a06>${</span><span style=color:#000>TEMPOUT</span><span style=color:#4e9a06>}</span><span style=color:#4e9a06>&#34;</span> <span style=color:#4e9a06>&#34;</span><span style=color:#4e9a06>${</span><span style=color:#000>ALIAS_VERSION</span><span style=color:#4e9a06>}</span><span style=color:#4e9a06>&#34;</span>
</span></span></code></pre></div><h3 id=create-a-cluster-and-add-karpenter>Create a cluster and add Karpenter</h3><p>Please refer to the <a href=https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html>Getting Started with Karpenter</a> to create a cluster and add Karpenter.</p><h3 id=install-the-gpu-operator>Install the gpu operator</h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>helm repo add nvidia https://helm.ngc.nvidia.com/nvidia <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>    <span style=color:#ce5c00;font-weight:700>&amp;&amp;</span> helm repo update
</span></span><span style=display:flex><span>helm install --wait --generate-name <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>    -n gpu-operator --create-namespace <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>    nvidia/gpu-operator <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>    --version<span style=color:#ce5c00;font-weight:700>=</span>v25.3.0
</span></span></code></pre></div><h3 id=install-llmaz-with-inftyai-scheduler-enabled>Install llmaz with InftyAI scheduler enabled</h3><p>Please refer to <a href=/docs/features/heterogeneous-cluster-support/>heterogeneous cluster support</a>.</p><h3 id=configure-karpenter-with-customized-image>Configure Karpenter with customized image</h3><p>We need to assign the <code>karpenter-core-llmaz</code> cluster role to the <code>karpenter</code> service account and update the karpenter image to the customized one.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>cat <span style=color:#4e9a06>&lt;&lt;EOF | envsubst | kubectl apply -f -
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>apiVersion: rbac.authorization.k8s.io/v1
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>kind: ClusterRoleBinding
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>metadata:
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>  name: karpenter-core-llmaz
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>roleRef:
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>  apiGroup: rbac.authorization.k8s.io
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>  kind: ClusterRole
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>  name: karpenter-core-llmaz
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>subjects:
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>- kind: ServiceAccount
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>  name: karpenter
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>  namespace: ${KARPENTER_NAMESPACE}
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>---
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>apiVersion: rbac.authorization.k8s.io/v1
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>kind: ClusterRole
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>metadata:
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>  name: karpenter-core-llmaz
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>rules:
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>- apiGroups: [&#34;llmaz.io&#34;]
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>  resources: [&#34;openmodels&#34;]
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>  verbs: [&#34;get&#34;, &#34;list&#34;, &#34;watch&#34;]
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>helm upgrade --install karpenter oci://public.ecr.aws/karpenter/karpenter --version <span style=color:#4e9a06>&#34;</span><span style=color:#4e9a06>${</span><span style=color:#000>KARPENTER_VERSION</span><span style=color:#4e9a06>}</span><span style=color:#4e9a06>&#34;</span> --namespace <span style=color:#4e9a06>&#34;</span><span style=color:#4e9a06>${</span><span style=color:#000>KARPENTER_NAMESPACE</span><span style=color:#4e9a06>}</span><span style=color:#4e9a06>&#34;</span> --create-namespace <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --set <span style=color:#4e9a06>&#34;settings.clusterName=</span><span style=color:#4e9a06>${</span><span style=color:#000>CLUSTER_NAME</span><span style=color:#4e9a06>}</span><span style=color:#4e9a06>&#34;</span> <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --set <span style=color:#4e9a06>&#34;settings.interruptionQueue=</span><span style=color:#4e9a06>${</span><span style=color:#000>CLUSTER_NAME</span><span style=color:#4e9a06>}</span><span style=color:#4e9a06>&#34;</span> <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --set controller.resources.requests.cpu<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>1</span> <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --set controller.resources.requests.memory<span style=color:#ce5c00;font-weight:700>=</span>1Gi <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --set controller.resources.limits.cpu<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>1</span> <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --set controller.resources.limits.memory<span style=color:#ce5c00;font-weight:700>=</span>1Gi <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --wait <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --set controller.image.repository<span style=color:#ce5c00;font-weight:700>=</span>inftyai/karpenter-provider-aws <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --set <span style=color:#4e9a06>&#34;controller.image.tag=</span><span style=color:#4e9a06>${</span><span style=color:#000>KARPENTER_VERSION</span><span style=color:#4e9a06>}</span><span style=color:#4e9a06>&#34;</span> <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --set controller.image.digest<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;&#34;</span>
</span></span></code></pre></div><h2 id=basic-example>Basic Example</h2><ol><li>Create a gpu node pool</li></ol><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>cat <span style=color:#4e9a06>&lt;&lt;EOF | envsubst | kubectl apply -f -
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>apiVersion: karpenter.k8s.aws/v1
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>kind: E</span>C2NodeClass
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: llmaz-demo            <span style=color:#8f5902;font-style:italic># you can change the name to a more meaningful one, please align with the node pool&#39;s nodeClassRef.</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  amiSelectorTerms:
</span></span><span style=display:flex><span>  - alias: al2023@<span style=color:#4e9a06>${</span><span style=color:#000>ALIAS_VERSION</span><span style=color:#4e9a06>}</span>
</span></span><span style=display:flex><span>  blockDeviceMappings:
</span></span><span style=display:flex><span>  <span style=color:#8f5902;font-style:italic># the default volume size of the selected AMI is 20Gi, it is not enough for kubelet to pull</span>
</span></span><span style=display:flex><span>  <span style=color:#8f5902;font-style:italic># the images and run the workloads. So we need to map a larger volume to the root device. </span>
</span></span><span style=display:flex><span>  <span style=color:#8f5902;font-style:italic># You can change the volume size to a larger value according to your actual needs.</span>
</span></span><span style=display:flex><span>  - deviceName: /dev/xvda
</span></span><span style=display:flex><span>    ebs:
</span></span><span style=display:flex><span>      deleteOnTermination: <span style=color:#204a87>true</span>
</span></span><span style=display:flex><span>      volumeSize: 50Gi     
</span></span><span style=display:flex><span>      volumeType: gp3
</span></span><span style=display:flex><span>  role: KarpenterNodeRole-<span style=color:#4e9a06>${</span><span style=color:#000>CLUSTER_NAME</span><span style=color:#4e9a06>}</span>          <span style=color:#8f5902;font-style:italic># replace with your cluster name</span>
</span></span><span style=display:flex><span>  securityGroupSelectorTerms:
</span></span><span style=display:flex><span>  - tags:
</span></span><span style=display:flex><span>      karpenter.sh/discovery: <span style=color:#4e9a06>${</span><span style=color:#000>CLUSTER_NAME</span><span style=color:#4e9a06>}</span>      <span style=color:#8f5902;font-style:italic># replace with your cluster name</span>
</span></span><span style=display:flex><span>  subnetSelectorTerms:
</span></span><span style=display:flex><span>  - tags:
</span></span><span style=display:flex><span>      karpenter.sh/discovery: <span style=color:#4e9a06>${</span><span style=color:#000>CLUSTER_NAME</span><span style=color:#4e9a06>}</span>      <span style=color:#8f5902;font-style:italic># replace with your cluster name</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: karpenter.sh/v1
</span></span><span style=display:flex><span>kind: NodePool
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: llmaz-demo-gpu-nodepool  <span style=color:#8f5902;font-style:italic># you can change the name to a more meaningful one. </span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  disruption:
</span></span><span style=display:flex><span>    budgets:
</span></span><span style=display:flex><span>    - nodes: 10%
</span></span><span style=display:flex><span>    consolidateAfter: 5m        
</span></span><span style=display:flex><span>    consolidationPolicy: WhenEmptyOrUnderutilized
</span></span><span style=display:flex><span>  limits:  <span style=color:#8f5902;font-style:italic># You can change the limits to match your actual needs.</span>
</span></span><span style=display:flex><span>    cpu: <span style=color:#0000cf;font-weight:700>1000</span>
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      expireAfter: 720h
</span></span><span style=display:flex><span>      nodeClassRef:
</span></span><span style=display:flex><span>        group: karpenter.k8s.aws
</span></span><span style=display:flex><span>        kind: EC2NodeClass
</span></span><span style=display:flex><span>        name: llmaz-demo
</span></span><span style=display:flex><span>      requirements:
</span></span><span style=display:flex><span>      - key: kubernetes.io/arch
</span></span><span style=display:flex><span>        operator: In
</span></span><span style=display:flex><span>        values:
</span></span><span style=display:flex><span>        - amd64
</span></span><span style=display:flex><span>      - key: kubernetes.io/os
</span></span><span style=display:flex><span>        operator: In
</span></span><span style=display:flex><span>        values:
</span></span><span style=display:flex><span>        - linux
</span></span><span style=display:flex><span>      - key: karpenter.sh/capacity-type
</span></span><span style=display:flex><span>        operator: In
</span></span><span style=display:flex><span>        values:
</span></span><span style=display:flex><span>        - spot
</span></span><span style=display:flex><span>      - key: karpenter.k8s.aws/instance-family
</span></span><span style=display:flex><span>        operator: In
</span></span><span style=display:flex><span>        values:                                <span style=color:#8f5902;font-style:italic># replace with your instance-family with gpu supported</span>
</span></span><span style=display:flex><span>        - g4dn
</span></span><span style=display:flex><span>        - g5g
</span></span><span style=display:flex><span>      taints:
</span></span><span style=display:flex><span>      - effect: NoSchedule
</span></span><span style=display:flex><span>        key: nvidia.com/gpu
</span></span><span style=display:flex><span>        value: <span style=color:#4e9a06>&#34;true&#34;</span>
</span></span></code></pre></div><ol start=2><li>Deploy a model with flavors</li></ol><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>cat <span style=color:#4e9a06>&lt;&lt;EOF | kubectl apply -f -
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>apiVersion: llmaz.io/v1alpha1
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>kind: OpenModel
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>metadata:
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>  name: qwen2-0--5b
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>spec:
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>  familyName: qwen2
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>  source:
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>    modelHub:
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>      modelID: Qwen/Qwen2-0.5B-Instruct
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>  inferenceConfig:
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>    flavors:
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>      # The g5g instance family in the aws cloud can provide the t4g GPU type.
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>      # we define the instance family in the node pool like llmaz-demo-gpu-nodepool.
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>      - name: t4g
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        limits:
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>          nvidia.com/gpu: 1
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        # The flavorName is not recongnized by the Karpenter, so we need to specify the
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        # instance-gpu-name via nodeSelector to match the t4g GPU type when node is provisioned
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        # by Karpenter from multiple node pools.
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        #
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        # When you only have a single node pool to provision the GPU instance and the node pool
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        # only has one GPU type, it is okay to not specify the nodeSelector. But in practice,
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        # it is better to specify the nodeSelector to make the provisioned node more predictable.
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        #
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        # The available node labels for selecting the target GPU device is listed below:
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        # karpenter.k8s.aws/instance-gpu-count
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        # karpenter.k8s.aws/instance-gpu-manufacturer
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        # karpenter.k8s.aws/instance-gpu-memory
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        # karpenter.k8s.aws/instance-gpu-name
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        nodeSelector:
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>          karpenter.k8s.aws/instance-gpu-name: t4g
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>      # The g4dn instance family in the aws cloud can provide the t4 GPU type.
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>      # we define the instance family in the node pool like llmaz-demo-gpu-nodepool.
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>      - name: t4
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        limits:
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>          nvidia.com/gpu: 1
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        # The flavorName is not recongnized by the Karpenter, so we need to specify the
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        # instance-gpu-name via nodeSelector to match the t4 GPU type when node is provisioned
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        # by Karpenter from multiple node pools.
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        #
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        # When you only have a single node pool to provision the GPU instance and the node pool
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        # only has one GPU type, it is okay to not specify the nodeSelector. But in practice,
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        # it is better to specify the nodeSelector to make the provisioned node more predictable.
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        #
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        # The available node labels for selecting the target GPU device is listed below:
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        # karpenter.k8s.aws/instance-gpu-count
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        # karpenter.k8s.aws/instance-gpu-manufacturer
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        # karpenter.k8s.aws/instance-gpu-memory
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        # karpenter.k8s.aws/instance-gpu-name
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        nodeSelector:
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>          karpenter.k8s.aws/instance-gpu-name: t4
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>---
</span></span></span><span style=display:flex><span><span style=color:#4e9a06># Currently, the Playground resource type does not support to configure tolerations
</span></span></span><span style=display:flex><span><span style=color:#4e9a06># for the generated pods. But luckily, when a pod with the `nvidia.com/gpu` resource  
</span></span></span><span style=display:flex><span><span style=color:#4e9a06># is created on the eks cluster, the generated pod will be tweaked with the following
</span></span></span><span style=display:flex><span><span style=color:#4e9a06># tolerations:
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>#   - effect: NoExecute
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>#      key: node.kubernetes.io/not-ready
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>#      operator: Exists
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>#      tolerationSeconds: 300
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>#   - effect: NoExecute
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>#     key: node.kubernetes.io/unreachable
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>#     operator: Exists
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>#     tolerationSeconds: 300
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>#   - effect: NoSchedule
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>#     key: nvidia.com/gpu
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>#     operator: Exists
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>apiVersion: inference.llmaz.io/v1alpha1
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>kind: Playground
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>metadata:
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>  labels:
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>    llmaz.io/model-name: qwen2-0--5b
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>  name: qwen2-0--5b
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>spec:
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>  backendRuntimeConfig:
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>    backendName: tgi
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>    # Due to the limitation of our aws account, we have to decrease the resources to match
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>    # the avaliable instance type which is g4dn.xlarge. If your account has no such limitation,
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>    # you can remove the custom resources settings below.
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>    resources:
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>      limits:
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        cpu: &#34;2&#34;
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        memory: 4Gi
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>      requests:
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        cpu: &#34;2&#34;
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        memory: 4Gi
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>  modelClaim:
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>    modelName: qwen2-0--5b
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>  replicas: 1
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>EOF</span>
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-f489e01e2e14261978166c1d55f39718>3.3 - Open-WebUI</h1><p><a href=https://github.com/open-webui/open-webui>Open WebUI</a> is a user-friendly AI interface with OpenAI-compatible APIs, serving as the default chatbot for llmaz.</p><h2 id=prerequisites>Prerequisites</h2><ul><li>Make sure <a href=https://github.com/envoyproxy/gateway>EnvoyGateway</a> and <a href=https://github.com/envoyproxy/ai-gateway>Envoy AI Gateway</a> are installed, both of them are installed by default in llmaz. See <a href=docs/envoy-ai-gateway.md>AI Gateway</a> for more details.</li></ul><h2 id=how-to-use>How to use</h2><h3 id=enable-open-webui>Enable Open WebUI</h3><p>Open-WebUI is enabled by default in the <code>values.global.yaml</code> and will be deployed in llmaz-system.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>open-webui</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>enabled</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#204a87;font-weight:700>true</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span></code></pre></div><h3 id=set-the-service-address>Set the Service Address</h3><ol><li><p>Run <code>kubectl get svc -n llmaz-system</code> to list out the services, the output looks like below, the LoadBalancer service name will be used later.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cmd data-lang=cmd><span style=display:flex><span>envoy-default-default-envoy-ai-gateway-dbec795a   LoadBalancer   10.96.145.150   <span style=color:#000;font-weight:700>&lt;</span>pending<span style=color:#000;font-weight:700>&gt;</span>     80:30548/TCP                              132m
</span></span><span style=display:flex><span>envoy-gateway                                     ClusterIP      10.96.52.76     <span style=color:#000;font-weight:700>&lt;</span>none<span style=color:#000;font-weight:700>&gt;</span>        18000/TCP,18001/TCP,18002/TCP,19001/TCP   172m
</span></span></code></pre></div></li><li><p>Port forward the Open-WebUI service, and visit <code>http://localhost:8080</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl port-forward svc/open-webui 8080:80 -n llmaz-system
</span></span></code></pre></div></li><li><p>Click <code>Settings -> Admin Settings -> Connections</code>, set the URL to <code>http://envoy-default-default-envoy-ai-gateway-dbec795a.llmaz-system.svc.cluster.local/v1</code> and save. (You can also set the <code>openaiBaseApiUrl</code> in the <code>values.global.yaml</code>)</p></li></ol><p><img alt=img src=/images/open-webui-setting.png></p><ol start=4><li>Start to chat now.</li></ol><h2 id=persistence>Persistence</h2><p>Set the <code>persistence=true</code> in <code>values.global.yaml</code> to enable persistence.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-7d3a7a2ca4bbfe3c7b0f023d3eac77c6>3.4 - Prometheus Operator</h1><p>This document provides deployment steps to install and configure Prometheus Operator in a Kubernetes cluster.</p><h3 id=install-the-prometheus-operator>Install the prometheus operator</h3><p>Please follow the <a href=https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/getting-started/installation.md>documentation</a> to install prometheus operator or simply run the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>curl -sL https://github.com/prometheus-operator/prometheus-operator/releases/download/v0.81.0/bundle.yaml <span style=color:#000;font-weight:700>|</span> kubectl create -f -
</span></span></code></pre></div><p>Ensure that the Prometheus Operator Pod is running successfully.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Installing the prometheus operator</span>
</span></span><span style=display:flex><span>root@VM-0-5-ubuntu:/home/ubuntu# kubectl get pods
</span></span><span style=display:flex><span>NAME                                   READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>prometheus-operator-55b5c96cf8-jl2nx   1/1     Running   <span style=color:#0000cf;font-weight:700>0</span>          12s
</span></span></code></pre></div><h3 id=install-the-servicemonitor-cr-for-llmaz>Install the ServiceMonitor CR for llmaz</h3><p>To enable monitoring for the llmaz system, you need to install the ServiceMonitor custom resource (CR).
You can either modify the Helm chart prometheus according to the <a href=https://github.com/InftyAI/llmaz/blob/main/chart/values.global.yaml>documentation</a> or use <code>make install-prometheus</code> in Makefile.</p><ul><li>Using Helm Chart: to modify the values.global.yaml</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>prometheus</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#8f5902;font-style:italic># -- Whether to enable Prometheus metrics exporting.</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>enable</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#204a87;font-weight:700>true</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span></code></pre></div><ul><li>Using Makefile Command: <code>make install-prometheus</code></li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>root@VM-0-5-ubuntu:/home/ubuntu/llmaz# make install-prometheus
</span></span><span style=display:flex><span>kubectl apply --server-side -k config/prometheus
</span></span><span style=display:flex><span>serviceaccount/llmaz-prometheus serverside-applied
</span></span><span style=display:flex><span>clusterrole.rbac.authorization.k8s.io/llmaz-prometheus serverside-applied
</span></span><span style=display:flex><span>clusterrolebinding.rbac.authorization.k8s.io/llmaz-prometheus serverside-applied
</span></span><span style=display:flex><span>prometheus.monitoring.coreos.com/llmaz-prometheus serverside-applied
</span></span><span style=display:flex><span>servicemonitor.monitoring.coreos.com/llmaz-controller-manager-metrics-monitor serverside-applied
</span></span></code></pre></div><h3 id=check-related-resources>Check Related Resources</h3><p>Verify that the necessary resources have been created:</p><ul><li>ServiceMonitor</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>root@VM-0-5-ubuntu:/home/ubuntu/llmaz# kubectl get ServiceMonitor -n llmaz-system
</span></span><span style=display:flex><span>NAME                                       AGE
</span></span><span style=display:flex><span>llmaz-controller-manager-metrics-monitor   59s
</span></span></code></pre></div><ul><li>Prometheus Pods</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>root@VM-0-5-ubuntu:/home/ubuntu/llmaz# kubectl get pods -n llmaz-system
</span></span><span style=display:flex><span>NAME                                        READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>llmaz-controller-manager-7ff8f7d9bd-vztls   2/2     Running   <span style=color:#0000cf;font-weight:700>0</span>          28s
</span></span><span style=display:flex><span>prometheus-llmaz-prometheus-0               2/2     Running   <span style=color:#0000cf;font-weight:700>0</span>          27s
</span></span></code></pre></div><ul><li>Services</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>root@VM-0-5-ubuntu:/home/ubuntu/llmaz# kubectl get svc -n llmaz-system
</span></span><span style=display:flex><span>NAME                                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT<span style=color:#ce5c00;font-weight:700>(</span>S<span style=color:#ce5c00;font-weight:700>)</span>    AGE
</span></span><span style=display:flex><span>llmaz-controller-manager-metrics-service   ClusterIP   10.96.79.226    &lt;none&gt;        8443/TCP   46s
</span></span><span style=display:flex><span>llmaz-webhook-service                      ClusterIP   10.96.249.226   &lt;none&gt;        443/TCP    46s
</span></span><span style=display:flex><span>prometheus-operated                        ClusterIP   None            &lt;none&gt;        9090/TCP   45s
</span></span></code></pre></div><h3 id=view-metrics-using-the-prometheus-ui>View metrics using the prometheus UI</h3><p>Use port forwarding to access the Prometheus UI from your local machine:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>root@VM-0-5-ubuntu:/home/ubuntu# kubectl port-forward services/prometheus-operated 9090:9090 --address 0.0.0.0 -n llmaz-system
</span></span><span style=display:flex><span>Forwarding from 0.0.0.0:9090 -&gt; <span style=color:#0000cf;font-weight:700>9090</span>
</span></span></code></pre></div><p>If using kind, we can use port-forward, <code>kubectl port-forward services/prometheus-operated 39090:9090 --address 0.0.0.0 -n llmaz-system</code>
This allows us to access prometheus using a browser: <code>http://localhost:9090/query</code></p><p><img alt=prometheus src="/images/prometheus.png?raw=true"></p></div><div class=td-content style=page-break-before:always><h1 id=pg-cbaa9992f08293ae721bb3068493a2ba>4 - Develop Guidance</h1><div class=lead>This section contains a develop guidance for people who want to learn more about this project.</div><h2 id=project-structure>Project Structure</h2><pre tabindex=0><code class=language-structure data-lang=structure>llmaz # root
â”œâ”€â”€ bin # where the binaries locates, like the kustomize, ginkgo, etc.
â”œâ”€â”€ chart # where the helm chart locates
â”œâ”€â”€ cmd # where the main entry locates
â”œâ”€â”€ docs # where all the documents locate, like examples, installation guidance, etc.
â”œâ”€â”€ llmaz # where the model loader logic locates
â”œâ”€â”€ pkg # where the main logic for Kubernetes controllers locates
</code></pre><h2 id=api-design>API design</h2><h3 id=core-apis>Core APIs</h3><p>See the <a href=/docs/reference/core.v1alpha1/>API Reference</a> for more details.</p><h3 id=inference-apis>Inference APIs</h3><p>See the <a href=/docs/reference/inference.v1alpha1/>API Reference</a> for more details.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-b00a88a07ceb21b1a83e5822e0c86c1d>5 - Reference</h1><div class=lead>This section contains the llmaz reference information.</div></div><div class=td-content><h1 id=pg-93feb518cca605ed55bd958508b1faf9>5.1 - llmaz core API</h1><div class=lead>Generated API reference documentation for llmaz.io/v1alpha1.</div><h2 id=resource-types>Resource Types</h2><ul><li><a href=/docs/reference/core.v1alpha1/#llmaz-io-v1alpha1-OpenModel>OpenModel</a></li></ul><h2 id=llmaz-io-v1alpha1-OpenModel><code>OpenModel</code></h2><p><strong>Appears in:</strong></p><p>OpenModel is the Schema for the open models API</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code><br>string</td><td><code>llmaz.io/v1alpha1</code></td></tr><tr><td><code>kind</code><br>string</td><td><code>OpenModel</code></td></tr><tr><td><code>spec</code> <b>[Required]</b><br><a href=#llmaz-io-v1alpha1-ModelSpec><code>ModelSpec</code></a></td><td><span class=text-muted>No description provided.</span></td></tr><tr><td><code>status</code> <b>[Required]</b><br><a href=#llmaz-io-v1alpha1-ModelStatus><code>ModelStatus</code></a></td><td><span class=text-muted>No description provided.</span></td></tr></tbody></table><h2 id=llmaz-io-v1alpha1-Flavor><code>Flavor</code></h2><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/core.v1alpha1/#llmaz-io-v1alpha1-InferenceConfig>InferenceConfig</a></li></ul><p>Flavor defines the accelerator requirements for a model and the necessary parameters
in autoscaling. Right now, it will be used in two places:</p><ul><li>Pod scheduling with node selectors specified.</li><li>Cluster autoscaling with essential parameters provided.</li></ul><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code> <b>[Required]</b><br><a href=#llmaz-io-v1alpha1-FlavorName><code>FlavorName</code></a></td><td><p>Name represents the flavor name, which will be used in model claim.</p></td></tr><tr><td><code>limits</code><br><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#resourcelist-v1-core><code>k8s.io/api/core/v1.ResourceList</code></a></td><td><p>Limits defines the required accelerators to serve the model for each replica,
like &lt;nvidia.com/gpu: 8>. For multi-hosts cases, the limits here indicates
the resource requirements for each replica, usually equals to the TP size.
Not recommended to set the cpu and memory usage here:</p><ul><li>if using playground, you can define the cpu/mem usage at backendConfig.</li><li>if using inference service, you can define the cpu/mem at the container resources.
However, if you define the same accelerator resources at playground/service as well,
the resources will be overwritten by the flavor limit here.</li></ul></td></tr><tr><td><code>nodeSelector</code><br><code>map[string]string</code></td><td><p>NodeSelector represents the node candidates for Pod placements, if a node doesn't
meet the nodeSelector, it will be filtered out in the resourceFungibility scheduler plugin.
If nodeSelector is empty, it means every node is a candidate.</p></td></tr><tr><td><code>params</code><br><code>map[string]string</code></td><td><p>Params stores other useful parameters and will be consumed by cluster-autoscaler / Karpenter
for autoscaling or be defined as model parallelism parameters like TP or PP size.
E.g. with autoscaling, when scaling up nodes with 8x Nvidia A00, the parameter can be injected
with &lt;INSTANCE-TYPE: p4d.24xlarge> for AWS.
Preset parameters: TP, PP, INSTANCE-TYPE.</p></td></tr></tbody></table><h2 id=llmaz-io-v1alpha1-FlavorName><code>FlavorName</code></h2><p>(Alias of <code>string</code>)</p><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/core.v1alpha1/#llmaz-io-v1alpha1-Flavor>Flavor</a></li></ul><h2 id=llmaz-io-v1alpha1-InferenceConfig><code>InferenceConfig</code></h2><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/core.v1alpha1/#llmaz-io-v1alpha1-ModelSpec>ModelSpec</a></li></ul><p>InferenceConfig represents the inference configurations for the model.</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>flavors</code><br><a href=#llmaz-io-v1alpha1-Flavor><code>[]Flavor</code></a></td><td><p>Flavors represents the accelerator requirements to serve the model.
Flavors are fungible following the priority represented by the slice order.</p></td></tr></tbody></table><h2 id=llmaz-io-v1alpha1-ModelHub><code>ModelHub</code></h2><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/core.v1alpha1/#llmaz-io-v1alpha1-ModelSource>ModelSource</a></li></ul><p>ModelHub represents the model registry for model downloads.</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code><br><code>string</code></td><td><p>Name refers to the model registry, such as huggingface.</p></td></tr><tr><td><code>modelID</code> <b>[Required]</b><br><code>string</code></td><td><p>ModelID refers to the model identifier on model hub,
such as meta-llama/Meta-Llama-3-8B.</p></td></tr><tr><td><code>filename</code> <b>[Required]</b><br><code>string</code></td><td><p>Filename refers to a specified model file rather than the whole repo.
This is helpful to download a specified GGUF model rather than downloading
the whole repo which includes all kinds of quantized models.
TODO: this is only supported with Huggingface, add support for ModelScope
in the near future.
Note: once filename is set, allowPatterns and ignorePatterns should be left unset.</p></td></tr><tr><td><code>revision</code><br><code>string</code></td><td><p>Revision refers to a Git revision id which can be a branch name, a tag, or a commit hash.</p></td></tr><tr><td><code>allowPatterns</code><br><code>[]string</code></td><td><p>AllowPatterns refers to files matched with at least one pattern will be downloaded.</p></td></tr><tr><td><code>ignorePatterns</code><br><code>[]string</code></td><td><p>IgnorePatterns refers to files matched with any of the patterns will not be downloaded.</p></td></tr></tbody></table><h2 id=llmaz-io-v1alpha1-ModelName><code>ModelName</code></h2><p>(Alias of <code>string</code>)</p><p><strong>Appears in:</strong></p><ul><li><p><a href=/docs/reference/core.v1alpha1/#llmaz-io-v1alpha1-ModelRef>ModelRef</a></p></li><li><p><a href=/docs/reference/core.v1alpha1/#llmaz-io-v1alpha1-ModelSpec>ModelSpec</a></p></li></ul><h2 id=llmaz-io-v1alpha1-ModelRef><code>ModelRef</code></h2><p><strong>Appears in:</strong></p><p>ModelRef refers to a created Model with it's role.</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code> <b>[Required]</b><br><a href=#llmaz-io-v1alpha1-ModelName><code>ModelName</code></a></td><td><p>Name represents the model name.</p></td></tr><tr><td><code>role</code><br><a href=#llmaz-io-v1alpha1-ModelRole><code>ModelRole</code></a></td><td><p>Role represents the model role once more than one model is required.
Such as a draft role, which means running with SpeculativeDecoding,
and default arguments for backend will be searched in backendRuntime
with the name of speculative-decoding.</p></td></tr></tbody></table><h2 id=llmaz-io-v1alpha1-ModelRole><code>ModelRole</code></h2><p>(Alias of <code>string</code>)</p><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/core.v1alpha1/#llmaz-io-v1alpha1-ModelRef>ModelRef</a></li></ul><h2 id=llmaz-io-v1alpha1-ModelSource><code>ModelSource</code></h2><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/core.v1alpha1/#llmaz-io-v1alpha1-ModelSpec>ModelSpec</a></li></ul><p>ModelSource represents the source of the model.
Only one model source will be used.</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>modelHub</code><br><a href=#llmaz-io-v1alpha1-ModelHub><code>ModelHub</code></a></td><td><p>ModelHub represents the model registry for model downloads.</p></td></tr><tr><td><code>uri</code><br><a href=#llmaz-io-v1alpha1-URIProtocol><code>URIProtocol</code></a></td><td><p>URI represents a various kinds of model sources following the uri protocol, protocol://, e.g.</p><ul><li>oss://./</li><li>ollama://llama3.3</li><li>host://</li></ul></td></tr></tbody></table><h2 id=llmaz-io-v1alpha1-ModelSpec><code>ModelSpec</code></h2><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/core.v1alpha1/#llmaz-io-v1alpha1-OpenModel>OpenModel</a></li></ul><p>ModelSpec defines the desired state of Model</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>familyName</code> <b>[Required]</b><br><a href=#llmaz-io-v1alpha1-ModelName><code>ModelName</code></a></td><td><p>FamilyName represents the model type, like llama2, which will be auto injected
to the labels with the key of <code>llmaz.io/model-family-name</code>.</p></td></tr><tr><td><code>source</code> <b>[Required]</b><br><a href=#llmaz-io-v1alpha1-ModelSource><code>ModelSource</code></a></td><td><p>Source represents the source of the model, there're several ways to load
the model such as loading from huggingface, OCI registry, s3, host path and so on.</p></td></tr><tr><td><code>inferenceConfig</code> <b>[Required]</b><br><a href=#llmaz-io-v1alpha1-InferenceConfig><code>InferenceConfig</code></a></td><td><p>InferenceConfig represents the inference configurations for the model.</p></td></tr><tr><td><code>ownedBy</code><br><code>string</code></td><td><p>OwnedBy represents the owner of the running models serving by the backends,
which will be exported as the field of "OwnedBy" in openai-compatible API "/models".
Default to "llmaz" if not set.</p></td></tr><tr><td><code>createdAt</code><br><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#time-v1-meta><code>k8s.io/apimachinery/pkg/apis/meta/v1.Time</code></a></td><td><p>CreatedAt represents the creation timestamp of the running models serving by the backends,
which will be exported as the field of "Created" in openai-compatible API "/models".
It follows the format of RFC 3339, for example "2024-05-21T10:00:00Z".</p></td></tr></tbody></table><h2 id=llmaz-io-v1alpha1-ModelStatus><code>ModelStatus</code></h2><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/core.v1alpha1/#llmaz-io-v1alpha1-OpenModel>OpenModel</a></li></ul><p>ModelStatus defines the observed state of Model</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>conditions</code> <b>[Required]</b><br><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#condition-v1-meta><code>[]k8s.io/apimachinery/pkg/apis/meta/v1.Condition</code></a></td><td><p>Conditions represents the Inference condition.</p></td></tr></tbody></table><h2 id=llmaz-io-v1alpha1-URIProtocol><code>URIProtocol</code></h2><p>(Alias of <code>string</code>)</p><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/core.v1alpha1/#llmaz-io-v1alpha1-ModelSource>ModelSource</a></li></ul><p>URIProtocol represents the protocol of the URI.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0b7e8a3664294b2677b095762fd03a79>5.2 - llmaz inference API</h1><div class=lead>Generated API reference documentation for inference.llmaz.io/v1alpha1.</div><h2 id=resource-types>Resource Types</h2><ul><li><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-Playground>Playground</a></li><li><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-Service>Service</a></li></ul><h2 id=inference-llmaz-io-v1alpha1-Playground><code>Playground</code></h2><p><strong>Appears in:</strong></p><p>Playground is the Schema for the playgrounds API</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code><br>string</td><td><code>inference.llmaz.io/v1alpha1</code></td></tr><tr><td><code>kind</code><br>string</td><td><code>Playground</code></td></tr><tr><td><code>spec</code> <b>[Required]</b><br><a href=#inference-llmaz-io-v1alpha1-PlaygroundSpec><code>PlaygroundSpec</code></a></td><td><span class=text-muted>No description provided.</span></td></tr><tr><td><code>status</code> <b>[Required]</b><br><a href=#inference-llmaz-io-v1alpha1-PlaygroundStatus><code>PlaygroundStatus</code></a></td><td><span class=text-muted>No description provided.</span></td></tr></tbody></table><h2 id=inference-llmaz-io-v1alpha1-Service><code>Service</code></h2><p><strong>Appears in:</strong></p><p>Service is the Schema for the services API</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code><br>string</td><td><code>inference.llmaz.io/v1alpha1</code></td></tr><tr><td><code>kind</code><br>string</td><td><code>Service</code></td></tr><tr><td><code>spec</code> <b>[Required]</b><br><a href=#inference-llmaz-io-v1alpha1-ServiceSpec><code>ServiceSpec</code></a></td><td><span class=text-muted>No description provided.</span></td></tr><tr><td><code>status</code> <b>[Required]</b><br><a href=#inference-llmaz-io-v1alpha1-ServiceStatus><code>ServiceStatus</code></a></td><td><span class=text-muted>No description provided.</span></td></tr></tbody></table><h2 id=inference-llmaz-io-v1alpha1-BackendName><code>BackendName</code></h2><p>(Alias of <code>string</code>)</p><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-BackendRuntimeConfig>BackendRuntimeConfig</a></li></ul><h2 id=inference-llmaz-io-v1alpha1-BackendRuntime><code>BackendRuntime</code></h2><p><strong>Appears in:</strong></p><p>BackendRuntime is the Schema for the backendRuntime API</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>spec</code> <b>[Required]</b><br><a href=#inference-llmaz-io-v1alpha1-BackendRuntimeSpec><code>BackendRuntimeSpec</code></a></td><td><span class=text-muted>No description provided.</span></td></tr><tr><td><code>status</code> <b>[Required]</b><br><a href=#inference-llmaz-io-v1alpha1-BackendRuntimeStatus><code>BackendRuntimeStatus</code></a></td><td><span class=text-muted>No description provided.</span></td></tr></tbody></table><h2 id=inference-llmaz-io-v1alpha1-BackendRuntimeConfig><code>BackendRuntimeConfig</code></h2><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-PlaygroundSpec>PlaygroundSpec</a></li></ul><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>backendName</code><br><a href=#inference-llmaz-io-v1alpha1-BackendName><code>BackendName</code></a></td><td><p>BackendName represents the inference backend under the hood, e.g. vLLM.</p></td></tr><tr><td><code>version</code><br><code>string</code></td><td><p>Version represents the backend version if you want a different one
from the default version.</p></td></tr><tr><td><code>envs</code><br><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#envvar-v1-core><code>[]k8s.io/api/core/v1.EnvVar</code></a></td><td><p>Envs represents the environments set to the container.</p></td></tr><tr><td><code>configName</code> <b>[Required]</b><br><code>string</code></td><td><p>ConfigName represents the recommended configuration name for the backend,
It will be inferred from the models in the runtime if not specified, e.g. default,
speculative-decoding.</p></td></tr><tr><td><code>args</code><br><code>[]string</code></td><td><p>Args defined here will "append" the args defined in the recommendedConfig,
either explicitly configured in configName or inferred in the runtime.</p></td></tr><tr><td><code>resources</code><br><a href=#inference-llmaz-io-v1alpha1-ResourceRequirements><code>ResourceRequirements</code></a></td><td><p>Resources represents the resource requirements for backend, like cpu/mem,
accelerators like GPU should not be defined here, but at the model flavors,
or the values here will be overwritten.
Resources defined here will "overwrite" the resources in the recommendedConfig.</p></td></tr><tr><td><code>sharedMemorySize</code><br><a href=https://pkg.go.dev/k8s.io/apimachinery/pkg/api/resource#Quantity><code>k8s.io/apimachinery/pkg/api/resource.Quantity</code></a></td><td><p>SharedMemorySize represents the size of /dev/shm required in the runtime of
inference workload.
SharedMemorySize defined here will "overwrite" the sharedMemorySize in the recommendedConfig.</p></td></tr></tbody></table><h2 id=inference-llmaz-io-v1alpha1-BackendRuntimeSpec><code>BackendRuntimeSpec</code></h2><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-BackendRuntime>BackendRuntime</a></li></ul><p>BackendRuntimeSpec defines the desired state of BackendRuntime</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>command</code><br><code>[]string</code></td><td><p>Command represents the default command for the backendRuntime.</p></td></tr><tr><td><code>image</code> <b>[Required]</b><br><code>string</code></td><td><p>Image represents the default image registry of the backendRuntime.
It will work together with version to make up a real image.</p></td></tr><tr><td><code>version</code> <b>[Required]</b><br><code>string</code></td><td><p>Version represents the default version of the backendRuntime.
It will be appended to the image as a tag.</p></td></tr><tr><td><code>envs</code><br><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#envvar-v1-core><code>[]k8s.io/api/core/v1.EnvVar</code></a></td><td><p>Envs represents the environments set to the container.</p></td></tr><tr><td><code>lifecycle</code><br><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#lifecycle-v1-core><code>k8s.io/api/core/v1.Lifecycle</code></a></td><td><p>Lifecycle represents hooks executed during the lifecycle of the container.</p></td></tr><tr><td><code>livenessProbe</code><br><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#probe-v1-core><code>k8s.io/api/core/v1.Probe</code></a></td><td><p>Periodic probe of backend liveness.
Backend will be restarted if the probe fails.
Cannot be updated.</p></td></tr><tr><td><code>readinessProbe</code><br><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#probe-v1-core><code>k8s.io/api/core/v1.Probe</code></a></td><td><p>Periodic probe of backend readiness.
Backend will be removed from service endpoints if the probe fails.</p></td></tr><tr><td><code>startupProbe</code><br><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#probe-v1-core><code>k8s.io/api/core/v1.Probe</code></a></td><td><p>StartupProbe indicates that the Backend has successfully initialized.
If specified, no other probes are executed until this completes successfully.
If this probe fails, the backend will be restarted, just as if the livenessProbe failed.
This can be used to provide different probe parameters at the beginning of a backend's lifecycle,
when it might take a long time to load data or warm a cache, than during steady-state operation.</p></td></tr><tr><td><code>recommendedConfigs</code><br><a href=#inference-llmaz-io-v1alpha1-RecommendedConfig><code>[]RecommendedConfig</code></a></td><td><p>RecommendedConfigs represents the recommended configurations for the backendRuntime.</p></td></tr></tbody></table><h2 id=inference-llmaz-io-v1alpha1-BackendRuntimeStatus><code>BackendRuntimeStatus</code></h2><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-BackendRuntime>BackendRuntime</a></li></ul><p>BackendRuntimeStatus defines the observed state of BackendRuntime</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>conditions</code> <b>[Required]</b><br><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#condition-v1-meta><code>[]k8s.io/apimachinery/pkg/apis/meta/v1.Condition</code></a></td><td><p>Conditions represents the Inference condition.</p></td></tr></tbody></table><h2 id=inference-llmaz-io-v1alpha1-ElasticConfig><code>ElasticConfig</code></h2><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-PlaygroundSpec>PlaygroundSpec</a></li></ul><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>minReplicas</code><br><code>int32</code></td><td><p>MinReplicas indicates the minimum number of inference workloads based on the traffic.
Default to 1.
MinReplicas couldn't be 0 now, will support serverless in the future.</p></td></tr><tr><td><code>maxReplicas</code> <b>[Required]</b><br><code>int32</code></td><td><p>MaxReplicas indicates the maximum number of inference workloads based on the traffic.
Default to nil means there's no limit for the instance number.</p></td></tr><tr><td><code>scaleTrigger</code><br><a href=#inference-llmaz-io-v1alpha1-ScaleTrigger><code>ScaleTrigger</code></a></td><td><p>ScaleTrigger defines the rules to scale the workloads.
Only one trigger cloud work at a time, mostly used in Playground.
ScaleTrigger defined here will "overwrite" the scaleTrigger in the recommendedConfig.</p></td></tr></tbody></table><h2 id=inference-llmaz-io-v1alpha1-HPATrigger><code>HPATrigger</code></h2><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-ScaleTrigger>ScaleTrigger</a></li></ul><p>HPATrigger represents the configuration of the HorizontalPodAutoscaler.
Inspired by kubernetes.io/pkg/apis/autoscaling/types.go#HorizontalPodAutoscalerSpec.
Note: HPA component should be installed in prior.</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>metrics</code><br><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#metricspec-v2-autoscaling><code>[]k8s.io/api/autoscaling/v2.MetricSpec</code></a></td><td><p>metrics contains the specifications for which to use to calculate the
desired replica count (the maximum replica count across all metrics will
be used). The desired replica count is calculated multiplying the
ratio between the target value and the current value by the current
number of pods. Ergo, metrics used must decrease as the pod count is
increased, and vice-versa. See the individual metric source types for
more information about how each type of metric must respond.</p></td></tr><tr><td><code>behavior</code><br><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#horizontalpodautoscalerbehavior-v2-autoscaling><code>k8s.io/api/autoscaling/v2.HorizontalPodAutoscalerBehavior</code></a></td><td><p>behavior configures the scaling behavior of the target
in both Up and Down directions (scaleUp and scaleDown fields respectively).
If not set, the default HPAScalingRules for scale up and scale down are used.</p></td></tr></tbody></table><h2 id=inference-llmaz-io-v1alpha1-PlaygroundSpec><code>PlaygroundSpec</code></h2><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-Playground>Playground</a></li></ul><p>PlaygroundSpec defines the desired state of Playground</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>replicas</code><br><code>int32</code></td><td><p>Replicas represents the replica number of inference workloads.</p></td></tr><tr><td><code>modelClaim</code><br><a href=#llmaz-io-v1alpha1-ModelClaim><code>ModelClaim</code></a></td><td><p>ModelClaim represents claiming for one model, it's a simplified use case
of modelClaims. Most of the time, modelClaim is enough.
ModelClaim and modelClaims are exclusive configured.</p></td></tr><tr><td><code>modelClaims</code><br><a href=#llmaz-io-v1alpha1-ModelClaims><code>ModelClaims</code></a></td><td><p>ModelClaims represents claiming for multiple models for more complicated
use cases like speculative-decoding.
ModelClaims and modelClaim are exclusive configured.</p></td></tr><tr><td><code>backendRuntimeConfig</code><br><a href=#inference-llmaz-io-v1alpha1-BackendRuntimeConfig><code>BackendRuntimeConfig</code></a></td><td><p>BackendRuntimeConfig represents the inference backendRuntime configuration
under the hood, e.g. vLLM, which is the default backendRuntime.</p></td></tr><tr><td><code>elasticConfig</code><br><a href=#inference-llmaz-io-v1alpha1-ElasticConfig><code>ElasticConfig</code></a></td><td><p>ElasticConfig defines the configuration for elastic usage,
e.g. the max/min replicas.</p></td></tr></tbody></table><h2 id=inference-llmaz-io-v1alpha1-PlaygroundStatus><code>PlaygroundStatus</code></h2><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-Playground>Playground</a></li></ul><p>PlaygroundStatus defines the observed state of Playground</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>conditions</code> <b>[Required]</b><br><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#condition-v1-meta><code>[]k8s.io/apimachinery/pkg/apis/meta/v1.Condition</code></a></td><td><p>Conditions represents the Inference condition.</p></td></tr><tr><td><code>replicas</code> <b>[Required]</b><br><code>int32</code></td><td><p>Replicas track the replicas that have been created, whether ready or not.</p></td></tr><tr><td><code>selector</code> <b>[Required]</b><br><code>string</code></td><td><p>Selector points to the string form of a label selector which will be used by HPA.</p></td></tr></tbody></table><h2 id=inference-llmaz-io-v1alpha1-RecommendedConfig><code>RecommendedConfig</code></h2><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-BackendRuntimeSpec>BackendRuntimeSpec</a></li></ul><p>RecommendedConfig represents the recommended configurations for the backendRuntime,
user can choose one of them to apply.</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code> <b>[Required]</b><br><code>string</code></td><td><p>Name represents the identifier of the config.</p></td></tr><tr><td><code>args</code><br><code>[]string</code></td><td><p>Args represents all the arguments for the command.
Argument around with {{ .CONFIG }} is a configuration waiting for render.</p></td></tr><tr><td><code>resources</code><br><a href=#inference-llmaz-io-v1alpha1-ResourceRequirements><code>ResourceRequirements</code></a></td><td><p>Resources represents the resource requirements for backend, like cpu/mem,
accelerators like GPU should not be defined here, but at the model flavors,
or the values here will be overwritten.</p></td></tr><tr><td><code>sharedMemorySize</code><br><a href=https://pkg.go.dev/k8s.io/apimachinery/pkg/api/resource#Quantity><code>k8s.io/apimachinery/pkg/api/resource.Quantity</code></a></td><td><p>SharedMemorySize represents the size of /dev/shm required in the runtime of
inference workload.</p></td></tr><tr><td><code>scaleTrigger</code><br><a href=#inference-llmaz-io-v1alpha1-ScaleTrigger><code>ScaleTrigger</code></a></td><td><p>ScaleTrigger defines the rules to scale the workloads.
Only one trigger cloud work at a time.</p></td></tr></tbody></table><h2 id=inference-llmaz-io-v1alpha1-ResourceRequirements><code>ResourceRequirements</code></h2><p><strong>Appears in:</strong></p><ul><li><p><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-BackendRuntimeConfig>BackendRuntimeConfig</a></p></li><li><p><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-RecommendedConfig>RecommendedConfig</a></p></li></ul><p>TODO: Do not support DRA yet, we can support that once needed.</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>limits</code><br><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#resourcelist-v1-core><code>k8s.io/api/core/v1.ResourceList</code></a></td><td><p>Limits describes the maximum amount of compute resources allowed.
More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/</p></td></tr><tr><td><code>requests</code><br><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#resourcelist-v1-core><code>k8s.io/api/core/v1.ResourceList</code></a></td><td><p>Requests describes the minimum amount of compute resources required.
If Requests is omitted for a container, it defaults to Limits if that is explicitly specified,
otherwise to an implementation-defined value. Requests cannot exceed Limits.
More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/</p></td></tr></tbody></table><h2 id=inference-llmaz-io-v1alpha1-ScaleTrigger><code>ScaleTrigger</code></h2><p><strong>Appears in:</strong></p><ul><li><p><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-ElasticConfig>ElasticConfig</a></p></li><li><p><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-RecommendedConfig>RecommendedConfig</a></p></li></ul><p>ScaleTrigger defines the rules to scale the workloads.
Only one trigger cloud work at a time, mostly used in Playground.</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>hpa</code> <b>[Required]</b><br><a href=#inference-llmaz-io-v1alpha1-HPATrigger><code>HPATrigger</code></a></td><td><p>HPA represents the trigger configuration of the HorizontalPodAutoscaler.</p></td></tr></tbody></table><h2 id=inference-llmaz-io-v1alpha1-ServiceSpec><code>ServiceSpec</code></h2><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-Service>Service</a></li></ul><p>ServiceSpec defines the desired state of Service.
Service controller will maintain multi-flavor of workloads with
different accelerators for cost or performance considerations.</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>modelClaims</code> <b>[Required]</b><br><a href=#llmaz-io-v1alpha1-ModelClaims><code>ModelClaims</code></a></td><td><p>ModelClaims represents multiple claims for different models.</p></td></tr><tr><td><code>replicas</code><br><code>int32</code></td><td><p>Replicas represents the replica number of inference workloads.</p></td></tr><tr><td><code>workloadTemplate</code> <b>[Required]</b><br><code>sigs.k8s.io/lws/api/leaderworkerset/v1.LeaderWorkerTemplate</code></td><td><p>WorkloadTemplate defines the template for leader/worker pods</p></td></tr><tr><td><code>rolloutStrategy</code><br><code>sigs.k8s.io/lws/api/leaderworkerset/v1.RolloutStrategy</code></td><td><p>RolloutStrategy defines the strategy that will be applied to update replicas
when a revision is made to the leaderWorkerTemplate.</p></td></tr></tbody></table><h2 id=inference-llmaz-io-v1alpha1-ServiceStatus><code>ServiceStatus</code></h2><p><strong>Appears in:</strong></p><ul><li><a href=/docs/reference/inference.v1alpha1/#inference-llmaz-io-v1alpha1-Service>Service</a></li></ul><p>ServiceStatus defines the observed state of Service</p><table class=table><thead><tr><th width=30%>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>conditions</code> <b>[Required]</b><br><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#condition-v1-meta><code>[]k8s.io/apimachinery/pkg/apis/meta/v1.Condition</code></a></td><td><p>Conditions represents the Inference condition.</p></td></tr><tr><td><code>replicas</code> <b>[Required]</b><br><code>int32</code></td><td><p>Replicas track the replicas that have been created, whether ready or not.</p></td></tr><tr><td><code>selector</code> <b>[Required]</b><br><code>string</code></td><td><p>Selector points to the string form of a label selector, the HPA will be
able to autoscale your resource.</p></td></tr></tbody></table></div></main></div></div><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class="td-footer__left col-6 col-sm-4 order-sm-1"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title aria-label><a target=_blank rel=noopener href aria-label><i></i></a></li></ul></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=GitHub aria-label=GitHub><a target=_blank rel=noopener href=https://github.com/InftyAI/llmaz aria-label=GitHub><i class="fab fa-github"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=X aria-label=X><a target=_blank rel=noopener href=https://x.com/InftyAI aria-label=X><i class="fab fa-x-twitter"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=Slack aria-label=Slack><a target=_blank rel=noopener href=https://inftyai.slack.com/ aria-label=Slack><i class="fab fa-slack"></i></a></li></ul></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"><span class=td-footer__copyright>&copy;
2025
<span class=td-footer__authors>The InftyAI Team</span></span><span class=td-footer__all_rights_reserved>All Rights Reserved</span></div></div></div></footer></div><script src=/js/main.min.69e2c1ae9320465ab10236d9ef752c6a4442c54b48b883b17c497b7c7d96a796.js integrity="sha256-aeLBrpMgRlqxAjbZ73UsakRCxUtIuIOxfEl7fH2Wp5Y=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script></body></html>