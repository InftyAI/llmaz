<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta name=generator content="Hugo 0.134.2"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=alternate type=application/rss+xml href=https://llmaz.inftyai.com/index.xml><meta name=robots content="index, follow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>llmaz</title>
<meta name=description content="Learn More GitHub Easy, advanced inference platform for large language models on Kubernetes
Key Features
Easy of Use People can quick deploy a LLM service with minimal configurations."><meta property="og:url" content="https://llmaz.inftyai.com/"><meta property="og:site_name" content="llmaz"><meta property="og:title" content="llmaz"><meta property="og:description" content=" Learn More GitHub Easy, advanced inference platform for large language models on Kubernetes
Key Features
Easy of Use People can quick deploy a LLM service with minimal configurations."><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta itemprop=name content="llmaz"><meta itemprop=description content=" Learn More GitHub Easy, advanced inference platform for large language models on Kubernetes
Key Features
Easy of Use People can quick deploy a LLM service with minimal configurations."><meta itemprop=dateModified content="2025-05-02T22:44:11+08:00"><meta itemprop=wordCount content="201"><meta name=twitter:card content="summary"><meta name=twitter:title content="llmaz"><meta name=twitter:description content=" Learn More GitHub Easy, advanced inference platform for large language models on Kubernetes
Key Features
Easy of Use People can quick deploy a LLM service with minimal configurations."><link rel=preload href=/scss/main.min.e26a880677830bd0f4860cb4ab2e650214ac89917d6eaa97183417553e4b2aed.css as=style integrity="sha256-4mqIBneDC9D0hgy0qy5lAhSsiZF9bqqXGDQXVT5LKu0=" crossorigin=anonymous><link href=/scss/main.min.e26a880677830bd0f4860cb4ab2e650214ac89917d6eaa97183417553e4b2aed.css rel=stylesheet integrity="sha256-4mqIBneDC9D0hgy0qy5lAhSsiZF9bqqXGDQXVT5LKu0=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script></head><body class=td-home><header><nav class="td-navbar js-navbar-scroll td-navbar-cover" data-bs-theme=dark><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"><svg id="_图层_2" data-name="图层 2" viewBox="0 0 616.66 634.81"><defs><style>.cls-1{fill:#ee4c1e;stroke:#e94751;stroke-linecap:round;stroke-linejoin:round;stroke-width:8px}</style></defs><g id="_图层_1-2" data-name="图层 1"><g><path class="cls-1" d="M304.81 4.96 13.82 140.69c-3.97 1.85-4.04 7.46-.13 9.42l50.2 25.1c2.92 1.46 6.36 1.47 9.29.03l234.7-115.29c1.86-.87 4.01-.87 5.87-.01l40.85 18.85c5.18 2.82 3.86 6.09-.05 8.06L127.99 201.19c-3.17 1.6-3.2 6.11-.06 7.75l53.28 27.91c2.07 1.08 4.54 1.06 6.58-.06l230.76-126.06c1.44-.76 3.14-.81 4.62-.13l51.6 23.58c3.95 1.82 4.08 7.4.21 9.4L260.37 259.52c-8.72 3.49-13.08 12.21-3.49 17.44l48.83 25.29c3.49 1.74 6.98.87 9.43-.46L601.5 151.15c7.52-3.97 7.15-14.86-.62-18.32L313.26 4.88c-2.7-1.2-5.78-1.17-8.45.08z"/><path class="cls-1" d="M612.55 182.12l.11 69.07c0 .87-.28 1.71-.8 2.4l-198.02 264.9 193.3-103c2.42-1.35 5.41.4 5.41 3.18l.11 63.2-280.79 148.24c-4.85 2.56-12.1-2.24-12.1-7.72v-74.9c0-.81.27-1.59.76-2.23l198.21-256.29c.68-.87-.35-2.04-1.3-1.47L325.25 402.07c-2.43 1.42-5.48-.33-5.48-3.14v-65.01c0-5.34 2.93-10.26 7.63-12.8l274.89-145.15c4.65-2.47 10.25.9 10.25 6.16z"/><path class="cls-1" d="M292.7 321.89 19.16 180.45C12.14 176.84 4 182.71 4 190.61v280.88c0 1.54.89 2.95 2.28 3.61l44.51 21.22c5.31 2.53 11.44-1.34 11.44-7.21l.06-68.13 159.64 80.45v90.66c0 2.62 1.47 5.02 3.81 6.22l57.35 31.02c7.02 3.58 15.34-1.52 15.34-9.4V331.28c0-3.95-2.21-7.57-5.73-9.38zM62.35 361.66l.07-72.14c0-2.58 2.62-4.73 5.9-3.13l147.58 73.39c3.49 1.74 6.03 4.69 6.03 8.81v71.88L62.35 361.66z"/></g></g></svg></span><span class=navbar-brand__name>llmaz</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class=nav-item><a class=nav-link href=/docs/><span>Documentation</span></a></li><li class=nav-item><a class=nav-link href=/docs/reference/><span>Reference</span></a></li></ul></div><div class="d-none d-lg-block"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.077338254ed649e9430aa7ba95cefecc.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></div></nav></header><div class="container-fluid td-default td-outer"><main role=main class=td-main><section id=td-cover-block-0 class="row td-cover-block td-cover-block--height-max js-td-cover td-overlay td-overlay--dark -bg-primary"><div class=col-12><div class="container td-overlay__inner"><div class=text-center><div class="pt-3 lead"><p><img class="w-50 h-auto mb-4" src=/images/logo.png class=llmaz-logo></p><a class="btn btn-lg btn-secondary me-3 mb-4" href=/docs/>Learn More <i class="fas fa-arrow-alt-circle-right ms-2"></i>
</a><a class="btn btn-lg btn-secondary me-3 mb-4" href=https://github.com/InftyAI/llmaz>GitHub <i class="fab fa-github ms-2"></i></a><p class="lead mt-5 -text-white">Easy, advanced inference platform for large language models on Kubernetes</p><p><a class="btn btn-link text-white" href=#td-block-1 aria-label="Read more"><i class="fa-solid fa-circle-chevron-down" style=font-size:400%></i></a></p></div></div></div></div></section><div><a id=td-block-1 class=td-offset-anchor></a></div><section class="row td-box td-box--white td-box--height-auto"><div class=col><div class=row><p class="h1 text-center mb-4">Key Features</p><div class="col-lg-4 mb-5 mb-lg-0 text-center"><div class="mb-4 h1"><i class="fas fa-user-shield"></i></div><h4 class=h3>Easy of Use</h4><div class=mb-0><p>People can quick deploy a LLM service with minimal configurations.</p></div></div><div class="col-lg-4 mb-5 mb-lg-0 text-center"><div class="mb-4 h1"><i class="fas fa-cogs"></i></div><h4 class=h3>Broad Backends Support</h4><div class=mb-0><p>llmaz supports a wide range of advanced inference backends for different scenarios, like <a href=https://github.com/vllm-project/vllm>vLLM</a>, <a href=https://github.com/huggingface/text-generation-inference>Text-Generation-Inference</a>, <a href=https://github.com/sgl-project/sglang>SGLang</a>, <a href=https://github.com/ggerganov/llama.cpp>llama.cpp</a>. Find the full list of supported backends <a href=/InftyAI/llmaz/blob/main/docs/support-backends.md>here</a>.</p></div></div><div class="col-lg-4 mb-5 mb-lg-0 text-center"><div class="mb-4 h1"><i class="fas fa-exchange-alt"></i></div><h4 class=h3>Accelerator Fungibility</h4><div class=mb-0><p>llmaz supports serving the same LLM with various accelerators to optimize cost and performance.</p></div></div><div class="col-lg-4 mb-5 mb-lg-0 text-center"><div class="mb-4 h1"><i class="fas fa-warehouse"></i></div><h4 class=h3>Various Model Providers</h4><div class=mb-0><p>llmaz supports a wide range of model providers, such as <a href=https://huggingface.co/ rel=nofollow>HuggingFace</a>, <a href=https://www.modelscope.cn rel=nofollow>ModelScope</a>, ObjectStores. llmaz will automatically handle the model loading, requiring no effort from users.</p></div></div><div class="col-lg-4 mb-5 mb-lg-0 text-center"><div class="mb-4 h1"><i class="fas fa-network-wired"></i></div><h4 class=h3>Multi-Host Support</h4><div class=mb-0><p>llmaz supports both single-host and multi-host scenarios with <a href=https://github.com/kubernetes-sigs/lws>LWS</a> from day 0.</p></div></div><div class="col-lg-4 mb-5 mb-lg-0 text-center"><div class="mb-4 h1"><i class="fas fa-door-open"></i></div><h4 class=h3>AI Gateway Support</h4><div class=mb-0><p>Offering capabilities like token-based rate limiting, model routing with the integration of <a href=https://aigateway.envoyproxy.io/ rel=nofollow>Envoy AI Gateway</a>.</p></div></div><div class="col-lg-4 mb-5 mb-lg-0 text-center"><div class="mb-4 h1"><i class="fas fa-comments"></i></div><h4 class=h3>Build-in ChatUI</h4><div class=mb-0><p>Out-of-the-box chatbot support with the integration of <a href=https://github.com/open-webui/open-webui>Open WebUI</a>, offering capacities like function call, RAG, web search and more, see configurations <a href=/InftyAI/llmaz/blob/main/docs/open-webui.md>here</a>.</p></div></div><div class="col-lg-4 mb-5 mb-lg-0 text-center"><div class="mb-4 h1"><i class="fas fa-expand-arrows-alt"></i></div><h4 class=h3>Scaling Efficiency</h4><div class=mb-0><p>llmaz supports horizontal scaling with <a href=/InftyAI/llmaz/blob/main/docs/examples/hpa/README.md>HPA</a> by default and will integrate with autoscaling components like <a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler>Cluster-Autoscaler</a> or <a href=https://github.com/kubernetes-sigs/karpenter>Karpenter</a> for smart scaling across different clouds.</p></div></div><div class="col-lg-4 mb-5 mb-lg-0 text-center"><div class="mb-4 h1"><i class="fas fa-box-open"></i></div><h4 class=h3>Efficient Model Distribution (WIP)</h4><div class=mb-0><p>Out-of-the-box model cache system support with <a href=https://github.com/InftyAI/Manta>Manta</a>, still under development right now with architecture reframing.</p></div></div></div></div></section></main><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class="td-footer__left col-6 col-sm-4 order-sm-1"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=Twitter aria-label=Twitter><a target=_blank rel=noopener href=https://x.com/InftyAI aria-label=Twitter><i class="fab fa-x-twitter"></i></a></li></ul></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=GitHub aria-label=GitHub><a target=_blank rel=noopener href=https://github.com/InftyAI/llmaz aria-label=GitHub><i class="fab fa-github"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=Slack aria-label=Slack><a target=_blank rel=noopener href=https://inftyai.slack.com/ aria-label=Slack><i class="fab fa-slack"></i></a></li></ul></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"><span class=td-footer__copyright>&copy;
2025
<span class=td-footer__authors>The InftyAI Team</span></span><span class=td-footer__all_rights_reserved>All Rights Reserved</span></div></div></div></footer></div><script src=/js/main.min.69e2c1ae9320465ab10236d9ef752c6a4442c54b48b883b17c497b7c7d96a796.js integrity="sha256-aeLBrpMgRlqxAjbZ73UsakRCxUtIuIOxfEl7fH2Wp5Y=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script></body></html>