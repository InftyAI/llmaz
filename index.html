<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta name=generator content="Hugo 0.134.2"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=alternate type=application/rss+xml href=https://llmaz.inftyai.com/index.xml><meta name=robots content="index, follow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>llmaz</title>
<meta name=description content="Learn More GitHub Easy, advanced inference platform for large language models on Kubernetes.
Key Features
Easy of Use People can quick deploy a LLM service with minimal configurations."><meta property="og:url" content="https://llmaz.inftyai.com/"><meta property="og:site_name" content="llmaz"><meta property="og:title" content="llmaz"><meta property="og:description" content=" Learn More GitHub Easy, advanced inference platform for large language models on Kubernetes.
Key Features
Easy of Use People can quick deploy a LLM service with minimal configurations."><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta itemprop=name content="llmaz"><meta itemprop=description content=" Learn More GitHub Easy, advanced inference platform for large language models on Kubernetes.
Key Features
Easy of Use People can quick deploy a LLM service with minimal configurations."><meta itemprop=dateModified content="2025-06-02T18:33:29+08:00"><meta itemprop=wordCount content="201"><meta name=twitter:card content="summary"><meta name=twitter:title content="llmaz"><meta name=twitter:description content=" Learn More GitHub Easy, advanced inference platform for large language models on Kubernetes.
Key Features
Easy of Use People can quick deploy a LLM service with minimal configurations."><link rel=preload href=/scss/main.min.df756438a7e020f7456327e32660cfd018c35e49c250d72b637e5a0fdc7f595c.css as=style integrity="sha256-33VkOKfgIPdFYyfjJmDP0BjDXknCUNcrY35aD9x/WVw=" crossorigin=anonymous><link href=/scss/main.min.df756438a7e020f7456327e32660cfd018c35e49c250d72b637e5a0fdc7f595c.css rel=stylesheet integrity="sha256-33VkOKfgIPdFYyfjJmDP0BjDXknCUNcrY35aD9x/WVw=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script></head><body class=td-home><header><nav class="td-navbar js-navbar-scroll td-navbar-cover" data-bs-theme=dark><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"><svg version="1.2" viewBox="0 0 29 30" width="29" height="30"><style>.a{fill:#fff;stroke:#e94751;stroke-linecap:round;stroke-linejoin:round;stroke-width:.4}</style><path class="a" d="m14.3.0-13.7 6.5c-.2.0-.2.3.0.4L3 8.1q.2.1.4.0l11-5.5q.2.0.3.0l1.9.9c.3.2.2.3.0.4L6 9.3c-.2.1-.2.3.0.4L8.5 11q.1.1.3.0l10.8-6q.1.0.2.0l2.5 1.2c.2.0.2.3.0.4l-10.1 5.5c-.4.1-.6.5-.2.8l2.3 1.2c.2.1.4.0.5.0L28.2 7c.4-.2.4-.7.0-.9L14.7.0q-.2.0-.4.0z"/><path class="a" d="m29 8.8v3.3l-.1.1-9.3 12.5 9.1-4.9c.1.0.3.0.3.2v3l-13.2 7c-.2.1-.6-.1-.6-.4v-3.5q0-.1.1-.1l9.3-12.2s0-.1-.1.0l-9 5.4c-.1.1-.3.0-.3-.2v-3c0-.3.2-.5.4-.6l12.9-6.9c.2-.1.5.0.5.3z"/><path class="a" d="m13.6 15.4-12.9-6.6c-.3-.2-.7.1-.7.4v13.3q0 .1.1.2l2.1 1c.2.1.5-.1.5-.4v-3.2l7.5 3.8v4.3q0 .2.2.3l2.7 1.5c.3.1.7-.1.7-.5V15.9c0-.2-.1-.4-.2-.5zM2.7 17.3v-3.4c0-.1.2-.2.3-.1l7 3.4c.1.1.2.3.2.5V21z"/></svg></span><span class=navbar-brand__name>llmaz</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class=nav-item><a class=nav-link href=/docs/><span>Documentation</span></a></li><li class=nav-item><a class=nav-link href=/docs/reference/><span>Reference</span></a></li><li class=nav-item><a class=nav-link href=/blog/><span>Blog</span></a></li><li class="nav-item dropdown d-none d-lg-block"><div class=dropdown><a class="nav-link dropdown-toggle" href=# role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>Versions</a><ul class=dropdown-menu><li><a class=dropdown-item href=/docs>latest</a></li></ul></div></li></ul></div><div class="d-none d-lg-block"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.363f79eef1626e91e6a10d7c287c4988.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></div></nav></header><div class="container-fluid td-default td-outer"><main role=main class=td-main><section id=td-cover-block-0 class="row td-cover-block td-cover-block--height-max js-td-cover td-overlay td-overlay--dark -bg-primary"><div class=col-12><div class="container td-overlay__inner"><div class=text-center><div class="pt-3 lead"><p><img class="w-50 h-auto mb-4" src=/images/logo-white.png class=llmaz-logo></p><a class="btn btn-lg btn-secondary me-3 mb-4" href=/docs/>Learn More <i class="fas fa-arrow-alt-circle-right ms-2"></i>
</a><a class="btn btn-lg btn-secondary me-3 mb-4" href=https://github.com/InftyAI/llmaz>GitHub <i class="fab fa-github ms-2"></i></a><p class="lead mt-5 -text-white">Easy, advanced inference platform for large language models on Kubernetes.</p><p><a class="btn btn-link text-white" href=#td-block-1 aria-label="Read more"><i class="fa-solid fa-circle-chevron-down" style=font-size:400%></i></a></p></div></div></div></div></section><div><a id=td-block-1 class=td-offset-anchor></a></div><section class="row td-box td-box--white td-box--height-auto"><div class=col><div class=row><p class="h1 text-center mb-4">Key Features</p><div class="col-lg-4 mb-5 mb-lg-0 text-center"><div class="mb-4 h1"><i class="fas fa-user-shield"></i></div><h4 class=h3>Easy of Use</h4><div class=mb-0><p>People can quick deploy a LLM service with minimal configurations.</p></div></div><div class="col-lg-4 mb-5 mb-lg-0 text-center"><div class="mb-4 h1"><i class="fas fa-cogs"></i></div><h4 class=h3>Broad Backends Support</h4><div class=mb-0><p>llmaz supports a wide range of advanced inference backends for different scenarios, like <a href=https://github.com/vllm-project/vllm>vLLM</a>, <a href=https://github.com/huggingface/text-generation-inference>Text-Generation-Inference</a>, <a href=https://github.com/sgl-project/sglang>SGLang</a>, <a href=https://github.com/ggerganov/llama.cpp>llama.cpp</a>. Find the full list of supported backends <a href=/InftyAI/llmaz/blob/main/docs/support-backends.md>here</a>.</p></div></div><div class="col-lg-4 mb-5 mb-lg-0 text-center"><div class="mb-4 h1"><i class="fas fa-exchange-alt"></i></div><h4 class=h3>Accelerator Fungibility</h4><div class=mb-0><p>llmaz supports serving the same LLM with various accelerators to optimize cost and performance.</p></div></div><div class="col-lg-4 mb-5 mb-lg-0 text-center"><div class="mb-4 h1"><i class="fas fa-warehouse"></i></div><h4 class=h3>Various Model Providers</h4><div class=mb-0><p>llmaz supports a wide range of model providers, such as <a href=https://huggingface.co/ rel=nofollow>HuggingFace</a>, <a href=https://www.modelscope.cn rel=nofollow>ModelScope</a>, ObjectStores. llmaz will automatically handle the model loading, requiring no effort from users.</p></div></div><div class="col-lg-4 mb-5 mb-lg-0 text-center"><div class="mb-4 h1"><i class="fas fa-network-wired"></i></div><h4 class=h3>Multi-Host Support</h4><div class=mb-0><p>llmaz supports both single-host and multi-host scenarios with <a href=https://github.com/kubernetes-sigs/lws>LWS</a> from day 0.</p></div></div><div class="col-lg-4 mb-5 mb-lg-0 text-center"><div class="mb-4 h1"><i class="fas fa-door-open"></i></div><h4 class=h3>AI Gateway Support</h4><div class=mb-0><p>Offering capabilities like token-based rate limiting, model routing with the integration of <a href=https://aigateway.envoyproxy.io/ rel=nofollow>Envoy AI Gateway</a>.</p></div></div><div class="col-lg-4 mb-5 mb-lg-0 text-center"><div class="mb-4 h1"><i class="fas fa-comments"></i></div><h4 class=h3>Build-in ChatUI</h4><div class=mb-0><p>Out-of-the-box chatbot support with the integration of <a href=https://github.com/open-webui/open-webui>Open WebUI</a>, offering capacities like function call, RAG, web search and more, see configurations <a href=/InftyAI/llmaz/blob/main/docs/open-webui.md>here</a>.</p></div></div><div class="col-lg-4 mb-5 mb-lg-0 text-center"><div class="mb-4 h1"><i class="fas fa-expand-arrows-alt"></i></div><h4 class=h3>Scaling Efficiency</h4><div class=mb-0><p>llmaz supports horizontal scaling with <a href=/InftyAI/llmaz/blob/main/docs/examples/hpa/README.md>HPA</a> by default and will integrate with autoscaling components like <a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler>Cluster-Autoscaler</a> or <a href=https://github.com/kubernetes-sigs/karpenter>Karpenter</a> for smart scaling across different clouds.</p></div></div><div class="col-lg-4 mb-5 mb-lg-0 text-center"><div class="mb-4 h1"><i class="fas fa-box-open"></i></div><h4 class=h3>Efficient Model Distribution (WIP)</h4><div class=mb-0><p>Out-of-the-box model cache system support with <a href=https://github.com/InftyAI/Manta>Manta</a>, still under development right now with architecture reframing.</p></div></div></div></div></section></main><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class="td-footer__left col-6 col-sm-4 order-sm-1"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title aria-label><a target=_blank rel=noopener href aria-label><i></i></a></li></ul></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=GitHub aria-label=GitHub><a target=_blank rel=noopener href=https://github.com/InftyAI/llmaz aria-label=GitHub><i class="fab fa-github"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=X aria-label=X><a target=_blank rel=noopener href=https://x.com/InftyAI aria-label=X><i class="fab fa-x-twitter"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=Slack aria-label=Slack><a target=_blank rel=noopener href=https://inftyai.slack.com/ aria-label=Slack><i class="fab fa-slack"></i></a></li></ul></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"><span class=td-footer__copyright>&copy;
2025
<span class=td-footer__authors>The InftyAI Team</span></span><span class=td-footer__all_rights_reserved>All Rights Reserved</span></div></div></div></footer></div><script src=/js/main.min.69e2c1ae9320465ab10236d9ef752c6a4442c54b48b883b17c497b7c7d96a796.js integrity="sha256-aeLBrpMgRlqxAjbZ73UsakRCxUtIuIOxfEl7fH2Wp5Y=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script></body></html>